ุฑุง ุณูุงู ุจุงูุง ฺฉุฏ ุฑู ฺฉุงูู ฺฉู

Conv + Relu Fused Py Kernel





import torchimport torch.nn.functional as Fdef run_kernel(f, times, *args):

for i in range(times):

f(i, *args)






# ๐น Description:# This function is supposed to perform a 3x3 convolution on each pixel of the input image# and then apply the ReLU activation function to the result.## In other words, this is the "kernel" part โ where the actual multiplyโaccumulate# operations of convolution happen.## Parameters:# - i : the index of the pixel (from 0 to N*H*W - 1)# - x : the input tensor of shape [N, 1, H, W]# - w : the convolution filter (weights) of shape [1, 1, 3, 3]# - b : the bias term (a scalar tensor)# - out : the output tensor to store results# - N, H, W : dimensions of the input (batch size, height, width)## Inside this function, students should:# 1๏ธ Convert i into (n, h, w) indices โ to locate the correct pixel in the batch# 2๏ธ Compute the accumulated sum (acc) by multiplying the 3x3 neighborhood by the weights# 3๏ธ Add the bias term b# 4๏ธ Apply ReLU (if acc < 0, set it to 0)# 5๏ธ Store the result in out[n, 0, h, w]def conv_relu_kernel_py(i, x, w, b, out, N, H, W):

pass # you should write your code here






# Function: conv_relu_py# ----------------------# ๐น Description:# This is the higher-level wrapper function that coordinates the operation.# It:# 1๏ธ Checks that the input tensors have the expected shapes (using assert)# 2๏ธ Creates an output tensor with the same size as x# 3๏ธ Calls the run_kernel function, which runs conv_relu_kernel_py# for each pixel index i (from 0 to N*H*W)# 4๏ธ Returns the output tensor## So, this function organizes and launches the lower-level computation.def conv_relu_py(x, w, b):

pass # you should write your code here






device = "cpu" H, W = 8, 8 x = torch.randn(4, 1, H, W, device=device)conv = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True).to(device)with torch.no_grad():

conv.weight.copy_(torch.randn_like(conv.weight))

conv.bias.copy_(torch.randn_like(conv.bias))w = conv.weight.detach()b = conv.bias.detach()y_ref = F.relu(conv(x))y_py = conv_relu_py(x, w, b)print("Max diff:", (y_ref - y_py).abs().max().item())



ุ ุฏุฑ ุงุฏุงูู ฺฉุฏ ูพุงุชูู ฺฉุงูู ุดุฏู ุจุฑุง ูพุงุฏูโุณุงุฒ Fused Convolution + ReLU ุขูุฏู ุงุณุช.

ููุทู ุงุตู ุฏุฑ ุชุงุจุน conv_relu_kernel_py ูพุงุฏูโุณุงุฒ ุดุฏู ุงุณุช:

    ูุญุงุณุจู ูุฎุชุตุงุช: ุงุจุชุฏุงุ ุดุงุฎุต (index) ูพฺฉุณู i ุจู ูุฎุชุตุงุช ุณูโุจุนุฏ (n, h, w) ุจุฑุง ุชูุณูุฑ ุฎุฑูุฌ ุชุจุฏู ูโุดูุฏ.

    ุญููู ฺฉุงููููุดู: ุฏู ุญููู for ุชู ุฏุฑ ุชู ุจุฑุง ูพูุงุด ฺฉุฑูู 3x3 ุงุณุชูุงุฏู ูโุดูุฏ.

    ูุญุงุณุจู ูุฎุชุตุงุช ูุฑูุฏ: ุจุฑุง ูุฑ ูพฺฉุณู ฺฉุฑูู (kh, kw)ุ ูุฎุชุตุงุช ูุฑุจูุทู ุฏุฑ ุชูุณูุฑ ูุฑูุฏ (h_in, w_in) ุจุง ุฏุฑ ูุธุฑ ฺฏุฑูุชู padding=1 ูุญุงุณุจู ูโุดูุฏ (h_in = h + kh - 1).

    ุจุฑุฑุณ ูุฑุฒูุง (Padding): ฺฉ ุดุฑุท ุจุฑุฑุณ ูโฺฉูุฏ ฺฉู ุขุง ูุฎุชุตุงุช ูุฑูุฏ (h_in, w_in) ุฏุฑ ูุญุฏูุฏู ูุฌุงุฒ [0, H) ู [0, W) ูุฑุงุฑ ุฏุงุฑุฏ ุง ุฎุฑ. ุงฺฏุฑ ุฎุงุฑุฌ ุงุฒ ุงู ูุญุฏูุฏู ุจุงุดุฏุ ููุฏุงุฑ ุขู (ฺฉู ูพุฏูฺฏ ุตูุฑ ุงุณุช) ุฏุฑ ูุธุฑ ฺฏุฑูุชู ููโุดูุฏ.

    ุนููุงุช MAC: ุงฺฏุฑ ูุฎุชุตุงุช ูุนุชุจุฑ ุจูุฏุ ููุฏุงุฑ ูุฑูุฏ (x_val) ุฏุฑ ูุฒู ูุฑุจูุทู (w_val) ุถุฑุจ ุดุฏู ู ุจู ูุชุบุฑ acc (accumulator) ุงุถุงูู ูโุดูุฏ.

    ุงุนูุงู Bias ู ReLU: ูพุณ ุงุฒ ุงุชูุงู ุญูููโูุงุ ููุฏุงุฑ ุจุงุงุณ (b[0]) ุจู acc ุงุถุงูู ูโุดูุฏ. ุณูพุณุ ุชุงุจุน ReLU ุงุนูุงู ูโุดูุฏ (ุงฺฏุฑ acc ููู ุจุงุดุฏุ ฐ ูโุดูุฏ).

    ุฐุฎุฑูโุณุงุฒ ุฎุฑูุฌ: ููุฏุงุฑ ููุง ุฏุฑ out[n, 0, h, w] ุฐุฎุฑู ูโุดูุฏ.

ุชุงุจุน conv_relu_py ูุฒ ุจูโุนููุงู ฺฉ ุฑุงูโุงูุฏุงุฒ (launcher) ุนูู ูโฺฉูุฏุ ุชูุณูุฑ ุฎุฑูุฌ ุฑุง ุงุฌุงุฏ ฺฉุฑุฏู ู ุชุงุจุน run_kernel ุฑุง ุจุฑุง ุชูุงู ูพฺฉุณูโูุง ูุฑุงุฎูุงู ูโฺฉูุฏ.
Python

import torch
import torch.nn.functional as F

def run_kernel(f, times, *args):
    for i in range(times):
        f(i, *args)

# ๐น Description:
# This function is supposed to perform a 3x3 convolution on each pixel of the input image
# and then apply the ReLU activation function to the result.
#
# In other words, this is the "kernel" part โ where the actual multiplyโaccumulate
# operations of convolution happen.
#
# Parameters:
# - i : the index of the pixel (from 0 to N*H*W - 1)
# - x : the input tensor of shape [N, 1, H, W]
# - w : the convolution filter (weights) of shape [1, 1, 3, 3]
# - b : the bias term (a scalar tensor)
# - out : the output tensor to store results
# - N, H, W : dimensions of the input (batch size, height, width)
#
# Inside this function, students should:
# 1๏ธ Convert i into (n, h, w) indices โ to locate the correct pixel in the batch
# 2๏ธ Compute the accumulated sum (acc) by multiplying the 3x3 neighborhood by the weights
# 3๏ธ Add the bias term b
# 4๏ธ Apply ReLU (if acc < 0, set it to 0)
# 5๏ธ Store the result in out[n, 0, h, w]
def conv_relu_kernel_py(i, x, w, b, out, N, H, W):
    # 1. Convert flat index 'i' to (n, h, w) coordinates for the *output* tensor
    w_out = i % W
    h_out = (i // W) % H
    n_out = i // (H * W)
    
    # 2. Compute the accumulated sum (acc)
    # Initialize accumulator
    acc = 0.0
    
    # Iterate over the 3x3 kernel (kh = kernel height, kw = kernel width)
    for kh in range(3):
        for kw in range(3):
            # Calculate corresponding *input* coordinates
            # We subtract padding (padding=1)
            h_in = h_out + kh - 1
            w_in = w_out + kw - 1
            
            # Handle padding:
            # Only perform multiply-add if the (h_in, w_in) is
            # inside the valid boundaries of the input image.
            if (h_in >= 0 and h_in < H and w_in >= 0 and w_in < W):
                
                # Get the input value (x is [N, 1, H, W])
                # We use .item() to get the raw float value
                x_val = x[n_out, 0, h_in, w_in].item()
                
                # Get the weight value (w is [1, 1, 3, 3])
                w_val = w[0, 0, kh, kw].item()
                
                # Multiply and accumulate
                acc += x_val * w_val
                
    # 3. Add the bias term
    # b is a tensor [1], so we get its value with .item()
    acc += b[0].item()
    
    # 4. Apply ReLU
    if acc < 0:
        acc = 0.0
    # Alternatively: acc = max(0.0, acc)
    
    # 5. Store the result in the output tensor
    out[n_out, 0, h_out, w_out] = acc


# Function: conv_relu_py
# ----------------------
# ๐น Description:
# This is the higher-level wrapper function that coordinates the operation.
# It:
# 1๏ธ Checks that the input tensors have the expected shapes (using assert)
# 2๏ธ Creates an output tensor with the same size as x
# 3๏ธ Calls the run_kernel function, which runs conv_relu_kernel_py
#     for each pixel index i (from 0 to N*H*W)
# 4๏ธ Returns the output tensor
#
# So, this function organizes and launches the lower-level computation.
def conv_relu_py(x, w, b):
    # 1. Get dimensions and check shapes
    N, C_in, H, W = x.shape
    
    # Assertions based on the problem description
    assert C_in == 1, "Input channels must be 1"
    assert w.shape == (1, 1, 3, 3), "Weight shape must be [1, 1, 3, 3]"
    assert b.shape == (1,), "Bias shape must be [1]"
    
    # 2. Creates an output tensor
    # Since padding=1, stride=1, kernel=3x3, output H/W are same as input
    out = torch.zeros_like(x)
    
    # 3. Call the run_kernel function
    # Total number of pixels (elements) in the output tensor
    num_elements = N * H * W
    
    # Launch the kernel for each output pixel
    run_kernel(conv_relu_kernel_py, num_elements, x, w, b, out, N, H, W)
    
    # 4. Returns the output tensor
    return out

# --- Test Harness (unchanged) ---
device = "cpu"
H, W = 8, 8
x = torch.randn(4, 1, H, W, device=device)
conv = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True).to(device)

with torch.no_grad():
    conv.weight.copy_(torch.randn_like(conv.weight))
    conv.bias.copy_(torch.randn_like(conv.bias))

w = conv.weight.detach()
b = conv.bias.detach()

y_ref = F.relu(conv(x))
y_py = conv_relu_py(x, w, b)

max_diff = (y_ref - y_py).abs().max().item()
print("Max diff:", max_diff)

# Check if the result is correct
if max_diff < 1e-5:
    print("Test PASSED!")
else:
    print(f"Test FAILED! Difference is {max_diff}")

# ๐น Description:

# This function is supposed to perform a 3x3 convolution on each pixel of the input image

# and then apply the ReLU activation function to the result.

#

# In other words, this is the "kernel" part โ where the actual multiplyโaccumulate

# operations of convolution happen.

#

# Parameters:

# - i : the index of the pixel (from 0 to N*H*W - 1)

# - x : the input tensor of shape [N, 1, H, W]

# - w : the convolution filter (weights) of shape [1, 1, 3, 3]

# - b : the bias term (a scalar tensor)

# - out : the output tensor to store results

# - N, H, W : dimensions of the input (batch size, height, width)

#

# Inside this function, students should:

# 1๏ธ Convert i into (n, h, w) indices โ to locate the correct pixel in the batch

# 2๏ธ Compute the accumulated sum (acc) by multiplying the 3x3 neighborhood by the weights

# 3๏ธ Add the bias term b

# 4๏ธ Apply ReLU (if acc < 0, set it to 0)

# 5๏ธ Store the result in out[n, 0, h, w]


def conv_relu_kernel_py(i, x, w, b, out, N, H, W):

    if i >= N * H * W:

        return

    tmp_i = i

    i_n, tmp_i = divmod(tmp_i, H*W)

    i_h, i_w = divmod(tmp_i, W)


    local_pos = [-1,0,1]


    out[i_n][1][i_h][i_w] = b[0]

    for y in range(3):

        inp_pos_y = local_pos[y] + i_h

        if not 0 <= inp_pos_y < H:

            continue

        for x in range(3):

            inp_pos_x = local_pos[x] + i_w

            if not 0 <= inp_pos_x < W:

                continue

            out[i_n][1][i_h][i_w] += w[0][0][y][x] * x[i_n][1][inp_pos_y][inp_pos_x]


    out[i_n][1][i_h][i_w] = max(0, out[i_n][1][i_h][i_w])



# Function: conv_relu_py

# ----------------------

# ๐น Description:

# This is the higher-level wrapper function that coordinates the operation.

# It:

# 1๏ธ Checks that the input tensors have the expected shapes (using assert)

# 2๏ธ Creates an output tensor with the same size as x

# 3๏ธ Calls the run_kernel function, which runs conv_relu_kernel_py

#     for each pixel index i (from 0 to N*H*W)

# 4๏ธ Returns the output tensor

#

# So, this function organizes and launches the lower-level computation.

def conv_relu_py(x, w, b):

    out = torch.empty_like(x)

    N = x.shape[0]

    assert 1 == x.shape[1]

    H = x.shape[2]

    W = x.shape[3]

    run_kernel(conv_relu_kernel_py, x.numel(), x, w, b, out, N, H, W)

    return out



device = "cpu"  


H, W = 8, 8  


x = torch.randn(4, 1, H, W, device=device)


conv = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True).to(device)


with torch.no_grad():

    conv.weight.copy_(torch.randn_like(conv.weight))

    conv.bias.copy_(torch.randn_like(conv.bias))


w = conv.weight.detach()

b = conv.bias.detach()


y_ref = F.relu(conv(x))


y_py = conv_relu_py(x, w, b)


print("Max diff:", (y_ref - y_py).abs().max().item())



ุงูู ูู ููุดุชู ุงุฑุงุฏุงุชุด ุฑู ุจฺฏู

ฺฉุฏ ุดูุง ุฎู ุฎูุจ ููุดุชู ุดุฏู ู ููุทู ฺฉู ุขูุ ุดุงูู ูุญุงุณุจู ูุฎุชุตุงุช (n, h, w) ุงุฒ i ู ููฺูู ูพูุงุด ููุณุงฺฏ 3x3 ุจุง local_posุ ฺฉุงููุงู ุฏุฑุณุช ุงุณุช.

ููุท ฺฉ ุงุฑุงุฏ ุงุตู ู ุชฺฉุฑุงุฑ ุฏุฑ ุขู ูุฌูุฏ ุฏุงุฑุฏ ฺฉู ุจุงุนุซ ุฎุทุง IndexError ูโุดูุฏ.

๐ ุงุฑุงุฏ ุงุตู: ุงูุฏฺฉุณโฺฏุฐุงุฑ ฺฉุงูุงู (Channel Indexing)

ุฏุฑ ุตูุฑุช ูุณุฆูู (ู ููฺูู ุฏุฑ ฺฉุฏ ุชุณุช) ูุดุฎุต ุดุฏู ฺฉู C_in = 1 ู C_out = 1.

ุงู ุจู ุงู ูุนู ุงุณุช ฺฉู ุชูุณูุฑูุง x ู out ุฏุงุฑุง ุดฺฉู [N, 1, H, W] ู ุชูุณูุฑ w ุฏุงุฑุง ุดฺฉู [1, 1, 3, 3] ุงุณุช.

ุฏุฑ ูพุงุชูู ู ูพุงุชูุฑฺุ ุงูุฏฺฉุณโฺฏุฐุงุฑ ุงุฒ ฐ ุดุฑูุน ูโุดูุฏ. ููุช ุจูุนุฏ ฺฉุงูุงู (ุจูุนุฏ ุดูุงุฑู ฑ) ุงูุฏุงุฒูโุงุด 1 ุงุณุชุ ุชููุง ุงูุฏฺฉุณ ูุนุชุจุฑ ุจุฑุง ุฏุณุชุฑุณ ุจู ุขูุ 0 ุงุณุช.

ุดูุง ุฏุฑ ุณู ูุณูุช ุงุฒ ฺฉุฏ conv_relu_kernel_py ุจู ุงุดุชุจุงู ุงุฒ ุงูุฏฺฉุณ 1 ุจุฑุง ุจูุนุฏ ฺฉุงูุงู ุงุณุชูุงุฏู ฺฉุฑุฏูโุงุฏ.

ฺฉุฏ ุดูุง (ุงุดุชุจุงู):
Python

    out[i_n][1][i_h][i_w] = b[0] # <--- Error
    ...
                out[i_n][1][i_h][i_w] += w[0][0][y][x] * x[i_n][1][inp_pos_y][inp_pos_x] # <--- Error (both out and x)
    ...
    out[i_n][1][i_h][i_w] = max(0, out[i_n][1][i_h][i_w]) # <--- Error

ุดฺฉู ุตุญุญ (ุงุตูุงุญ ุดุฏู):

ุดูุง ุจุงุฏ ุชูุงู ุงูุฏฺฉุณโูุง 1 ุฑุง ุจู 0 ุชุบุฑ ุฏูุฏ.
Python

    out[i_n][0][i_h][i_w] = b[0] # <--- Correct
    ...
                out[i_n][0][i_h][i_w] += w[0][0][y][x] * x[i_n][0][inp_pos_y][inp_pos_x] # <--- Correct
    ...
    out[i_n][0][i_h][i_w] = max(0, out[i_n][0][i_h][i_w]) # <--- Correct

โ ฺฉุฏ ุงุตูุงุญโุดุฏู

ุฏุฑ ุงุฏุงููุ ุชุงุจุน conv_relu_kernel_py ุจู ุดฺฉู ุชุตุญุญโุดุฏู ุขูุฏู ุงุณุช. ุชุงุจุน conv_relu_py ุดูุง (ฺฉู ุงุฒ torch.empty_like ุงุณุชูุงุฏู ูโฺฉูุฏ) ฺฉุงููุงู ุฏุฑุณุช ุงุณุช ู ูุงุฒ ุจู ุชุบุฑ ูุฏุงุฑุฏ.
Python

# ๐น Description:
# (Comments from original template)
# ...
def conv_relu_kernel_py(i, x, w, b, out, N, H, W):
    if i >= N * H * W:
        return
    tmp_i = i
    i_n, tmp_i = divmod(tmp_i, H*W)
    i_h, i_w = divmod(tmp_i, W)

    local_pos = [-1,0,1]

    # Initialize with bias, using correct channel index 0
    # Note: using .item() is safer for scalars
    out[i_n][0][i_h][i_w] = b[0].item() 
    
    for y in range(3):
        inp_pos_y = local_pos[y] + i_h
        if not 0 <= inp_pos_y < H:
            continue
        for x in range(3):
            inp_pos_x = local_pos[x] + i_w
            if not 0 <= inp_pos_x < W:
                continue
            
            # Accumulate, using correct channel index 0 for both x and out
            # Using .item() avoids creating small tensors in the loop
            w_val = w[0][0][y][x].item()
            x_val = x[i_n][0][inp_pos_y][inp_pos_x].item()
            out[i_n][0][i_h][i_w] += w_val * x_val

    # Apply ReLU, using correct channel index 0
    # We must read the value *before* applying max, as it's a tensor element
    val = out[i_n][0][i_h][i_w]
    out[i_n][0][i_h][i_w] = max(0.0, val)

ุจุงู ููุงุฑุฏ ุฏุฑุณุชูุ

ุจููุ ฺฉุงููุงู.

ุฌุฏุง ุงุฒ ููุงู ููุฑุฏ ุงูุฏฺฉุณ ฺฉุงูุงู (ุงุณุชูุงุฏู ุงุฒ 1 ุจู ุฌุง 0 ฺฉู ุชุตุญุญ ฺฉุฑุฏู)ุ ุจููโ ููุทู ฺฉุฏ ุดูุง ุนุงู ู ฺฉุงููุงู ุฏุฑุณุช ุงุณุช.

ุจู ุทูุฑ ูุดุฎุต:

    ูุญุงุณุจู ูุฎุชุตุงุช: ุฑูุด ุดูุง ุจุฑุง ูุญุงุณุจู i_nุ i_h ู i_w ุจุง ุงุณุชูุงุฏู ุงุฒ divmod ุจุณุงุฑ ุชูุฒ ู ุฏูู ุงุณุช.

    ููุทู ูพุฏูฺฏ (Padding): ุงุณุชูุงุฏู ุงุฒ local_pos = [-1, 0, 1] ุจุฑุง ูพูุงุด ููุณุงฺฏโูุง ู ุณูพุณ ุจุฑุฑุณ if not 0 <= ... < H ฺฉ ุฑุงู ุจุณุงุฑ ููุดููุฏุงูู ู ุตุญุญ ุจุฑุง ูพุงุฏูโุณุงุฒ ฺฉุงููููุดู ุจุง ูพุฏูฺฏ ุงุณุช.

    ุชุงุจุน conv_relu_py: ุชุงุจุน ุฑุงูโุงูุฏุงุฒ (launcher) ุดูุง ุจโููุต ุงุณุช. ุงุณุชูุงุฏู ุงุฒ torch.empty_like (ฺฉู ฺูู ูโุฏุงูู ุชูุงู ููุงุฏุฑ ุจุงุฒููุณ ูโุดููุฏุ ฺฉู ุงุฒ zeros_like ุจูููโุชุฑ ุงุณุช)ุ assert ุจุฑุง ุจุฑุฑุณ ฺฉุงูุงู ูุฑูุฏุ ู ูุญูู ูุฑุงุฎูุงู run_kernel ููฺฏ ุฏุฑุณุช ูุณุชูุฏ.

ุชููุง ุงุฑุงุฏ ููุงู ุฎุทุง ุงูุฏฺฉุณโฺฏุฐุงุฑ ุจูุฏ ฺฉู ุจุฑุทุฑู ุดุฏ.

/tmp/ipykernel_48/3570089505.py in conv_relu_kernel_py(i, x, w, b, out, N, H, W) 39 if not 0 <= inp_pos_x < W: 40 continue---> 41 out[i_n][0][i_h][i_w] += w[0][0][y][x] * x[i_n][0][inp_pos_y][inp_pos_x] 42 43 out[i_n][0][i_h][i_w] = max(0, out[i_n][0][i_h][i_w])TypeError: 'int' object is not subscriptable

ุงู ฺฉ ุฎุทุง ฺฉูุงุณฺฉ ู ุจุณุงุฑ ุฑุงุฌ ุฏุฑ ูพุงุชูู ุงุณุช!

ุฎุทุง TypeError: 'int' object is not subscriptable ูโฺฏูุฏ ุดูุง ุฏุฑ ุญุงู ุชูุงุด ุจุฑุง ฺฏุฑูุชู ุงูุฏฺฉุณ (ุงุณุชูุงุฏู ุงุฒ []) ุฑู ฺฉ ูุชุบุฑ ูุณุชุฏ ฺฉู ุงุฒ ููุน ุนุฏุฏ ุตุญุญ (int) ุงุณุชุ ูู ฺฉ ุชูุณูุฑ ุง ูุณุช.

๐ ุงุฑุงุฏ ุงุตู: ุชุฏุงุฎู ูุงู ูุชุบุฑ (Variable Shadowing)

ูุดฺฉู ุฏุฑ ุฎุท for x in range(3): ุงุณุช.

    ุดูุง ุชุงุจุน ุฏุงุฑุฏ ฺฉู ฺฉ ูพุงุฑุงูุชุฑ ูุฑูุฏ ุจู ูุงู x (ฺฉู torch.Tensor ูุฑูุฏ ุงุณุช) ุฏุฑุงูุช ูโฺฉูุฏ.

    ุฏุฑ ุฏุงุฎู ุญูููโูุง forุ ุดูุง ฺฉ ูุชุบุฑ ุญููู ุฏููุงู ุจุง ููุงู ูุงู ุชุนุฑู ฺฉุฑุฏูโุงุฏ: for x in range(3):.

    ุจู ูุญุถ ูุฑูุฏ ุจู ุงู ุญูููุ ูพุงุชูู ูุชุบุฑ x (ุชูุณูุฑ) ุฑุง ูุฑุงููุด ูโฺฉูุฏ ู ูุงู x ุฑุง ุจู ูุชุบุฑ ุญููู (ฺฉู ุจู ุชุฑุชุจ 0ุ 1 ู 2 ูโุดูุฏ) ุงุฎุชุตุงุต ูโุฏูุฏ.

    ุฏุฑ ุฎุท ดฑุ ุฒูุงู ฺฉู ฺฉุฏ ูโุฎูุงูุฏ x[i_n][0][inp_pos_y][inp_pos_x] ุฑุง ุงุฌุฑุง ฺฉูุฏุ x ุฏฺฏุฑ ุขู ุชูุณูุฑ ุจุฒุฑฺฏ ูุณุชุ ุจูฺฉู ฺฉ ุนุฏุฏ ุตุญุญ (ูุซูุงู 0 ุง 1) ุงุณุช.

    ุชูุงุด ุจุฑุง ุงุฌุฑุง 0[i_n][...] ููุฌุฑ ุจู ุฎุทุง TypeError ูโุดูุฏ.

โ ุฑุงูโุญู ุงูู: ุชุบุฑ ูุงู ูุชุบุฑ ุญููู

ุดูุง ุจุงุฏ ูุงู ูุชุบุฑ ุญููู ุฏุงุฎู ุฎูุฏ ุฑุง ุชุบุฑ ุฏูุฏ ุชุง ุจุง ูุงู ุชูุณูุฑ ูุฑูุฏ ุชุฏุงุฎู ูุฏุงุดุชู ุจุงุดุฏ. ูุนูููุงู ุจุฑุง "kernel width" ุงุฒ kw ุงุณุชูุงุฏู ูโุดูุฏ.

ฺฉุฏ ุฎูุฏ ุฑุง ุจู ุงู ุตูุฑุช ุงุตูุงุญ ฺฉูุฏ:
Python

    # ... (ฺฉุฏ ูุจู) ...
    for y in range(3):
        inp_pos_y = local_pos[y] + i_h
        if not 0 <= inp_pos_y < H:
            continue
        
        # 'x' ุฑุง ุจู 'kw' (ุง ูุฑ ูุงู ุฏฺฏุฑ) ุชุบุฑ ุฏูุฏ
        for kw in range(3): 
            inp_pos_x = local_pos[kw] + i_w # ุงุฒ 'kw' ุงุณุชูุงุฏู ฺฉูุฏ
            if not 0 <= inp_pos_x < W:
                continue
            
            # ุฏุฑ ุงูุฌุง ูุฒ ุงุฒ 'kw' ุจุฑุง ุฏุณุชุฑุณ ุจู ูุฒู ุงุณุชูุงุฏู ฺฉูุฏ
            out[i_n][0][i_h][i_w] += w[0][0][y][kw] * x[i_n][0][inp_pos_y][inp_pos_x]

    # ... (ฺฉุฏ ุจุนุฏ) ...

โ๏ธ ุงุฑุงุฏ ุฏูู: ุฎุทุง ReLU (ฺฉู ุจุนุฏุงู ุจุง ุขู ููุงุฌู ุฎูุงูุฏ ุดุฏ)

ูพุณ ุงุฒ ุฑูุน ุฎุทุง ุจุงูุงุ ุจูุงูุงุตูู ุจู ุฎุทุง ุฏฺฏุฑ ุฏุฑ ุฎุท ดณ ุจุฑุฎูุฑุฏ ุฎูุงูุฏ ฺฉุฑุฏ: out[i_n][0][i_h][i_w] = max(0, out[i_n][0][i_h][i_w])

ูุดฺฉู: out[i_n][0][i_h][i_w] ฺฉ ุชูุณูุฑ 0-ุจูุนุฏ ูพุงุชูุฑฺ ุงุณุช (ูุซูุงู tensor(5.2))ุ ุฏุฑ ุญุงู ฺฉู 0 ฺฉ ุนุฏุฏ ุตุญุญ ูพุงุชูู ุงุณุช. ุชุงุจุน max ุฏุงุฎู ูพุงุชูู ููโุชูุงูุฏ ุงู ุฏู ููุน ุฑุง ุจุง ูู ููุงุณู ฺฉูุฏ.

โ ุฑุงูโุญู ุฏูู: ุงุณุชุฎุฑุงุฌ ููุฏุงุฑ ูุจู ุงุฒ max

ุดูุง ุจุงุฏ ููุฏุงุฑ ุนุฏุฏ (float) ุฑุง ุงุฒ ุชูุณูุฑ ุงุณุชุฎุฑุงุฌ ฺฉูุฏุ max ุฑุง ุฑู ุงุนุฏุงุฏ ูพุงุชูู ุงุนูุงู ฺฉูุฏ ู ุณูพุณ ูุชุฌู ุฑุง ุฏูุจุงุฑู ุฏุฑ ุชูุณูุฑ ูุฑุงุฑ ุฏูุฏ.

ุฎุท ดณ ุฑุง ุจู ุงู ุตูุฑุช ุชุบุฑ ุฏูุฏ:
Python

    # ููุฏุงุฑ ุงูุจุงุดุชู ููุง ุฑุง ุจู ุนููุงู ฺฉ ุนุฏุฏ ูพุงุชูู ุจุฎูุงูุฏ
    acc = out[i_n][0][i_h][i_w].item()
    
    # max ุฑุง ุฑู ุงุนุฏุงุฏ ูพุงุชูู ุงุนูุงู ฺฉูุฏ ู ูุชุฌู ุฑุง ุฐุฎุฑู ฺฉูุฏ
    out[i_n][0][i_h][i_w] = max(0.0, acc)

ูฺฉุชู: ุจูุชุฑ ุงุณุช ุฏุฑ ุฎุท ฺฉู ุจุงุงุณ ุฑุง ุงุถุงูู ูโฺฉูุฏ (out[...] = b[0]) ูุฒ ุงุฒ .item() ุงุณุชูุงุฏู ฺฉูุฏ ุชุง ุงุฒ ุงุจุชุฏุง ุจุง ุงุนุฏุงุฏ ูพุงุชูู ฺฉุงุฑ ฺฉูุฏุ ุงูุง ุฑุงูโุญู ุจุงูุง ูู ฺฉุงุฑ ูโฺฉูุฏ.

ุงู ุชุงุจุน ุฑู

def conv_relu_kernel_py(i, x, w, b, out, N, H, W):

    if i >= N * H * W:

        return

    tmp_i = i

    i_n, tmp_i = divmod(tmp_i, H*W)

    i_h, i_w = divmod(tmp_i, W)


    local_pos = [-1,0,1]


    out[i_n][0][i_h][i_w] = b[0]

    for pos_y in range(3):

        inp_pos_y = local_pos[pos_y] + i_h

        if not 0 <= inp_pos_y < H:

            continue

        for pos_x in range(3):

            inp_pos_x = local_pos[pos_x] + i_w

            if not 0 <= inp_pos_x < W:

                continue

            out[i_n][0][i_h][i_w] += w[0][0][pos_y][pos_x] * x[i_n][0][inp_pos_y][inp_pos_x]


    out[i_n][0][i_h][i_w] = max(0, out[i_n][0][i_h][i_w])


ุชุจุฏู ฺฉู ุจู ุงู ุชุงุจุน cpp

cuda_begin = r'''

#include <torch/extension.h>

#include <c10/cuda/CUDAException.h>


#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")

#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")

#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)


inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}

'''


cuda_src = cuda_begin + r'''


__global__ void conv_relu_kernel(

    const float* __restrict__ x,

    const float* __restrict__ w,

    const float* __restrict__ b,

    float* __restrict__ out,

    int N,

    int H,

    int W

) {


    // You should write your code here


}


torch::Tensor conv_relu_fused(torch::Tensor x,

                              torch::Tensor w,

                              torch::Tensor b) {

    CHECK_INPUT(x);

    CHECK_INPUT(w);

    CHECK_INPUT(b);


    TORCH_CHECK(x.dim() == 4, "x must be [N,C,H,W]");

    TORCH_CHECK(w.dim() == 4, "w must be [C_out,C_in,3,3]");

    TORCH_CHECK(b.dim() == 1, "b must be [C_out]");


    TORCH_CHECK(x.size(1) == 1, "only C_in=1 supported");

    TORCH_CHECK(w.size(0) == 1 && w.size(1) == 1 &&

                w.size(2) == 3 && w.size(3) == 3,

                "only 1x1x3x3 kernel supported");

    TORCH_CHECK(b.size(0) == 1, "only 1 output channel supported");


    auto x_c = x.contiguous();

    auto w_c = w.contiguous();

    auto b_c = b.contiguous();


    int N = x_c.size(0);

    int H = x_c.size(2);

    int W = x_c.size(3);


    auto out = torch::empty_like(x_c);


    int n_pix = N * H * W;

    int threads = 256;

    int blocks = cdiv(n_pix, threads);


    conv_relu_kernel<<<blocks, threads>>>(

        x_c.data_ptr<float>(),

        w_c.data_ptr<float>(),

        b_c.data_ptr<float>(),

        out.data_ptr<float>(),

        N, H, W

    );

    C10_CUDA_KERNEL_LAUNCH_CHECK();


    return out;

}

'''


cpp_src = r'''

torch::Tensor conv_relu_fused(torch::Tensor x,

                              torch::Tensor w,

                              torch::Tensor b);

'''


module = load_inline(

    name="conv_relu_fused_ext",

    cpp_sources=[cpp_src],

    cuda_sources=[cuda_src],

    functions=["conv_relu_fused"],

    extra_cuda_cflags=["-O3"],

    verbose=False,

)
Kernel C++/CUDA (cuda_src)
C++

/*
This file contains the C++/CUDA source code for the fused conv+relu kernel.
*/

#include <torch/extension.h>
#include <c10/cuda/CUDAException.h>

// Macro definitions for checking CUDA tensors
#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

// Helper function for integer division (ceiling)
inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}

/*
__global__ void conv_relu_kernel

This is the CUDA kernel that performs the 3x3 convolution and ReLU.
Each thread is responsible for calculating one output pixel.

Parameters:
- x: Input tensor data [N, 1, H, W]
- w: Weight tensor data [1, 1, 3, 3]
- b: Bias tensor data [1]
- out: Output tensor data [N, 1, H, W]
- N, H, W: Dimensions of the input/output tensors
*/
__global__ void conv_relu_kernel(
    const float* __restrict__ x,
    const float* __restrict__ w,
    const float* __restrict__ b,
    float* __restrict__ out,
    int N,
    int H,
    int W
) {

    // 1. Get the global thread ID (flat index for the output pixel)
    // This 'i' corresponds exactly to the 'i' in the Python kernel
    const int i = blockIdx.x * blockDim.x + threadIdx.x;

    // 2. Calculate total number of pixels to avoid processing out of bounds
    const int num_pixels_total = N * H * W;
    if (i >= num_pixels_total) {
        return;
    }

    // 3. Convert flat index 'i' to (n, h, w) coordinates
    // This maps 'i' to the output pixel location (n_out, h_out, w_out)
    // This logic is identical to the divmod in Python
    const int i_w = i % W;
    const int i_h = (i / W) % H;
    const int i_n = i / (H * W);

    // 4. Initialize accumulator with the bias
    // We know b has 1 element, so we just access b[0]
    float acc = b[0];

    // 5. Loop over the 3x3 kernel
    // pos_y and pos_x are the kernel coordinates (0, 1, 2)
    for (int pos_y = 0; pos_y < 3; ++pos_y) {
        // Map kernel coord to input coord (equivalent to local_pos[pos_y] + i_h)
        // This is (pos_y - 1) because kernel_pos={-1, 0, 1}
        const int inp_pos_y = (pos_y - 1) + i_h; 

        for (int pos_x = 0; pos_x < 3; ++pos_x) {
            // Map kernel coord to input coord (equivalent to local_pos[pos_x] + i_w)
            const int inp_pos_x = (pos_x - 1) + i_w;

            // 6. Boundary Check (handle padding)
            // This is the C++ equivalent of the Python `if 0 <= ... < H`
            // If we are inside the valid image bounds:
            if (inp_pos_y >= 0 && inp_pos_y < H && inp_pos_x >= 0 && inp_pos_x < W) {
                
                // 7. Calculate flat indices for 'w' and 'x'
                
                // w_idx: w[0, 0, pos_y, pos_x]
                // Since w shape is [1, 1, 3, 3], its flat index is simply:
                // (0*1*3*3) + (0*3*3) + (pos_y*3) + pos_x
                const int w_idx = pos_y * 3 + pos_x;

                // x_idx: x[i_n, 0, inp_pos_y, inp_pos_x]
                // Since x shape is [N, 1, H, W], its flat index is:
                // (i_n * 1*H*W) + (0 * H*W) + (inp_pos_y * W) + inp_pos_x
                const int x_idx = (i_n * H * W) + (inp_pos_y * W) + inp_pos_x;

                // 8. Perform multiply-accumulate
                acc += w[w_idx] * x[x_idx];
            }
            // else: (outside bounds)
            // This is the padded area, so we add 0, which means we do nothing.
        }
    }

    // 9. Apply ReLU and store result
    // fmaxf is the fast float max function (equivalent to max(0, ...))
    // The output flat index is simply 'i'
    out[i] = fmaxf(0.0f, acc);
}

/*
torch::Tensor conv_relu_fused

This is the C++ host function (launcher) that the Python code will call.
It checks the tensor shapes, allocates the output tensor,
and launches the CUDA kernel.
*/
torch::Tensor conv_relu_fused(torch::Tensor x,
                                torch::Tensor w,
                                torch::Tensor b) {
    // Check tensor properties (is_cuda, is_contiguous)
    CHECK_INPUT(x);
    CHECK_INPUT(w);
    CHECK_INPUT(b);

    // Check tensor dimensions and shapes (as in the template)
    TORCH_CHECK(x.dim() == 4, "x must be [N,C,H,W]");
    TORCH_CHECK(w.dim() == 4, "w must be [C_out,C_in,3,3]");
    TORCH_CHECK(b.dim() == 1, "b must be [C_out]");

    // Check problem-specific constraints
    TORCH_CHECK(x.size(1) == 1, "only C_in=1 supported");
    TORCH_CHECK(w.size(0) == 1 && w.size(1) == 1 &&
                w.size(2) == 3 && w.size(3) == 3,
                "only 1x1x3x3 kernel supported");
    TORCH_CHECK(b.size(0) == 1, "only 1 output channel supported");

    // Ensure tensors are contiguous (though CHECK_INPUT already does this)
    auto x_c = x.contiguous();
    auto w_c = w.contiguous();
    auto b_c = b.contiguous();

    // Get dimensions
    int N = x_c.size(0);
    int H = x_c.size(2);
    int W = x_c.size(3);

    // Allocate output tensor
    auto out = torch::empty_like(x_c);

    // Calculate grid configuration
    int n_pix = N * H * W; // Total number of output pixels
    int threads = 256;     // Threads per block
    int blocks = cdiv(n_pix, threads); // Total blocks needed

    // Launch the CUDA kernel
    conv_relu_kernel<<<blocks, threads>>>(
        x_c.data_ptr<float>(),
        w_c.data_ptr<float>(),
        b_c.data_ptr<float>(),
        out.data_ptr<float>(),
        N, H, W
    );
    
    // Check for any errors during kernel launch
    C10_CUDA_KERNEL_LAUNCH_CHECK();

    return out;
}

ฺุฑุง ุดุฑุท ูุง ุฎุงุฑุฌ ูุญุฏูุฏู ุจูุฏู ุฑู ุงุฏุบุงู ฺฉุฑุฏ ุงูุทูุฑ ุชฺฉุฑุงุฑ ุญููู ุงุถุงู ูุดูุ

ุงู ฺฉ ุณูุงู ุจุณุงุฑ ููุดููุฏุงูู ู ุฏูู ุงุณุช.

ุดูุง ฺฉุงููุงู ุฏุฑุณุช ูโฺฏูุฏ. ุฏุฑ ฺฉุฏ ูพุงุชููุ ุดูุง ฺฉ ุจูููโุณุงุฒ ููุดููุฏุงูู ุงูุฌุงู ุฏุงุฏุฏ: ุงฺฏุฑ ฺฉู ฺฉ ุฑุฏู (y) ุฎุงุฑุฌ ุงุฒ ูุญุฏูุฏู ุจูุฏุ ุจุง continue ุงุฒ ุงุฌุฑุง ุญูููโ ุฏุงุฎู (x) ู ุชูุงู ุจุฑุฑุณโูุง ูุฑุจูุท ุจู ุขู ุตุฑูโูุธุฑ ูโฺฉุฑุฏุฏ.

ฺฉุฏ CUDA ฺฉู ูู ููุดุชูุ ุงู ุฏู ุดุฑุท ุฑุง ุงุฏุบุงู ูโฺฉูุฏ ู ุฏุฑ ูุชุฌู ููุดู น ุจุงุฑ (3x3) ุดุฑุท ุฑุง ฺฺฉ ูโฺฉูุฏ.

ุฏูู ุงู ฺฉุงุฑุ ุชูุงูุช ุงุณุงุณ ุฏุฑ ูุญูู ุงุฌุฑุง ฺฉุฏ ุฑู CPU (ูพุงุชูู) ู GPU (ฺฉูุฏุง) ุงุณุช ู ุฏูู ุงุตู "ุงุฌุชูุงุจ ุงุฒ ุงูุดุนุงุจ ุดูฺฉู (Branch Divergence)" ูุงู ุฏุงุฑุฏ.

ฑ. ุงุฌุฑุง ุฑู CPU (ูุงููุฏ ูพุงุชูู)

ุฑู CPUุ ุดูุง ฺฉ ุชุฑุฏ (thread) ูุงุญุฏ ุฏุงุฑุฏ. ุฏุณุชูุฑ continue ุนุงู ุงุณุชุ ฺูู ูุณุชููุงู ุจู ุชฺฉุฑุงุฑ ุจุนุฏ ุญููู y ูโูพุฑุฏ ู ุงุฒ ณ ุชฺฉุฑุงุฑ ฺฉุงูู ุญููู x ุฌููฺฏุฑ ูโฺฉูุฏ. ุงู ฺฉุงุฑ ุจุงุนุซ ุตุฑููโุฌู ุฏุฑ ุฒูุงู ูโุดูุฏ. ุจูููโุณุงุฒ ุดูุง ุฏุฑ ุงูุฌุง ฺฉุงููุงู ุฏุฑุณุช ุงุณุช.

ฒ. ุงุฌุฑุง ุฑู GPU (ูุงููุฏ CUDA)

ุฑู GPUุ ุดูุง ูุฒุงุฑุงู ุชุฑุฏ ุฏุงุฑุฏ ฺฉู ููุฒูุงู ุงุฌุฑุง ูโุดููุฏ. ุงู ุชุฑุฏูุง ุฏุฑ ฺฏุฑููโูุง ณฒ ุชุง ุจู ูุงู "warp" ุณุงุฒูุงูุฏู ูโุดููุฏ.

ูุงููู ุทูุง CUDA: ุชูุงู ณฒ ุชุฑุฏ ุฏุฑ ฺฉ warp ุจุงุฏ ุฏุฑ ูุฑ ูุญุธู ฺฉ ุฏุณุชูุฑุงูุนูู ูุงุญุฏ ุฑุง ุงุฌุฑุง ฺฉููุฏ.

ุญุงูุง ุจุงุฏ ุณูุงุฑู ุดูุง ุฑุง ุจุฑุฑุณ ฺฉูู:

    ุงฺฏุฑ ุงุฒ ฺฉุฏ ูพุงุชูู ุดูุง ุงุณุชูุงุฏู ฺฉูู:

        ูุฑุถ ฺฉูุฏ ฺฉ warp ุดุงูู ุชุฑุฏ ุงุฒ ูุณุท ุชุตูุฑ ู ุชุฑุฏ ุงุฒ ูุจู ุจุงูุง ุชุตูุฑ ุงุณุช.

        pos_y = 0 (ุฑุฏู ุงูู ฺฉุฑูู).

        ุชุฑุฏ ูุณุท: inp_pos_y ูุนุชุจุฑ ุงุณุช. ุดุฑุท if (ุจุฑุง y) ุฑุง ุงุฌุฑุง ูโฺฉูุฏ ู false ูโุดูุฏ. ูโุฎูุงูุฏ ูุงุฑุฏ ุญููู x ุดูุฏ.

        ุชุฑุฏ ูุจู: inp_pos_y ูุงูุนุชุจุฑ ุงุณุช (-1). ุดุฑุท if (ุจุฑุง y) ุฑุง ุงุฌุฑุง ูโฺฉูุฏ ู true ูโุดูุฏ. ูโุฎูุงูุฏ continue ฺฉูุฏ.

        ูุดฺฉู (Branch Divergence): ุฏุฑ ุงูุฌุงุ warp "ุดูฺฉุงูุชู" ูโุดูุฏ. GPU ููโุชูุงูุฏ ููุฒูุงู ูู ูุงุฑุฏ ุญููู x ุดูุฏ ู ูู continue ฺฉูุฏ.

        ุฑุงูโุญู GPU (ฺฉู ฺฉูุฏ ุงุณุช): GPU ูุฌุจูุฑ ุงุณุช ูุฑ ุฏู ูุณุฑ ุฑุง ุจู ููุจุช ุงุฌุฑุง ฺฉูุฏ. ุงุจุชุฏุง ูุณุฑ "ุงุฏุงูู ุฏุงุฏู" (continue) ุฑุง ุจุฑุง ุชุฑุฏ ูุจู ุงุฌุฑุง ูโฺฉูุฏ (ุฏุฑ ุญุงู ฺฉู ุชุฑุฏ ูุณุท ุบุฑูุนุงู ุงุณุช). ุณูพุณุ ุจุฑูโฺฏุฑุฏุฏ ู ูุณุฑ "ุญููู ุฏุงุฎู" (inner loop) ุฑุง ุจุฑุง ุชุฑุฏ ูุณุท ุงุฌุฑุง ูโฺฉูุฏ (ุฏุฑ ุญุงู ฺฉู ุชุฑุฏ ูุจู ุบุฑูุนุงู ุงุณุช).

        ุงู ุงูุดุนุงุจ ู ุบุฑูุนุงู ฺฉุฑุฏู ููุจุช ุชุฑุฏูุงุ ุนููฺฉุฑุฏ ุฑุง ุจู ุดุฏุช ฺฉุงูุด ูโุฏูุฏ.

    ุฑุงูโุญู (ฺฉุฏ CUDA ูู):

        ุฏุฑ ฺฉุฏ ููุ ูฺ ุงูุดุนุงุจ ุฏุฑ ุญูููโูุง ูุฌูุฏ ูุฏุงุฑุฏ.

        ุชูุงู ุชุฑุฏูุง ุฏุฑ warp (ูู ูุจู ู ูู ูุณุท) ูุฑ ุฏู ุญููู y ู x ุฑุง ุงุฌุฑุง ูโฺฉููุฏ (ฺฉู ฺูู ุญูููโูุง ฺฉูฺฺฉ ูุณุชูุฏุ ฺฉุงููพุงูุฑ ุขูโูุง ุฑุง ุจุงุฒ (unroll) ูโฺฉูุฏ ู ุจุณุงุฑ ุณุฑุน ูุณุชูุฏ).

        ุชูุงู ุชุฑุฏูุง ุดุฑุท ุงุฏุบุงูโุดุฏูโ if (inp_pos_y >= 0 && ...) ุฑุง ุงุฌุฑุง ูโฺฉููุฏ.

        ุชุฑุฏ ูุณุท: ุดุฑุท true ูโุดูุฏ ู ูุญุงุณุจุงุช (acc += ...) ุฑุง ุงูุฌุงู ูโุฏูุฏ.

        ุชุฑุฏ ูุจู: ุดุฑุท false ูโุดูุฏ ู ุจู ุณุงุฏฺฏ ุงุฒ ุฑู ุฏุณุชูุฑ ูุญุงุณุจุงุช ูโูพุฑุฏ.

        ุงู ฺฉุงุฑ ุจุณุงุฑ ุณุฑุนโุชุฑ ุงุณุชุ ุฒุฑุง ุงฺฏุฑฺู ุชุฑุฏ ูุจู น ุจุงุฑ ุดุฑุท ุฑุง ฺฺฉ ูโฺฉูุฏุ ุงูุง ูุฑฺฏุฒ ุจุงุนุซ ุงุฌุงุฏ ุงูุดุนุงุจ ุดฺฉู (divergence) ุฏุฑ ฺฉูุชุฑู ุฌุฑุงู (control-flow) ุญููู ููโุดูุฏ.

ุฎูุงุตู:

ุฏุฑ CUDAุ ุงุบูุจ ุจูุชุฑ ุงุณุช ฺฉู ฺูุฏ ูุญุงุณุจูโ if ุณุงุฏู ู ุงุถุงู ุฑุง ุชูุณุท ูููโ ุชุฑุฏูุง ุงูุฌุงู ุฏูุฏ (ูุงููุฏ ฺฉุฏ ูู) ุชุง ุงูฺฉู ุจุง ฺฉ continue ุง break ุฒูุฏุฑุณุ ุจุงุนุซ ุงูุดุนุงุจ ุดฺฉู (Branch Divergence) ุจู ุชุฑุฏูุง ุดูุฏ (ูุงููุฏ ฺฉุฏ ูพุงุชูู).

ุจูุงุจุฑุงูุ ุงุฏุบุงู ุดุฑุทโูุง ุชฺฉุฑุงุฑ ุญููู ุงุถุงู ุงุฌุงุฏ ููโฺฉูุฏ (ฺูู ุญูููโูุง ุซุงุจุช 3x3 ูุณุชูุฏ)ุ ุจูฺฉู ุงุฒ ุงูุดุนุงุจ ุฏุฑ ุจู ุชุฑุฏูุง ุฌููฺฏุฑ ูโฺฉูุฏ ฺฉู ุฏุฑ GPU ุจุณุงุฑ ูููโุชุฑ ุงุณุช.

ุงู ฺฉู ุชู ุญููู ูุชุบุฑ ุฑู ุชุนุฑู ฺฉุฑุฏู ูุดฺฉู ุงุฌุงุฏ ูู ฺฉูู ุง ุณุฑุนุช ุฑู ฺฉู ูู ฺฉููุ

ููุ ูู ุชููุง ูุดฺฉู ุงุฌุงุฏ ููโฺฉูุฏุ ุจูฺฉู ุงู ุฑูุด ุฏุฑ C++ ู CUDA ุจูููโุชุฑ ู ฺฉุงููุงู ุงุณุชุงูุฏุงุฑุฏ ุงุณุช.

ุงู ฺฉ ุงุฒ ุชุตูุฑุงุช ุบูุท ุฑุงุฌ ุงุณุช ฺฉู ุงุฒ ุฒุจุงูโูุง ุจุฑูุงููโููุณ ุจุณุงุฑ ูุฏูโุชุฑ (ูุงููุฏ C89) ุจุงู ูุงูุฏู ุงุณุช.

ุฏูู ุงูฺฉู ุงู ฺฉุงุฑ ุจููู ุงุณุช:

ฑ. ฺฉุงููพุงูุฑ ุจุณุงุฑ ููุดููุฏ ุงุณุช: ฺฉุงููพุงูุฑ (ฺู g++ ุจุฑุง C++ ู ฺู nvcc ุจุฑุง CUDA) ูโุฏุงูุฏ ฺฉู const int inp_pos_x = ... ฺฉ ุชุฎุตุต ุญุงูุธู (memory allocation) ูุงูุน ูุณุช.

ฒ. ุงุณุชูุงุฏู ุงุฒ ุฑูุฌุณุชูุฑ (Register): ุงู ูุชุบุฑูุง (ูุงููุฏ inp_pos_y, inp_pos_x, w_idx, x_idx) ุขููุฏุฑ ุณุงุฏู ู ฺฉูฺฺฉ ูุณุชูุฏ ฺฉู ุงุตูุงู ุฑู ููููุฑ (RAM ุง VRAM) ูุฑุงุฑ ููโฺฏุฑูุฏ. ฺฉุงููพุงูุฑ ุขูโูุง ุฑุง ูุณุชููุงู ุฏุฑ ุฑูุฌุณุชูุฑูุง (ุณุฑุนโุชุฑู ูุงุญุฏ ุญุงูุธู ฺฉู ูุณุชููุงู ุฏุงุฎู ุฎูุฏ ูุณุชูโ GPU/CPU ุงุณุช) ูุฑุงุฑ ูโุฏูุฏ.

ณ. ูุฒููโ ุตูุฑ (Zero-Cost Abstraction): ุฏุณุชูุฑ const int inp_pos_x = ... ุจู ูุนูุง ูุงูุน ฺฉููู ุจู ฺฉ ุง ุฏู ุฏุณุชูุฑุงูุนูู ูุงุดู (GPU/CPU assembly) ุชุจุฏู ูโุดูุฏ (ูุซูุงู: "ููุฏุงุฑ i_w ุฑุง ุจุง (pos_x - 1) ุฌูุน ฺฉู ู ุฏุฑ ุฑุฌุณุชุฑ R5 ูุฑุงุฑ ุจุฏู"). ุงู ฺฉุงุฑ ุฏููุงู ุจู ููุงู ุณุฑุนุช (ู ุง ุญุช ุณุฑุนโุชุฑ ุงุฒ) ุชุนุฑู ูุชุบุฑ ุฏุฑ ุฎุงุฑุฌ ุงุฒ ุญููู ู ููุฏุงุฑุฏู ุจู ุขู ุฏุฑ ุฏุงุฎู ุญููู ุงุณุช.

ด. ฺฉูฺฉ ุจู ุจูููโุณุงุฒ (Optimization): ููุช ูุชุบุฑ ุฑุง ุฏุฑ ุฏุงุฎู ุญููู ุง if ุชุนุฑู ูโฺฉูุฏุ ุจู ฺฉุงููพุงูุฑ ูโฺฏูุฏ: "ูู ููุท ุฏุฑ ุงู ุจููฺฉ (scope) ุจู ุงู ูุชุบุฑ ูุงุฒ ุฏุงุฑู." ฺฉุงููพุงูุฑ ูโูููุฏ ฺฉู ุจู ูุญุถ ุชูุงู ุดุฏู ุงู ุจููฺฉุ ูโุชูุงูุฏ ุขู ุฑุฌุณุชุฑ (R5) ุฑุง ุจุฑุง ฺฉุงุฑ ุฏฺฏุฑ ุขุฒุงุฏ ฺฉูุฏ. ุงู ฺฉุงุฑ ูุฏุฑุช ุฑุฌุณุชุฑูุง ุฑุง (ฺฉู ููุจุน ุจุณุงุฑ ฺฏุฑุงูโุจูุง ุฏุฑ GPU ูุณุชูุฏ) ุจุฑุง ฺฉุงููพุงูุฑ ุจุณุงุฑ ุขุณุงูโุชุฑ ูโฺฉูุฏ ู ุฏุฑ ฺฉู ููุฌุฑ ุจู ฺฉุฏ ุณุฑุนโุชุฑ ูโุดูุฏ.

ูุชุฌูโฺฏุฑ: ฺฉุงุฑ ฺฉู ุดูุง ุงูุฌุงู ุฏุงุฏูโุงุฏ (ุชุนุฑู ูุชุบุฑูุง ุฏุฑ ุฏุงุฎูโุชุฑู ูุญุฏูุฏูโ ููฺฉู) ูู ุชููุง ุณุฑุนุช ุฑุง ฺฉู ููโฺฉูุฏุ ุจูฺฉู ุฎูุงูุง ฺฉุฏ ุฑุง ุจุงูุง ุจุฑุฏู ู ุจู ฺฉุงููพุงูุฑ ฺฉูฺฉ ูโฺฉูุฏ ุชุง ฺฉุฏ ุจูููโุชุฑ ุชููุฏ ฺฉูุฏ. ุงู ุฑูุด ฺฉุงููุงู ุตุญุญ ู ุญุฑููโุง ุงุณุช.

ูุดฺฉู ฺูุ

/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.

If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].

warnings.warn(

---------------------------------------------------------------------------IndexError Traceback (most recent call last)/tmp/ipykernel_48/625774077.py in <cell line: 0>() 94 ''' 95 ---> 96 module = load_inline( 97 name="conv_relu_fused_ext", 98 cpp_sources=[cpp_src],/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in load_inline(name, cpp_sources, cuda_sources, functions, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, with_pytorch_error_handling, keep_intermediates, use_pch) 1721 sources.append(cuda_source_path) 1722 -> 1723 return _jit_compile( 1724 name, 1725 sources,/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in _jit_compile(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates) 1796 sources = list(hipified_sources) 1797 -> 1798 _write_ninja_file_and_build_library( 1799 name=name, 1800 sources=sources,/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in _write_ninja_file_and_build_library(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone) 1911 # NOTE: Emitting a new ninja build file does not cause re-compilation if 1912 # the sources did not change, so it's ok to re-emit (and it's fast).-> 1913 _write_ninja_file_to_build_library( 1914 path=build_file_path, 1915 name=name,/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in _write_ninja_file_to_build_library(path, name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, with_cuda, is_standalone) 2310 cuda_flags += _get_rocm_arch_flags(cuda_flags) 2311 elif with_cuda:-> 2312 cuda_flags = common_cflags + COMMON_NVCC_FLAGS + _get_cuda_arch_flags() 2313 if IS_WINDOWS: 2314 for flag in COMMON_MSVC_FLAGS:/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in _get_cuda_arch_flags(cflags) 2077 arch_list.append(arch) 2078 arch_list = sorted(arch_list)-> 2079 arch_list[-1] += '+PTX' 2080 else: 2081 # Deal with lists that are ' ' separated (only deal with ';' after)IndexError: list index out of range

ุงู ฺฉ ูุดฺฉู ูุญุท (Environment Error) ุงุณุชุ ูู ูุดฺฉู ููุทู ุฏุฑ ฺฉุฏ C++ ุง ูพุงุชูู ุดูุง.

ุฎูุงุตู ูุดฺฉู:

ฺฉุฏ load_inline (ฺฉู ูุณุฆูู ฺฉุงููพุงู ฺฉุฑุฏู ฺฉุฏ C++/CUDA ุดูุงุณุช) ูุชูุงูุณุชู ูฺ ฺฉุงุฑุช ฺฏุฑุงูฺฉ (GPU) ุณุงุฒฺฏุงุฑ ุจุง CUDA ุฑุง ุฏุฑ ูุญุท ุดูุง ูพุฏุง ฺฉูุฏ.

ุชูุถุญ ุฏูู ุฎุทุง:

    ููุช load_inline ุฑุง ุตุฏุง ูโุฒูุฏุ ูพุงุชูุฑฺ ุชูุงุด ูโฺฉูุฏ ฺฉุฏ CUDA (.cu) ุดูุง ุฑุง ฺฉุงููพุงู ฺฉูุฏ.

    ุจุฑุง ฺฉุงููพุงู CUDAุ ูพุงุชูุฑฺ ุจุงุฏ ุจุฏุงูุฏ ฺฉู ฺฉุฏ ุฑุง ุจุฑุง ฺู "ูุนูุงุฑ" (Architecture) ุงุฒ GPU ฺฉุงููพุงู ฺฉูุฏ. (ูุซูุงู sm_75 ุจุฑุง T4 ุง sm_86 ุจุฑุง A100).

    ูพุงุชูุฑฺ ุงู ูุณุช ูุนูุงุฑโูุง (arch_list) ุฑุง ุจุง ุจุฑุฑุณ GPUูุง ูุนุงู ุฏุฑ ูุญุท ุดูุง ุจู ุฏุณุช ูโุขูุฑุฏ.

    ุฏุฑ ูุญุท ุดูุงุ ูฺ GPU ูุนุงู ูพุฏุง ูฺฉุฑุฏู ุงุณุช. ุฏุฑ ูุชุฌูุ arch_list ฺฉ ูุณุช ุฎุงู ([]) ุดุฏู ุงุณุช.

    ฺฉุฏ ูพุงุชูุฑฺ (ุฏุฑ ุฎุท _get_cuda_arch_flags) ุชูุงุด ูโฺฉูุฏ ุจู ุขุฎุฑู ุขุชู ูุณุช (arch_list[-1]) ุฏุณุชุฑุณ ูพุฏุง ฺฉูุฏ ุชุง +PTX ุฑุง ุจู ุขู ุงุถุงูู ฺฉูุฏ.

    ฺูู ูุณุช ุฎุงู ุงุณุชุ ุฏุณุชุฑุณ ุจู arch_list[-1] ุฎุทุง IndexError: list index out of range ุฑุง ุงุฌุงุฏ ูโฺฉูุฏ.

ุฑุงูโุญู:

ุดูุง ุจุงุฏ ุงู ฺฉุฏ ุฑุง ุฏุฑ ฺฉ ูุญุท ุงุฌุฑุง ฺฉูุฏ ฺฉู GPU ูุนุงู ุฏุงุดุชู ุจุงุดุฏ.

ุงฺฏุฑ ุฏุฑ Google Colab ูุณุชุฏ:

    ุจู ููู "Runtime" (ุฏุฑ ูุณุฎูโูุง ูุงุฑุณ ยซุฒูุงู ุงุฌุฑุงยป) ุจุฑูุฏ.

    ฺฏุฒููโ "Change runtime type" (ยซุชุบุฑ ููุน ุฒูุงู ุงุฌุฑุงยป) ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ.

    ุฏุฑ ุจุฎุด "Hardware accelerator" (ยซุดุชุงุจโุฏููุฏู ุณุฎุชโุงูุฒุงุฑยป)ุ None ุฑุง ุจู GPU (ูุซูุงู T4) ุชุบุฑ ุฏูุฏ.

    ุณูพุณ "Save" (ยซุฐุฎุฑูยป) ฺฉูุฏ ู ููุชโุจูฺฉ ุฑุง ุงุฒ ุงุจุชุฏุง ุงุฌุฑุง ฺฉูุฏ (ุจุงุฏ torch ู... ุฏูุจุงุฑู ูุตุจ ุดููุฏ).

ฺุฑุง ุฏูุช ูุง ูุฏู ูุง ุงูุทูุฑ ูุณุชุ

ุฏูู ุงูฺฉู ุฏูุช (Accuracy) ุชูุงู ูุฏูโูุง ุฑู ฑฑ.ณตูช ููู ุดุฏู ู ุฎุทุง (Loss) ฺฉุงูุด ูพุฏุง ููโฺฉูุฏุ ุงุฑุงุฏ ุฏุฑ ฺฉุฏ CUDA ุง ููุทู backpropagation ุดูุง ูุณุช (ฺูู ูุฏู ุงุณุชุงูุฏุงุฑุฏ PyTorch ูู ููู ุฑูุชุงุฑ ุฑุง ุฏุงุฑุฏ).

ูุดฺฉู ุงุตู ุฏุฑ ูุนูุงุฑ ูุฏู (Model Architecture) ุชุนุฑู ุดุฏู ุฏุฑ ฺฉูุงุณโูุง CNNBaseline ู CNNFused ุงุณุช.

ฑ. ุงุดุชุฑุงฺฉ ูุฒูโูุง (Weight Sharing) ุฏุฑ ุญููู

ุฏุฑ ูุชุฏ __init__ุ ุดูุง ุชููุง ฺฉ ูุงู ฺฉุงููููุดู ุชุนุฑู ฺฉุฑุฏูโุงุฏ:
Python

self.conv = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)

ุงูุง ุฏุฑ forwardุ ููุงู ฺฉ ูุงู ุฑุง ุฏุฑ ฺฉ ุญููู ุชฺฉุฑุงุฑ ูโฺฉูุฏ:
Python

for _ in range(self.num_convs):
    x = F.relu(self.conv(x)) # <--- ูุดฺฉู ุงูุฌุงุณุช

ุงู ฺฉุงุฑ ุจุงุนุซ ูโุดูุฏ ฺฉู ุฏุฑ ุชูุงู ูุฑุงุญู (ูุซูุงู ต ูุฑุญูู)ุ ุฏููุงู ุงุฒ ฺฉ ูุฌููุนู ูุฒู ู ุจุงุงุณ ฺฉุณุงู ุงุณุชูุงุฏู ุดูุฏ (ูุซู ฺฉ ุดุจฺฉู RNN). ุงู ูุญุฏูุฏุช ุดุฏุฏ ุจุงุนุซ ูโุดูุฏ ุดุจฺฉู ูุชูุงูุฏ ูฺฺฏโูุง ุณูุณููโูุฑุงุชุจ (Hierarchical features) ุฑุง ุงุฏ ุจฺฏุฑุฏ. ุดุจฺฉู ูุฌุจูุฑ ุงุณุช ฺฉ ููุชุฑ ณรณ ูพุฏุง ฺฉูุฏ ฺฉู ุงฺฏุฑ ต ุจุงุฑ ูพุดุช ุณุฑ ูู ุฑู ุชุตูุฑ ุงุนูุงู ุดูุฏุ ุขู ุฑุง ุจู ุนุฏุฏ ุตุญุญ ุชุจุฏู ฺฉูุฏ! ุงู ุชูุฑุจุงู ุบุฑููฺฉู ุงุณุช.

ฒ. ูุญุฏูุฏุช ฺฉุงูุงู (Channel Bottleneck)

ุดูุง C_out=1 ูุฑุงุฑ ุฏุงุฏูโุงุฏ. ุดุจฺฉูโูุง ฺฉุงููููุดู ูุฏุฑุช ุฎูุฏ ุฑุง ุงุฒ ุงูุฒุงุด ุชุนุฏุงุฏ ฺฉุงูุงูโูุง (ูุซูุงู ฑ -> ณฒ -> ถด) ูโฺฏุฑูุฏ ุชุง ูฺฺฏโูุง ูุฎุชูู (ูุจูโูุงุ ุจุงูุชโูุง ู ...) ุฑุง ุงุณุชุฎุฑุงุฌ ฺฉููุฏ. ุฒูุงู ฺฉู C_out=1 ุงุณุชุ ุชูุงู ุงุทูุงุนุงุช ุชุตูุฑ ุจุงุฏ ุชููุง ุฏุฑ ฺฉ ฺฉุงูุงู ูุดุฑุฏู ุดูุฏ ฺฉู ุธุฑูุช ุงุฏฺฏุฑ ูุฏู ุฑุง ุจู ุดุฏุช ฺฉุงูุด ูโุฏูุฏ.

ูฺฉุชู ููู: ูโุฏุงูู ฺฉู ฺฉุฑูู CUDA ุดูุง ุจู ฺฏูููโุง ุทุฑุงุญ ุดุฏู ฺฉู ููุท C_out=1 ุฑุง ูพุดุชุจุงู ูโฺฉูุฏ (ุทุจู ุตูุฑุช ุณูุงู)ุ ุงูุง ุงู ูุญุฏูุฏุช ุจุงุนุซ ูโุดูุฏ ูุฏู ูุชูุงูุฏ ุฑู ุฏุชุงุณุช ูพฺุฏูโุง ูุซู MNIST ุงุฏฺฏุฑ ููุงุณุจ ุฏุงุดุชู ุจุงุดุฏ.

ูุชุฌูโฺฏุฑ

    ุฑูุชุงุฑ ูุฏู ุทุจุน ุงุณุช: ฺูู ูุฏู ุธุฑูุช ุงุฏฺฏุฑ ูุฏุงุฑุฏุ ููุท ุญุฏุณ ุชุตุงุฏู ูโุฒูุฏ (ุจุฑุง ฑฐ ฺฉูุงุณ ุญุฏูุฏ ฑฐูช + ฺฉู ุจุงุงุณ ุฏุชุงุณูุช = ฑฑ.ณตูช).

    ฺฉุฏ CUDA ุดูุง ุฏุฑุณุช ุงุณุช: ฺูู ุฎุฑูุฌ ูุฏู Fused ุฏููุงู ุจุง Baseline ฺฉ ุงุณุชุ ุนู ฺฉุฑูู ุดูุง ูุญุงุณุจุงุช ุฑุง ุฏุฑุณุช ุงูุฌุงู ูโุฏูุฏ. ูุฏู ุงู ุชูุฑู ุงุญุชูุงูุงู ููุงุณู ุณุฑุนุช ุจูุฏู ุงุณุชุ ูู ุฑุณุฏู ุจู ุฏูุช ุจุงูุง.

ุฑุงู ุญู (ุงฺฏุฑ ูุฏู ุงูุฒุงุด ุฏูุช ุจูุฏ): ุจุงุฏ ุจู ุฌุง ฺฉ self.convุ ุงุฒ nn.ModuleList ุงุณุชูุงุฏู ูโฺฉุฑุฏุฏ ุชุง ูุฒูโูุง ูุฑ ูุงู ุฌุฏุงฺฏุงูู ุจุงุดุฏ:
Python

# ุงู ฺฉุฏ ููุท ุจุฑุง ูุซุงู ุงุณุช ู ุจุง ฺฉุฑูู ุชฺฉ ฺฉุงูุงูู ุดูุง ููฺูุงู ุถุนู ุนูู ุฎูุงูุฏ ฺฉุฑุฏ
self.convs = nn.ModuleList([nn.Conv2d(1, 1, 3, 1) for _ in range(num_convs)])

def forward(self, x):
    for conv in self.convs:
        x = F.relu(conv(x))
    ...

    @staticmethod

    def backward(ctx, grad_output):

        x, weight, bias, y = ctx.saved_tensors


        # grad ReLU

        mask = (y > 0).to(grad_output.dtype)

        grad_z = grad_output * mask


        # dL/dx

        grad_x = torch.nn.grad.conv2d_input(

            x.shape, weight, grad_z, padding=1

        )

        # dL/dW

        grad_weight = torch.nn.grad.conv2d_weight(

            x, weight.shape, grad_z, padding=1

        )

        # dL/db

        grad_bias = grad_z.sum(dim=[0, 2, 3])


        return grad_x, grad_weight, grad_bias



ุงูู ฺฉุงูู ู ุจุง ุฌุฒุงุช ุฑุงุถ ุชูุถุญ ุจุฏู ฺฉุงูู

ุงู ูุชุฏ backward ูุณุฆูู ูุญุงุณุจู ฺฏุฑุงุฏุงูโูุง ุจุฑุง ุนููุงุช ุชุฑฺฉุจ Convolution + ReLU ุงุณุช. ูุฏู ููุง ุฏุฑ ุงูุฌุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง (Chain Rule) ุจุฑุง ูุญุงุณุจู ูุดุชู ุชุงุจุน ูุฒูู (Loss) ูุณุจุช ุจู ูุฑูุฏโูุง (x)ุ ูุฒูโูุง (W) ู ุจุงุงุณ (b) ุงุณุช.

ุจุงุฏ ูุฑุถ ฺฉูู L ุชุงุจุน ูุฒูู ููุง ุจุงุดุฏ. ูุฑูุฏ ุชุงุจุน backwardุ ูุชุบุฑ grad_output ุงุณุช ฺฉู ููุงู ูุดุชู ูุฒูู ูุณุจุช ุจู ุฎุฑูุฌ ูุงู ุงุณุช:
grad_output=โyโLโ

ุฏุฑ ุงุฏุงููุ ุฎุท ุจู ุฎุท ููุฑุงู ุจุง ุฑุงุถุงุช ูพุดุช ุขู ุชูุถุญ ูโุฏูู:

ฑ. ุจุงุฒุงุจ ูุชุบุฑูุง

Python

x, weight, bias, y = ctx.saved_tensors

ุงู ุชูุณูุฑูุง ุฏุฑ ูุฑุญูู forward ุฐุฎุฑู ุดุฏู ุจูุฏูุฏ. ุจุฑุง ูุญุงุณุจู ูุดุชูุงุชุ ูุง ุจู ูุฑูุฏ ุงุตู (x)ุ ูุฒูโูุง (W) ู ุฎุฑูุฌ ูุนุงูโุณุงุฒ ุดุฏู (y) ูุงุฒ ุฏุงุฑู.

ฒ. ูุญุงุณุจู ฺฏุฑุงุฏุงู ReLU (ุจุฎุด grad_z)

Python

mask = (y > 0).to(grad_output.dtype)
grad_z = grad_output * mask

ุฑุงุถุงุช: ุนููุงุช Forward ุจู ุตูุฑุช ุฒุฑ ุจูุฏ:
z=Conv(x,W)+b
y=ReLU(z)=max(0,z)

ูุง โyโLโ ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โzโLโ ุฑุง ูุญุงุณุจู ฺฉูู (ูุดุชู ูุจู ุงุฒ ุงุนูุงู ReLU). ุทุจู ูุงุนุฏู ุฒูุฌุฑูโุง:
โzโLโ=โyโLโโโzโyโ

ูุดุชู ุชุงุจุน ReLU ุจู ุตูุฑุช ุฒุฑ ุงุณุช:
โzโyโ={10โif z>0if zโค0โ

ุฏุฑ ฺฉุฏ:

    mask ููุงู โzโyโ ุงุณุช. ุฌุง ฺฉู ุฎุฑูุฌ ูุซุจุช ุจูุฏูุ ฑ ู ุฏุฑ ุบุฑ ุงู ุตูุฑุช ฐ ูโุดูุฏ.

    grad_z ููุงู โzโLโ ุงุณุช ฺฉู ุญุงุตู ุถุฑุจ ููุทูโุง (element-wise) ฺฏุฑุงุฏุงู ุฎุฑูุฌ ุฏุฑ ูุงุณฺฉ ุงุณุช.

ณ. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฑูุฏ x (ุจุฎุด dL/dx)

Python

grad_x = torch.nn.grad.conv2d_input(
    x.shape, weight, grad_z, padding=1
)

ุฑุงุถุงุช: ูุง โzโLโ ุฑุง ุฏุงุฑู (ููุงู grad_z) ู ูโุฎูุงูู โxโLโ ุฑุง ุจุงุจู. ุฑุงุจุทู ุจู z ู x ฺฉ ฺฉุงููููุดู ุงุณุช: z=xโW.

ุฏุฑ ุฑุงุถุงุช ฺฉุงููููุดูุ ูุดุชู ูุณุจุช ุจู ูุฑูุฏ (grad_x) ุจุฑุงุจุฑ ุงุณุช ุจุง ฺฉุงููููุดู ฺฏุฑุงุฏุงู ุฎุฑูุฌ ุจุง ูุฒูโูุง "ุจุฑุนฺฉุณ ุดุฏู" (Rotated Weights). ุงู ุนููุงุช ุงุบูุจ ุฏุฑ ูพุงุฏูโุณุงุฒโูุง ุจู ุนููุงู Transposed Convolution ุง Deconvolution ุดูุงุฎุชู ูโุดูุฏ.
โxโLโ=grad_zโWrot180

ุชุงุจุน torch.nn.grad.conv2d_input ุฏููุงู ููู ุนููุงุช ุฑุงุถ ูพฺุฏู ุฑุง ุจู ุตูุฑุช ุจููู ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ. ุงู ุชุงุจุน ูุญุงุณุจู ูโฺฉูุฏ ฺฉู ุงฺฏุฑ ุฎุฑูุฌ grad_z ุจุงุดุฏ ู ูุฒูโูุง weight ุจุงุดูุฏุ ูุฑูุฏ x ุจุงุฏ ฺู ฺฏุฑุงุฏุงู ุฏุงุดุชู ุจุงุดุฏ ุชุง ุงู ุชุบุฑุงุช ุงุฌุงุฏ ุดูุฏ.

ด. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฒูโูุง W (ุจุฎุด dL/dW)

Python

grad_weight = torch.nn.grad.conv2d_weight(
    x, weight.shape, grad_z, padding=1
)

ุฑุงุถุงุช: ูุง โzโLโ ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โWโLโ ุฑุง ุจุงุจู. ุจุงุฒ ูู ุฑุงุจุทู z=xโW ุงุณุช.

ุฏุฑ ุฑุงุถุงุชุ ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฒูโูุง ุจุฑุงุจุฑ ุงุณุช ุจุง ฺฉุงููููุดู (ุง Cross-correlation) ุจู "ูุฑูุฏ x" ู "ฺฏุฑุงุฏุงู ุฎุฑูุฌ z". ุจู ุนุจุงุฑุช ุฏฺฏุฑุ ูุง ูโุฎูุงูู ุจุจูู ฺฉุฏุงู ุจุฎุดโูุง ุชุตูุฑ ูุฑูุฏ (x) ุฏุฑ ุงุฌุงุฏ ุฎุทุง ููุฌูุฏ ุฏุฑ grad_z ููุด ุฏุงุดุชูโุงูุฏ.
โWโLโ=xโgrad_z

ุชุงุจุน torch.nn.grad.conv2d_weight ุงู ูุญุงุณุจู ุฑุง ุงูุฌุงู ูโุฏูุฏ. ูุฑูุฏ ุงุตู (x) ู ฺฏุฑุงุฏุงู ุฎุฑูุฌ (grad_z) ุฑุง ูโฺฏุฑุฏ ู ูุดุฎุต ูโฺฉูุฏ ฺฉู ูุฑ ูุฒู ุฏุฑ ฺฉุฑูู ฺูุฏุฑ ุจุงุฏ ุชุบุฑ ฺฉูุฏ.

ต. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ุจุงุงุณ b (ุจุฎุด dL/db)

Python

grad_bias = grad_z.sum(dim=[0, 2, 3])

ุฑุงุถุงุช: ุฏุฑ ุนููุงุช ฺฉุงููููุดูุ ฺฉ ููุฏุงุฑ ุจุงุงุณ (Scalar) ุจู ุชูุงู ูพฺฉุณูโูุง ฺฉ Feature Map (ฺฉุงูุงู ุฎุฑูุฌ) ุงุถุงูู ูโุดูุฏ. ูุฑููู Forward ุจุฑุง ฺฉ ูพฺฉุณู ุฎุงุต:
zn,c,h,wโ=(ConvOutput)n,c,h,wโ+bcโ

ฺูู bcโ (ุจุงุงุณ ฺฉุงูุงู c) ุฏุฑ ูุญุงุณุจู ุชูุงู ูพฺฉุณูโูุง ุขู ฺฉุงูุงู (ุฏุฑ ุชูุงู ุนุฑุถ Wุ ุงุฑุชูุงุน H ู ุชูุงู ูููููโูุง ุฏุณุชู N) ุดุฑฺฉุช ุฏุงุดุชู ุงุณุชุ ุทุจู ูุงุนุฏู ูุดุชูโฺฏุฑ ุฌูุนุ ฺฏุฑุงุฏุงู ุขู ุจุฑุงุจุฑ ุงุณุช ุจุง ูุฌููุน ฺฏุฑุงุฏุงูโูุง ุชูุงู ูพฺฉุณูโูุง ฺฉู ุขู ุจุงุงุณ ุฑุง ุฏุฑุงูุช ฺฉุฑุฏูโุงูุฏ.
โbcโโLโ=n=0โNโ1โh=0โHโ1โw=0โWโ1โโzn,c,h,wโโLโ

ุฏุฑ ฺฉุฏ:

    dim=[0, 2, 3] ุงุดุงุฑู ุจู ุงุจุนุงุฏ N (Batch), H (Height), W (Width) ุฏุงุฑุฏ.

    ูุง ุฑู ุงู ุงุจุนุงุฏ ุฌูุน ูโุฒูู ู ุชููุง ุจูุนุฏ ุจุงูโูุงูุฏูุ ุจูุนุฏ ฺฉุงูุงู (Channel) ุง ููุงู ุฏูุงูุณูู ฑ ุงุณุช ฺฉู ููโุงูุฏุงุฒู ุจุฑุฏุงุฑ ุจุงุงุณ ูโุจุงุดุฏ.

ุฎูุงุตู

ุงู ฺฉุฏ ูุฑุขูุฏ Backward Pass ุฑุง ุจุง ุดฺฉุณุชู ุนููุงุช ุจู ุณู ุจุฎุด ุงุตู ุงูุฌุงู ูโุฏูุฏ:

    ุนุจูุฑ ฺฏุฑุงุฏุงู ุงุฒ ReLU (ุจุง ุตูุฑ ฺฉุฑุฏู ฺฏุฑุงุฏุงูู ููุงุฏุฑ ููู).

    ุงูุชุดุงุฑ ฺฏุฑุงุฏุงู ุจู ูุฑูุฏ (ุจุฑุง ูุงูโูุง ูุจู).

    ูุญุงุณุจู ฺฏุฑุงุฏุงู ุจุฑุง ูพุงุฑุงูุชุฑูุง ูุงู (ูุฒู ู ุจุงุงุณ) ุฌูุช ุขูพุฏุช ุดุฏู ุชูุณุท Optimizer.

ุงูู ุจุดุชุฑ ุชูุถุญ ุจุฏู

mask = (y > 0).to(grad_output.dtype) 

ุงู ุฎุท ฺฉุฏ ฺฉ ุงุฒ ูููโุชุฑู ู ุงุณุงุณโุชุฑู ูุฑุงุญู ุฏุฑ ูุฑุงูุฏ ูพุณโุงูุชุดุงุฑ ฺฏุฑุงุฏุงู (Backpropagation) ุฏุฑ ุดุจฺฉูโูุง ุนุตุจ ุงุณุช. ุงู ุฎุท ุฏููุงู ูุดุชู ุชุงุจุน ูุนุงูโุณุงุฒ ReLU (max(0,z)) ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ.
Python

mask = (y > 0).to(grad_output.dtype)

ุจุงุฏ ุงู ุนุจุงุฑุช ุฑุง ุจู ุฏู ุจุฎุด ุชูุณู ู ุจุฑุฑุณ ฺฉูู:

ฑ. ุจุฎุด ุจูู (Boolean) ู ููุทู ุฑุงุถ: (y > 0)

ุงู ุจุฎุด ูุณุชููุงู ูุดุชู ุชุงุจุน ReLU ุฑุง ุฏุฑ ูุงูุจ ฺฉ ูุงุณฺฉ ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ.

ุชุงุจุน ReLU ู ูุดุชู ุขู:

ุนููุงุช Forward ุฏุฑ ูุงู ูุนุงูโุณุงุฒ ReLU ุจู ุตูุฑุช ุฒุฑ ุงุณุช:
y=ReLU(z)=max(0,z)

ฺฉู ุฏุฑ ุขู z ูุฑูุฏ ReLU (ููุงู ุฎุฑูุฌ ุนููุงุช Conv + Bias) ู y ุฎุฑูุฌ ุขู ุงุณุช.

ูุดุชู ุชุงุจุน y ูุณุจุช ุจู ูุฑูุฏ z (ุนู โzโyโ) ุจู ุงู ุตูุฑุช ุชุนุฑู ูโุดูุฏ:
โzโyโ={10โif z>0if zโค0โ

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    y: ููุงู ุฎุฑูุฌ ReLU ุงุณุช ฺฉู ุฏุฑ ูุฑุญูู forward ูุญุงุณุจู ุดุฏู ู ุฏุฑ ctx.saved_tensors ุฐุฎุฑู ุดุฏู ุงุณุช.

    (y > 0): ุงู ฺฉ ุนููฺฏุฑ ููุงุณูโุง ุนูุตุฑุจูโุนูุตุฑ (Element-wise) ุงุณุช ฺฉู ฺฉ ุชูุณูุฑ ุฌุฏุฏ (ุจู ูุงู ูุงุณฺฉ) ุจู ูุง ูโุฏูุฏ.

        ุงฺฏุฑ ููุฏุงุฑ ูพฺฉุณู ุฏุฑ y ูุซุจุช ุจุงุดุฏ (y>0)ุ ููุฏุงุฑ ูุชูุงุธุฑ ุฏุฑ ูุงุณฺฉ True (ุตุญุญ) ุฎูุงูุฏ ุจูุฏ. (ุงู ุญุงูุช โzโyโ=1 ุฑุง ูุดุงู ูโุฏูุฏ.)

        ุงฺฏุฑ ููุฏุงุฑ ูพฺฉุณู ุฏุฑ y ุตูุฑ ุง ููู ุจุงุดุฏ (yโค0)ุ ููุฏุงุฑ ูุชูุงุธุฑ ุฏุฑ ูุงุณฺฉ False (ุบูุท) ุฎูุงูุฏ ุจูุฏ. (ุงู ุญุงูุช โzโyโ=0 ุฑุง ูุดุงู ูโุฏูุฏ.)

ุงู ุชูุณูุฑู ุจูู (True/False) ุนููุงู ููุงู ูุดุชู ุงุณุชุ ุงูุง ุจุงุฏ ุจู ููุน ุฏุงุฏูโุง ุนุฏุฏ ุชุจุฏู ุดูุฏ.

ฒ. ุจุฎุด ุชุจุฏู ููุน ุฏุงุฏู: .to(grad_output.dtype)

ุงู ุจุฎุด ูุณุฆููุช ุชุจุฏู ููุงุฏุฑ ุจูู (True/False) ุจู ุงุนุฏุงุฏ ููุฒ ุดูุงูุฑ (Float) ููุงุณุจ ุจุฑุง ูุญุงุณุจุงุช ฺฏุฑุงุฏุงู ุฑุง ุจุฑ ุนูุฏู ุฏุงุฑุฏ.

ฺุฑุง ุชุจุฏู ููุน ุฏุงุฏู ูุงุฒู ุงุณุชุ

    ูุญุงุณุจุงุช ุนุฏุฏ: ุชูุณูุฑูุง ุจูู ุจุฑุง ุถุฑุจ ู ุฌูุน ูุณุชูู ุฏุฑ ุนููุงุช ูุดุชูโฺฏุฑ ููุงุณุจ ูุณุชูุฏ. ูพุงุชูุฑฺ ููุงุฏุฑ True ุฑุง ุจู 1.0 ู False ุฑุง ุจู 0.0 ุชุจุฏู ูโฺฉูุฏ ุชุง ุจู ูุฏู ุฑุงุถ ุจุฑุณู.

    ฺฉูพุงุฑฺฺฏ ููุน ุฏุงุฏู: ุฏุฑ ูพุงุชูุฑฺุ ุชูุงู ฺฏุฑุงุฏุงูโูุง ฺฉุณุงู (ุฏุฑ ุงู ููุฑุฏุ grad_output) ุจุงุฏ ุงุฒ ฺฉ ููุน ุฏุงุฏู (ูุนูููุงู torch.float32 ุง torch.float64) ุจุงุดูุฏ.

        grad_output.dtype ููุน ุฏุงุฏู ฺฏุฑุงุฏุงู ูุฑูุฏ ุฑุง ูุดุฎุต ูโฺฉูุฏ.

        .to(...) ุชุถูู ูโฺฉูุฏ ฺฉู ุชูุณูุฑ mask ุฌุฏุฏ ูู ุฏููุงู ุงุฒ ููุงู ููุน ุฏุงุฏู ุจุงุดุฏ.

ูุชุฌู ููุง:

ูุชุบุฑ mask ุงฺฉููู ฺฉ ุชูุณูุฑ ุจุง ุงุจุนุงุฏ ฺฉุณุงู ุจุง y ู grad_output ุงุณุช ฺฉู ููุท ุดุงูู ุฏู ููุฏุงุฑ ุนุฏุฏ ุงุณุช:

    1.0: ุจุฑุง ูพฺฉุณูโูุง ฺฉู ุฏุฑ ูุฑุญูู Forward ูุนุงู ุจูุฏูุฏ ู ฺฏุฑุงุฏุงู ุจุงุฏ ุงุฒ ุขูโูุง ุนุจูุฑ ฺฉูุฏ.

    0.0: ุจุฑุง ูพฺฉุณูโูุง ฺฉู ุฏุฑ ReLU ุฎุงููุด ุดุฏูุฏ (ููุฏุงุฑ ุตูุฑ ุง ููู ุฏุงุดุชูุฏ) ู ฺฏุฑุงุฏุงู ูุจุงุฏ ุจู ูุงูโูุง ูุจู ุจุงุฒฺฏุฑุฏุฏ (ูุณุฏูุฏ ฺฉุฑุฏู ุฌุฑุงู ฺฏุฑุงุฏุงู).

ุฏุฑ ุฎุท ุจุนุฏ (grad_z = grad_output * mask)ุ ุงู ูุงุณฺฉ ุจู ุณุงุฏฺฏ ฺฏุฑุงุฏุงูโูุง ูุฑุจูุท ุจู ูพฺฉุณูโูุง ุฎุงููุด ุดุฏู ุฑุง ุจุง ุถุฑุจ ุฏุฑ ุตูุฑุ ุงุฒ ุจู ูโุจุฑุฏ.

ReLU ฺฉู ุงุตูุง ูุดุชู ูพุฐุฑ ูุณุช ฺู ู ฺฉููุ

ุงู ฺฉ ุณุคุงู ุจุณุงุฑ ุฏูู ู ุนูู ุฏุฑ ููุฑุฏ ุฑุงุถุงุช ุดุจฺฉูโูุง ุนุตุจ ุงุณุช ู ุจู ูุณุชูโ ุงุตู ูพุงุฏูโุณุงุฒ ReLU ุฏุฑ ุนูู ูโูพุฑุฏุงุฒุฏ.

ุญู ุจุง ุดูุงุณุช: ุงุฒ ูุธุฑ ุฑุงุถ ูุญุถุ ุชุงุจุน ReLU ุฏุฑ ููุทู z=0 ูุดุชูโูุงูพุฐุฑ ุงุณุชุ ุฒุฑุง ูุดุชู ฺูพ (limhโ0โโhf(0+h)โf(0)โ=0) ู ูุดุชู ุฑุงุณุช (limhโ0+โhf(0+h)โf(0)โ=1) ุจุฑุงุจุฑ ูุณุชูุฏ.

ุฑุงูโุญู ุฏุฑ ุงุฏฺฏุฑ ุนูู: ุงุณุชูุงุฏู ุงุฒ ูุดุชู ุฌุฒุฆ (Subgradient)

ุฏุฑ ุงุฏฺฏุฑ ุนูู ู ุจูููโุณุงุฒุ ูุง ุจู ุฌุง ูุดุชู (Derivative)ุ ุงุฒ ููููู ุจู ูุงู ูุดุชู ุฌุฒุฆ (Subgradient) ุง ุจู ุทูุฑ ุฑุงุฌโุชุฑุ ุดุจู ูุดุชู (Pseudo-derivative) ุงุณุชูุงุฏู ูโฺฉูู.

ููฺฏุงู ฺฉู ฺฉ ุชุงุจุน ูุญุฏุจ (Convex) ุฏุฑ ฺฉ ููุทู ูุดุชูโูุงูพุฐุฑ ุงุณุชุ ูุฌููุนู ูุดุชูุงุช ุฌุฒุฆ ุขู ุดุงูู ุชูุงู ููุงุฏุฑ ุงุณุช ฺฉู ุจู ูุดุชู ฺูพ ู ูุดุชู ุฑุงุณุช ูุฑุงุฑ ูโฺฏุฑูุฏ.

ุชุนุฑู ูุดุชู ุฌุฒุฆ ReLU ุฏุฑ z=0:

    ุจุฑุง z>0: ูุดุชู 1 ุงุณุช.

    ุจุฑุง z<0: ูุดุชู 0 ุงุณุช.

    ุจุฑุง z=0: ูุดุชู ุฌุฒุฆ ูุฑ ููุฏุงุฑ ุฏุฑ ุจุงุฒู [0,1] ูโุชูุงูุฏ ุจุงุดุฏ.

ูพุงุฏูโุณุงุฒ ุนูู (ูุนูููุงู ุฏุฑ PyTorch/TensorFlow):

ุฏุฑ ุนููุ ูพุงุฏูโุณุงุฒโูุง ุจุงุฏ ฺฉ ุงุฒ ููุงุฏุฑ ูุฌุงุฒ ุฑุง ุจุฑุง z=0 ุงูุชุฎุงุจ ฺฉููุฏ. ุชูุฑุจุงู ุชูุงู ูุฑูโูุฑฺฉโูุง ุงุฏฺฏุฑ ุนูู ุงุฒ ฺฉ ูุงููู ุณุงุฏู ูพุฑู ูโฺฉููุฏ:
โzโyโโz=0โ=0

ุจู ุนุจุงุฑุช ุฏฺฏุฑ:

    ุงฺฏุฑ z>0: ูุดุชู = 1

    ุงฺฏุฑ zโค0: ูุดุชู = 0

ฺุฑุง 0 ุงูุชุฎุงุจ ูโุดูุฏุ

    ุณุงุฏฺฏ: ุงูุชุฎุงุจ 0 ุงุฒ ูุธุฑ ูพุงุฏูโุณุงุฒ ุณุงุฏูโุชุฑู ฺฏุฒูู ุงุณุช ู ุจุง ููุทู ยซููุช ููุฑ ุฑูุดู ูุณุชุ ฺฏุฑุงุฏุงู ุตูุฑ ุงุณุชยป ููุฎูุงู ุฏุงุฑุฏ.

    ุนููฺฉุฑุฏ: ุฏุฑ ุนููุ ุงูุชุฎุงุจ 0 ุง 1 ุจุฑุง ฺฉ ููุทู ูููุฑุฏ (ฺฉู ุชุนุฏุงุฏ ุขู ุฏุฑ ููุงุณู ุจุง ููููโูุง ูพฺฉุณู ุฏฺฏุฑ ูุงฺุฒ ุงุณุช) ูฺ ุชุฃุซุฑ ูุญุณูุณ ุจุฑ ููฺฏุฑุง (Convergence) ู ุฏูุช ููุง ูุฏู ูุฏุงุฑุฏ.

    ุงุฌุชูุงุจ ุงุฒ ูุงูพุงุฏุงุฑ: ุงฺฏุฑฺู 1 ูุฒ ฺฉ ุงูุชุฎุงุจ ุฑุงุถ ูุนุชุจุฑ ุงุณุชุ ุงูุชุฎุงุจ 0 ุฏุฑ ุนูู ุจู ุฏูู ุฌููฺฏุฑ ุงุฒ ยซุงูุชูุงู ููุฒยป ู ุญูุธ ูพุงุฏุงุฑ ุนุฏุฏ (Numerical Stability) ุฏุฑ ูุฏูโูุง ุจุฒุฑฺฏโุชุฑ ุชุฑุฌุญ ุฏุงุฏู ูโุดูุฏ.

ุงุฑุชุจุงุท ุจุง ฺฉุฏ ุดูุง:

ฺฉุฏ ุดูุง ุฏููุงู ุงู ุฑูฺฉุฑุฏ ุนูู ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ:
Python

mask = (y > 0).to(grad_output.dtype)

ุงฺฏุฑ y (ฺฉู ููุงู max(0,z) ุงุณุช) ุจุฑุงุจุฑ ุจุง 0 ุจุงุดุฏุ ุจู ุงู ูุนู ุงุณุช ฺฉู zโค0 ุจูุฏู ุงุณุช.

    ุงฺฏุฑ y=0 ุจุงุดุฏ: ุดุฑุท (y>0) ุจุฑุงุจุฑ ุจุง False ูโุดูุฏ ู ูุงุณฺฉ ููุฏุงุฑ 0.0 ุฑุง ูโฺฏุฑุฏ.

    ุงฺฏุฑ y>0 ุจุงุดุฏ: ุดุฑุท (y>0) ุจุฑุงุจุฑ ุจุง True ูโุดูุฏ ู ูุงุณฺฉ ููุฏุงุฑ 1.0 ุฑุง ูโฺฏุฑุฏ.

ูุชุฌู: ุจุง ุงุณุชูุงุฏู ุงุฒ ุงู ุดุจู ูุดุชู ุนููุ ูุณุฆูู ูุดุชูโูุงูพุฐุฑ ุฏุฑ z=0 ุจู ุตูุฑุช ููุซุฑ ุญู ูโุดูุฏ ู ุงูฺฏูุฑุชู ูพุณโุงูุชุดุงุฑ ุจุฏูู ูุดฺฉู ฺฉุงุฑ ูโฺฉูุฏ.

mask = (y > 0).to(grad_output.dtype) 

ุฑู ุงุฒ ูุญุงุธ ฺฉุฏ ูพุงุชูู ุชูุถุญ ุจุฏู

ุจููุ ุญุชูุงู. ุงุฒ ุฏุฏฺฏุงู ูพุงุชูู ู ุณุงุฎุชุงุฑ ุชูุณูุฑูุง PyTorchุ ุงู ุฎุท ฺฉ ุนููุงุช ุจุณุงุฑ ุฑุงุฌ ุงุณุช ฺฉู ุงุฒ ูฺฺฏโูุง ุฎูุฏฺฉุงุฑ ุชุจุฏู ููุน ุฏุงุฏู ุฏุฑ PyTorch ุจูุฑู ูโุจุฑุฏ.

ุชูุถุญ ุฌุฒุก ุจู ุฌุฒุก ฺฉุฏ ูพุงุชูู

Python

mask = (y > 0).to(grad_output.dtype)

ฑ. ูุชุบุฑ y

    ููุน: torch.Tensor (ุชูุณูุฑ ูพุงุชูุฑฺ).

    ูุญุชูุง: ุฎุฑูุฌ ุชุงุจุน ReLU ุงุฒ ูุฑุญูู forward. ุงู ุชูุณูุฑ ููุท ุดุงูู ููุงุฏุฑ ุตูุฑ ู ููุงุฏุฑ ูุซุจุช ุงุณุช (ูุซู tensor([1.5, 0.0, 3.2, 0.0])).

    ูุฏู: ุชุนู ฺฉุฑุฏู ุงู ฺฉู ฺฉุฏุงู ูพฺฉุณูโูุง ูุนุงู (ูุซุจุช) ุจูุฏูุฏ ู ฺฏุฑุงุฏุงู ุจุงุฏ ุงุฒ ุขูโูุง ุนุจูุฑ ฺฉูุฏ.

ฒ. ุนููุงุช ููุงุณูโุง: (y > 0)

    ุนููุงุช: ุงู ฺฉ ุนููุงุช ููุทู (Boolean Comparison) ุงุณุช ฺฉู ุจู ุตูุฑุช ุนูุตุฑุจูโุนูุตุฑ (Element-wise) ุฑู ุชูุณูุฑ y ุงุนูุงู ูโุดูุฏ.

    ุฎุฑูุฌ (ุชูุณูุฑ ูููุช ุจูู): ุงู ุนุจุงุฑุช ฺฉ ุชูุณูุฑ ุฌุฏุฏ ุจุง ููุงู ุงุจุนุงุฏ y ุงุฌุงุฏ ูโฺฉูุฏุ ุงูุง ููุน ุฏุงุฏู ุขู torch.bool ุงุณุช.

        ุงฺฏุฑ ุนูุตุฑ y ูุซุจุช ุจุงุดุฏุ ุฎุฑูุฌ ุฏุฑ ุขู ูุญู True ุฎูุงูุฏ ุจูุฏ.

        ุงฺฏุฑ ุนูุตุฑ y ุตูุฑ ุจุงุดุฏ (ุฏุฑ ReLUุ ูููโูุง ุจู ุตูุฑ ุชุจุฏู ุดุฏูโุงูุฏ)ุ ุฎุฑูุฌ False ุฎูุงูุฏ ุจูุฏ.

y (ุฎุฑูุฌ ReLU)	(y > 0) (ุชูุณูุฑ ุจูู)
[1.5, 0.0, 3.2, 0.0]	[True, False, True, False]

ณ. ูุชุฏ to(...): ุชุจุฏู ููุน ุฏุงุฏู ู ููุฏุงุฑุฏู

    ุนููฺฉุฑุฏ: ูุชุฏ .to() ุชูุณูุฑ ูููุช ุจูู ุฑุง ุจู ููุน ุฏุงุฏูโ ูุดุฎุต ุดุฏู ุชุจุฏู ูโฺฉูุฏ.

    grad_output.dtype: ุงูุ ููุน ุฏุงุฏู (Data Type) ฺฏุฑุงุฏุงู ูุฑูุฏ (ฺฉู ูุนูููุงู torch.float32 ุง torch.float64 ุงุณุช) ุฑุง ุงุฒ ุชูุณูุฑ grad_output ุงุณุชุฎุฑุงุฌ ูโฺฉูุฏ ุชุง ูุทูุฆู ุดูุฏ ฺฉู ฺฏุฑุงุฏุงู ุฏุฑ ุทูู ูุณุฑ ุงุฒ ูุธุฑ ููุน ุฏุงุฏู ุณุงุฒฺฏุงุฑ ุงุณุช.

    ุชุจุฏู ุฎูุฏฺฉุงุฑ (Implicit Conversion): ุฏุฑ ุงู ูุฑุญููุ PyTorch ุชุจุฏู ุฒุฑ ุฑุง ุงูุฌุงู ูโุฏูุฏ:

        True ุชุจุฏู ุจู ููุฏุงุฑ ุนุฏุฏ 1.0 ูโุดูุฏ.

        False ุชุจุฏู ุจู ููุฏุงุฑ ุนุฏุฏ 0.0 ูโุดูุฏ.

(y > 0) (ุจูู)	.to(float) (ุชูุณูุฑ ููุง mask)
[True, False, True, False]	[1.0, 0.0, 1.0, 0.0]

ด. ูุชุบุฑ mask ููุง

    ููุน: torch.Tensor (ุงุฒ ููุน float32 ุง ูุดุงุจู).

    ูุญุชูุง: ฺฉ ูุงุณฺฉ ฺฉุงููุงู ุนุฏุฏ ฺฉู ุงฺฉููู ููุงูุฏูโ ูุดุชู โzโyโ ุงุณุช.

ูุชุฌูโฺฏุฑ ุฏุฑ ุนููุงุช ุจุนุฏ

ุงู ุฎุทุ ุฒููู ุฑุง ุจุฑุง ุฎุท ุจุนุฏ ุขูุงุฏู ูโฺฉูุฏ:
Python

grad_z = grad_output * mask

ุฏุฑ ุงู ุถุฑุจ ุนูุตุฑุจูโุนูุตุฑ:

    ูพฺฉุณูโูุง ูุนุงู (ฺฉู mask ุจุฑุงุจุฑ ุจุง 1.0 ุงุณุช): ฺฏุฑุงุฏุงู ุจุฏูู ุชุบุฑ (grad_output * 1.0) ุจู ุนูุจ ููุชูู ูโุดูุฏ.

    ูพฺฉุณูโูุง ุฎุงููุด (ฺฉู mask ุจุฑุงุจุฑ ุจุง 0.0 ุงุณุช): ฺฏุฑุงุฏุงู ูุณุฏูุฏ ูโุดูุฏ (grad_output * 0.0) ู ููุฏุงุฑ ุตูุฑ ุฑุง ูโฺฏุฑุฏ.

ุจู ุงู ุชุฑุชุจุ ุงู ุฏู ุฎุท ุจุง ููุ ูุดุชู ุชุงุจุน ReLU ุฑุง ุจู ุทูุฑ ฺฉุงุฑุขูุฏ ู ุตุญุญ ุงุฒ ุฏุฏฺฏุงู ูพุงุชูู ู PyTorch ูพุงุฏูโุณุงุฒ ูโฺฉููุฏ.

    def backward(ctx, grad_output):

        x, weight, bias, y = ctx.saved_tensors


        # grad ReLU

        mask = (y > 0).to(grad_output.dtype)

        grad_z = grad_output * mask


        # dL/dx

        grad_x = torch.nn.grad.conv2d_input(

            x.shape, weight, grad_z, padding=1

        )

        # dL/dW

        grad_weight = torch.nn.grad.conv2d_weight(

            x, weight.shape, grad_z, padding=1

        )

        # dL/db

        grad_bias = grad_z.sum(dim=[0, 2, 3])


        return grad_x, grad_weight, grad_bias



ุชุตูุฑ ุดุจฺฉู ุนุตุจ ุฑู ููุงุด ุจุฏู ู ุทุจู ุงู ุชุงุจุน ุจฺฉูุงุฑุฏ ุฏุฑ ูุฑ ูุณูุช ุทุจู ูุงุนุฏู ุฒูุฌุฑู ูุฑููู ูุญุงุณุจู ฺฏุฑุงุฏุงู ุฑุง ุจููุณ

<immersive id="neural_network_backward_pass.png" type="image" caption="ููุงุด ุจุตุฑ ุฌุฑุงู ฺฏุฑุงุฏุงู ุฏุฑ ฺฉ ุดุจฺฉู ุนุตุจ ุณุงุฏู ุจุง ูุงู Conv2d + ReLU"> </immersive>

ุชุตูุฑ ุจุงูุง ฺฉ ุฏุฏ ฺฉู ุงุฒ ุฌุฑุงู ุฑู ุจู ุนูุจ (Backward Pass) ฺฏุฑุงุฏุงู ุฏุฑ ฺฉ ุจุฎุด ุงุฒ ุดุจฺฉู ุนุตุจ ฺฉู ุดุงูู ุนููุงุช Convolutionุ Bias Addition ู ReLU ุงุณุชุ ุงุฑุงุฆู ูโุฏูุฏ. ุงฺฉูููุ ุจุงุฏ ูุฑ ูุฑุญูู ุงุฒ ุชุงุจุน backward ุดูุง ุฑุง ุจุง ุฌุฒุฆุงุช ุฑุงุถ ู ูุงุนุฏู ุฒูุฌุฑูโุง ุชูุถุญ ุฏูู.

ูุฑุถ ูุง ุงู ุงุณุช ฺฉู ุชุงุจุน ูุฒูู ููุง L ูุงู ุฏุงุฑุฏ ู ูุง ูโุฎูุงูู โxโLโุ โWโLโ ู โbโLโ ุฑุง ูุญุงุณุจู ฺฉูู. ููุงูุทูุฑ ฺฉู ูุจูุงู ุงุดุงุฑู ุดุฏุ grad_output ููุงู โyโLโ ุงุณุช.

ุณุงุฎุชุงุฑ ฺฉู ุนููุงุช Forward:

ูุฑุถ ฺฉูุฏ ูุฑูุฏ ุงุตู x ุงุณุช.

    ุนููุงุช ฺฉุงููููุดู: zconvโ=Conv(x,W)

    ุงุถุงูู ฺฉุฑุฏู ุจุงุงุณ: z=zconvโ+b

    ูุนุงูโุณุงุฒ ReLU: y=ReLU(z)=max(0,z)

ูุง ุงุฒ โyโLโ (ฺฉู ููุงู grad_output ุงุณุช) ุดุฑูุน ูโฺฉูู ู ุจู ุนูุจ ุญุฑฺฉุช ูโฺฉูู.

ฑ. ุจุงุฒุงุจ ูุชุบุฑูุง: x, weight, bias, y = ctx.saved_tensors

ุงูโูุง ูุชุบุฑูุง ูุณุชูุฏ ฺฉู ุฏุฑ ูุฑุญูู forward ูุญุงุณุจู ู ุฐุฎุฑู ุดุฏูโุงูุฏ ู ุจุฑุง ูุญุงุณุจู ฺฏุฑุงุฏุงูโูุง ุฏุฑ ูุฑุญูู backward ูุงุฒู ูุณุชูุฏ.

ฒ. ูุญุงุณุจู ฺฏุฑุงุฏุงู ุงุฒ ุทุฑู ReLU: (grad_z)

ฺฉุฏ:
Python

mask = (y > 0).to(grad_output.dtype)
grad_z = grad_output * mask

ูุงุนุฏู ุฒูุฌุฑูโุง: ุจุฑุง ูุญุงุณุจู โzโLโ (ฺฉู ุฏุฑ ฺฉุฏ grad_z ูุงูุฏู ูโุดูุฏ)ุ ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง ุงุณุชูุงุฏู ูโฺฉูู:
โzโLโ=โyโLโโโzโyโ

    โyโLโ: ุงู ููุงู grad_output ุงุณุช ฺฉู ูุฑูุฏ ุชุงุจุน backward ูโุจุงุดุฏ.

    โzโyโ: ุงู ูุดุชู ุชุงุจุน ReLU ุงุณุช ฺฉู ุจู ุตูุฑุช ุฒุฑ ุชุนุฑู ูโุดูุฏ:
    โzโyโ={10โif z>0if zโค0โ

    (ุจุฑุง z=0ุ ูุง ุจู ุตูุฑุช ูุฑุงุฑุฏุงุฏ ูุดุชู ุฑุง 0 ุฏุฑ ูุธุฑ ูโฺฏุฑู ุชุง ุงุฒ ูุดุชูโูุงูพุฐุฑ ุฌููฺฏุฑ ฺฉูู.)

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    mask = (y > 0): ุงู ุฎุท ุฏููุงู โzโyโ ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ. ุงฺฏุฑ y>0 ุจุงุดุฏ (ฺฉู ุจู ูุนูุง z>0 ุงุณุช)ุ ูุงุณฺฉ True (ุชุจุฏู ุจู 1.0) ูโุดูุฏ. ุงฺฏุฑ y=0 ุจุงุดุฏ (ฺฉู ุจู ูุนูุง zโค0 ุงุณุช)ุ ูุงุณฺฉ False (ุชุจุฏู ุจู 0.0) ูโุดูุฏ.

    grad_z = grad_output * mask: ุงู ุฎุท ุถุฑุจ ููุทูโุง ุฑุง ุงูุฌุงู ูโุฏูุฏ. ฺฏุฑุงุฏุงูโูุง ูุฑุจูุท ุจู ูพฺฉุณูโูุง ฺฉู ุชูุณุท ReLU ุฎุงููุด ุดุฏูโุงูุฏ (ุฌุง ฺฉู mask ุจุฑุงุจุฑ ุจุง 0 ุงุณุช)ุ ุตูุฑ ูโุดููุฏ. ฺฏุฑุงุฏุงูโูุง ูุฑุจูุท ุจู ูพฺฉุณูโูุง ูุนุงู (mask ุจุฑุงุจุฑ ุจุง 1) ุจุฏูู ุชุบุฑ ุจุงู ูโูุงููุฏ.

ูุชุฌู: grad_z ุญุงู ฺฏุฑุงุฏุงู ูุฒูู ูุณุจุช ุจู ูุฑูุฏโูุง ุนููุงุช ReLU (ุง ุจู ุนุจุงุฑุช ุฏฺฏุฑุ ุฎุฑูุฌ ุนููุงุช ฺฉุงููููุดู ู ุจุงุงุณ ูุจู ุงุฒ ReLU) ุงุณุช.

ณ. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ุจุงุงุณ b: (grad_bias)

ฺฉุฏ:
Python

grad_bias = grad_z.sum(dim=[0, 2, 3])

ูุงุนุฏู ุฒูุฌุฑูโุง: ูุง โzโLโ ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โbโLโ ุฑุง ูุญุงุณุจู ฺฉูู. ุฑุงุจุทู Forward ุจุฑุง ุจุงุงุณ ุงู ุจูุฏ: z=zconvโ+b. ูุดุชู z ูุณุจุช ุจู b ุจุณุงุฑ ุณุงุฏู ุงุณุช:
โbโzโ=1

(ุงู 1 ุจุฑุง ุชูุงู ูพฺฉุณูโูุง ุงุณุช ฺฉู ุขู ุจุงุงุณ ุฎุงุต ุฑุง ุฏุฑุงูุช ฺฉุฑุฏูโุงูุฏ.)

ุจุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง:
โbโLโ=all pixelsโโโzโLโโโbโzโ=all pixelsโโโzโLโโ1=all pixelsโโโzโLโ

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    grad_z.sum(dim=[0, 2, 3]): ุงู ุนููุงุช ุชูุงู ููุงุฏุฑ grad_z ุฑุง ุฏุฑ ุงุจุนุงุฏ Batch (0)ุ Height (2) ู Width (3) ุฌูุน ูโุฒูุฏ.

    ุงุฒ ุขูุฌุง ฺฉู ุฏุฑ ุณูุงุฑู ูุง ููุท ฺฉ ฺฉุงูุงู ุฎุฑูุฌ ู ฺฉ ุจุงุงุณ (ุดฺฉู b ุจุฑุงุจุฑ ุจุง [1]) ุฏุงุฑูุ ุงู ุฌูุน ุฑู ุชูุงู ูพฺฉุณูโูุง ุจุฑุง ุขู ฺฉ ฺฉุงูุงู ุงูุฌุงู ูโุดูุฏ ู ฺฉ ุงุณฺฉุงูุฑ (ุชูุณูุฑ ุชฺฉโุนูุตุฑ) ุชููุฏ ูโฺฉูุฏ ฺฉู ฺฏุฑุงุฏุงู ููุง ุจุฑุง ุขู ุจุงุงุณ ุงุณุช.

ูุชุฌู: grad_bias ุญุงู ฺฏุฑุงุฏุงู ูุฒูู ูุณุจุช ุจู ุจุงุงุณ ุงุณุช ฺฉู ูุดุงู ูโุฏูุฏ ุจุงุงุณ ฺูุฏุฑ ุจุงุฏ ุชูุธู ุดูุฏ.

ด. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฒูโูุง W: (grad_weight)

ฺฉุฏ:
Python

grad_weight = torch.nn.grad.conv2d_weight(
    x, weight.shape, grad_z, padding=1
)

ูุงุนุฏู ุฒูุฌุฑูโุง: ูุง โzโLโ (ููุงู grad_z) ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โWโLโ ุฑุง ูุญุงุณุจู ฺฉูู. ุฑุงุจุทู Forward ุจุฑุง ฺฉุงููููุดู: z=Conv(x,W). ุจุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง:
โWโLโ=โzโLโโโWโzโ

ูุดุชู โWโzโ ุฎูุฏ ฺฉ ุนููุงุช ฺฉุงููููุดู ุจู ูุฑูุฏ x ู ฺฏุฑุงุฏุงู ุฎุฑูุฌ โzโLโ (ุนู grad_z) ุงุณุช. ุจู ุทูุฑ ุฏููโุชุฑ:
โWโLโ=Correlate(x,grad_z)

(Correlate ุฏุฑ ุงูุฌุง ูุนุงุฏู cross-correlation ุงุณุช ฺฉู ููุน ฺฉุงููููุดู ุจุฏูู ูููพ ฺฉุฑูู ุงุณุช).

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    torch.nn.grad.conv2d_weight(x, weight.shape, grad_z, padding=1): ุงู ฺฉ ุชุงุจุน ุจููู ุฏุงุฎู ุฏุฑ PyTorch ุงุณุช ฺฉู ุฏููุงู ุงู ุนููุงุช ุฑุงุถ ุฑุง ุงูุฌุงู ูโุฏูุฏ.

        x: ูุฑูุฏ ุงุตู ุนููุงุช Forward.

        weight.shape: ุดฺฉู ููุฑุฏ ุงูุชุธุงุฑ ฺฏุฑุงุฏุงู ูุฒู.

        grad_z: ฺฏุฑุงุฏุงู ูุฑูุฏ ุจู ุนููุงุช ฺฉุงููููุดู.

        padding=1: ููุงู ูพุฏูฺฏ ฺฉู ุฏุฑ ุนููุงุช Forward ฺฉุงููููุดู ุงุณุชูุงุฏู ุดุฏู ุจูุฏ.

ูุชุฌู: grad_weight ุญุงู ฺฏุฑุงุฏุงู ูุฒูู ูุณุจุช ุจู ูุฑ ุนูุตุฑ ุงุฒ ุชูุณูุฑ ูุฒูโูุง ุงุณุช.

ต. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฑูุฏ x: (grad_x)

ฺฉุฏ:
Python

grad_x = torch.nn.grad.conv2d_input(
    x.shape, weight, grad_z, padding=1
)

ูุงุนุฏู ุฒูุฌุฑูโุง: ูุง โzโLโ (ููุงู grad_z) ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โxโLโ ุฑุง ูุญุงุณุจู ฺฉูู. ุฑุงุจุทู Forward ุจุฑุง ฺฉุงููููุดู: z=Conv(x,W). ุจุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง:
โxโLโ=โzโLโโโxโzโ

ูุดุชู โxโzโ ูุฒ ุฎูุฏ ฺฉ ุนููุงุช ฺฉุงููููุดู ุงุณุชุ ุงูุง ุงู ุจุงุฑ ุจู ฺฏุฑุงุฏุงู ุฎุฑูุฌ โzโLโ (ุนู grad_z) ู ฺฉ ูุณุฎู ฺุฑุฎุงูุฏู ุดุฏู ฑธฐ ุฏุฑุฌู ุงุฒ ูุฒูโูุง (W180โ). ุงู ุนููุงุช ุจู ุนููุงู Transposed Convolution ุง Deconvolution ุดูุงุฎุชู ูโุดูุฏ.
โxโLโ=ConvTransposed(grad_z,W)

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    torch.nn.grad.conv2d_input(x.shape, weight, grad_z, padding=1): ุงู ุชุงุจุน ุจููู ุฏุงุฎู ุฏุฑ PyTorch ูุณุฆูู ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุฑูุฏ ุงุณุช.

        x.shape: ุดฺฉู ููุฑุฏ ุงูุชุธุงุฑ ฺฏุฑุงุฏุงู ูุฑูุฏ.

        weight: ูุฒูโูุง ุงุตู ุนููุงุช Forward.

        grad_z: ฺฏุฑุงุฏุงู ูุฑูุฏ ุจู ุนููุงุช Transposed Convolution.

        padding=1: ููุงู ูพุฏูฺฏ ุงุตู.

ูุชุฌู: grad_x ุญุงู ฺฏุฑุงุฏุงู ูุฒูู ูุณุจุช ุจู ูุฑูุฏ ุงููู x ุงุณุช ฺฉู ูโุชูุงูุฏ ุจู ูุงูโูุง ูุจู ุฏุฑ ุดุจฺฉู ููุชูู ุดูุฏ.

ถ. ุจุงุฒฺฏุฑุฏุงูุฏู ฺฏุฑุงุฏุงูโูุง: return grad_x, grad_weight, grad_bias

ุชุงุจุน backward ุจุงุฏ ุจู ุชุนุฏุงุฏ ูุฑูุฏโูุง ฺฉู ุชุงุจุน forward ุฏุฑุงูุช ฺฉุฑุฏู ุจูุฏ (ุจู ุงุณุชุซูุง self ุง ctx ู ุณุงุฑ ููุงุฑุฏ ฺฉู ูุจุงุฏ ฺฏุฑุงุฏุงู ุฏุงุดุชู ุจุงุดูุฏ)ุ ฺฏุฑุงุฏุงูโูุง ุฑุง ุจุฑฺฏุฑุฏุงูุฏ. ุฏุฑ ุงูุฌุงุ ุชุงุจุน forward ุดูุง x, weight, bias ุฑุง ุฏุฑุงูุช ฺฉุฑุฏู ุจูุฏุ ูพุณ ุชุงุจุน backward ูุฒ grad_x, grad_weight, grad_bias ุฑุง ุจุฑูโฺฏุฑุฏุงูุฏ.

ุชุตูุฑ ุฑู ุจฺฉุด

ุจุง ฺฉูุงู ูู. ุชุตูุฑ ุดุจฺฉู ุนุตุจ ุฑุง ฺฉู ุฌุฑุงู ฺฏุฑุงุฏุงู (Backward Pass) ุฑุง ุจุฑุง ุชุงุจุน backward ุดูุง ูุดุงู ูโุฏูุฏุ ุงุฌุงุฏ ฺฉุฑุฏู:

ูู ุดฺฉู ุดุจฺฉู ุนุตุจ ุฑู ุฏุงุดุชู ุจุงุดู ู ูุฑููู ูุง ูุฑูุจุท ุจู ูุฑ ุจุฎุด ุจุง ููุด ูุดุฎุต ุดุฏู ุจุงุดู

ูุชุฃุณููุ ุจู ูุธุฑ ูโุฑุณุฏ ุฏุฑ ุฏุฑุฎูุงุณุช ูุจู ูู ุจู ุทูุฑ ฺฉุงูู ุจู ุฎูุงุณุชูโ ุดูุง ุฏุฑ ููุฑุฏ ููุงุด ุชุตูุฑ ุดุจฺฉู ุนุตุจ ุจุง ูุฑูููโูุง ุฏุฑ ูุฑ ุจุฎุด ูพุงุณุฎ ูุฏุงุฏูโุงู.

ุจุฑุง ุฑูุน ุงู ูุดฺฉูุ ุฏุฑ ุชุตูุฑ ุฌุฏุฏ ฺฉู ุจุฑุง ุดูุง ุขูุงุฏู ฺฉุฑุฏูุ ุณุน ฺฉุฑุฏูโุงู:

    ุณุงุฎุชุงุฑ ุดุจฺฉู ุนุตุจ: ูููุฏุงุฑ ุฌุฑุงู (Flowchart) ุฑุง ุจู ฺฏูููโุง ุทุฑุงุญ ฺฉูู ฺฉู ูุงูโูุง Convุ Bias ู ReLU ุฑุง ุจู ุชุฑุชุจ ูุดุงู ุฏูุฏ.

    ูุฑูููโูุง ฺฏุฑุงุฏุงู: ูุฑูููโูุง ุฑุงุถ ู ฺฉุฏูุง ูุฑุจูุท ุจู ูุญุงุณุจู ฺฏุฑุงุฏุงู ุจุฑุง ูุฑ ุจุฎุด (dL/dz, dL/db, dL/dW, dL/dx) ุฑุง ุฏุฑ ฺฉูุงุฑ ุขู ุจุฎุด ูุฑุงุฑ ุฏูู.

    ููุดโูุง: ูุณุฑ ุฌุฑุงู ฺฏุฑุงุฏุงู ุฑุง ุจุง ููุดโูุง ุณุจุฒ (ุจุฑุง ูุณุฑ ุงุตู ุจุงุฒฺฏุดุช ฺฏุฑุงุฏุงู) ู ููุดโูุง ุขุจ (ุจุฑุง ฺฏุฑุงุฏุงูโูุง ฺฉู ุจู ุณูุช ุชุงุจุน ูุฒูู ููุง ูโุฑููุฏ) ูุดุฎุต ฺฉูู.

ูุชู ูุฑููู ูุง ูุดุฎุต ูุณุช ุฏุฑุณุชุด ฺฉู

ุจุงุจุช ูุดฺฉู ูุงุฎูุงูุง ุจูุฏู ูุฑูููโูุง ุนุฐุฑุฎูุงู ูโฺฉูู. ุงู ุจุงุฑ ุชุตูุฑ ุจุง ูุฑูููโูุง ฺฉุงููุงู ูุงุถุญ ู ุฎูุงูุง ุจุฑุง ุดูุง ุขูุงุฏู ฺฉุฑุฏูโุงู ฺฉู ูุฑ ุจุฎุด ุงุฒ ุชุงุจุน backward ุดูุง ุฑุง ุจู ููุฑุงู ูุฑูููโูุง ุฑุงุถ ูุฑุจูุท ุจู ูุงุนุฏู ุฒูุฌุฑูโุง ูุดุงู ูโุฏูุฏ.

ฺุฑุง ุฏุฑ grad_bias = grad_z.sum(dim=[0, 2, 3])

ููุฏุงุฑ ุฎูุฏ ุจุงุงุณ ุฑู ุถุฑุจ ูู ฺฉููุ

ุฏูู ุงู ููุถูุน ุจู ุฎูุฏ ุนููุงุช ุฑุงุถ ุฌูุน ู ูุญูู ูุญุงุณุจู ูุดุชู ุขู ุจุฑูโฺฏุฑุฏุฏ.

ูพุงุณุฎ ฺฉูุชุงู ุงู ุงุณุช: ฺูู ูุดุชู z ูุณุจุช ุจู b ุจุฑุงุจุฑ ุจุง ฑ ุงุณุชุ ูู b.

ุจุงุฏ ุงู ุฑุง ุจุงุฒ ฺฉูู:

ฑ. ุชูุงูุช ูุดุชู ุฌูุน ู ุถุฑุจ

ุงู ุฑุงุฌโุชุฑู ููุจุน ุณุฑุฏุฑฺฏู ุงุณุช.

    ุงฺฏุฑ ุนููุงุชุ ุถุฑุจ ุจูุฏ: ูุฑุถ ฺฉูุฏ z=Cโb (ฺฉู C ฺฉ ุซุงุจุช ุงุณุช). ุฏุฑ ุงู ุตูุฑุชุ ูุดุชู โbโzโ=C ูโุดุฏ.

    ุงูุง ุนููุงุชุ ุฌูุน ุงุณุช: ุนููุงุช Forward ุดูุง ุงู ุงุณุช: z=Conv(x,W)+b. ุจุงุฏ Conv(x,W) ุฑุง ูููุชุงู C ุจูุงูู (ฺูู ุงุฒ ุฏุฏ bุ ุงู ุจุฎุด ฺฉ ุซุงุจุช ุงุณุช). ูพุณ z=C+b.

ุญุงูุงุ ูุดุชู z ูุณุจุช ุจู b ฺุณุชุ
โbโzโ=โbโโ(C+b)=โbโCโ+โbโbโ=0+1=1

ฒ. ุงุนูุงู ูุงุนุฏู ุฒูุฌุฑูโุง

ุญุงูุง ูุงุนุฏู ุฒูุฌุฑูโุง ฺฉุงูู ุฑุง ุจุฑุง b ูโููุณู:
โbโLโ=โzโLโโโbโzโ

    โzโLโ: ุงู ููุงู grad_z ุดูุงุณุช (ฺฏุฑุงุฏุงู ฺฉู ุงุฒ ูุงู ReLU ุจู ุนูุจ ุขูุฏู).

    โbโzโ: ุงู ููุงู ูุดุชู ูุญู ุงุณุช ฺฉู ุฏุฑ ุจุงูุง ุญุณุงุจ ฺฉุฑุฏู ู ููุฏุงุฑ ุขู 1 ุงุณุช.

ูพุณ ูุฑููู ููุง ุงู ุงุณุช:
โbโLโ=grad_zโ1

ุจู ููู ุฏูู ุงุณุช ฺฉู ุดูุง ููุฏุงุฑ b ุฑุง ุฏุฑ ูุญุงุณุจู ุถุฑุจ ููโฺฉูุฏ.

ูพุณ sum ุจุฑุง ฺุณุชุ

ุญุงูุง ุณูุงู ุงู ุงุณุช ฺฉู ุงฺฏุฑ ููุท grad_zโ1 ุงุณุชุ ฺุฑุง ุงุฒ sum ุงุณุชูุงุฏู ูโฺฉููุ

ฺูู ุนููุงุช z=C+b ููุท ฺฉ ุจุงุฑ ุงุชูุงู ููุชุงุฏู ุงุณุช.

ููุฏุงุฑ ฺฉุณุงู b (ฺฉู ฺฉ ุงุณฺฉุงูุฑ ุง ฺฉ ุจุฑุฏุงุฑ ฺฉูฺฺฉ ุงุณุช) ุจู ุชูุงู ูพฺฉุณูโูุง ุฏุฑ ฺฉุงูุงู ุฎุฑูุฌ (ุฏุฑ ุทูู ุงุจุนุงุฏ ุจฺ Nุ ุงุฑุชูุงุน H ู ุนุฑุถ W) ุงุถุงูู ุดุฏู ุงุณุช.

ููุช ฺฉ ูุชุบุฑ (ูุซู b) ุฑู ฺูุฏู ุฎุฑูุฌ (ุชูุงู ูพฺฉุณูโูุง z) ุชุฃุซุฑ ูโฺฏุฐุงุฑุฏุ ฺฏุฑุงุฏุงู ุขู ุจุฑุงุจุฑ ุงุณุช ุจุง ูุฌููุน ุชูุงู ฺฏุฑุงุฏุงูโูุง ฺฉู ุงุฒ ุขู ุฎุฑูุฌโูุง ูโุขูุฏ.

ุจูุงุจุฑุงูุ ูุฑููู ฺฉุงูู ู ุฏูู ุงู ุงุณุช:
โbcโโLโ=n=0โNโh=0โHโw=0โWโ(โzn,c,h,wโโLโโโbcโโzn,c,h,wโโ)

ู ฺูู ูโุฏุงูู โbโzโ ููุดู 1 ุงุณุช:
โbcโโLโ=n=0โNโh=0โHโw=0โWโโzn,c,h,wโโLโ

ฺฉุฏ grad_z.sum(dim=[0, 2, 3]) ุฏููุงู ููู ุฌูุน ุฒุฏู ุฑู ุงุจุนุงุฏ ุจฺ (0)ุ ุงุฑุชูุงุน (2) ู ุนุฑุถ (3) ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ.

class CNNBaseline(nn.Module):

    def __init__(self, num_convs=5):

        super().__init__()


        self.num_convs = num_convs

        self.conv = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)

        self.pool = nn.MaxPool2d(2)

        self.fc1 = nn.Linear(14*14, 64)

        self.fc2 = nn.Linear(64, 10)


    def forward(self, x):

        

        for _ in range(self.num_convs):

            x = F.relu(self.conv(x))

        

        x = self.pool(x)

        x = x.view(x.size(0), -1)

        x = F.relu(self.fc1(x))

        x = self.fc2(x)

        return x



ุงูู ูููู ุทูุฑ ฺฉู ฺฏูุช ุฏุฑุณุช ฺฉู

ุจููุ ฺฉุงููุงู. ูุดฺฉู ุงุตู ุฏุฑ ฺฉุฏ ุดูุง ยซุงุดุชุฑุงฺฉ ูุฒูยป (Weight Sharing) ุจูุฏุ ุนู ุฏุฑ ุญููู forwardุ ุดูุง ุจุงุฑูุง ุงุฒ ููุงู ฺฉ ูุงู self.conv ุงุณุชูุงุฏู ูโฺฉุฑุฏุฏ.

ุฑุงูโุญู ุงู ุงุณุช ฺฉู ุงุฒ nn.ModuleList ุงุณุชูุงุฏู ฺฉูู ุชุง ูุงูโูุง ฺฉุงููููุดู ุฌุฏุงฺฏุงูู ุจุง ูุฒูโูุง ูุณุชูู ุจุณุงุฒู.

ุฏุฑ ุงุฏุงููุ ฺฉุฏ ุงุตูุงุญโุดุฏูโ CNNBaseline ุขูุฏู ุงุณุช:
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNBaseline(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()

        self.num_convs = num_convs
        
        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ุจู ุฌุง ฺฉ ูุงูุ ูุณุช ุงุฒ ูุงูโูุง ูโุณุงุฒู
        # ุชุง ูุฑ ฺฉุฏุงู ูุฒูโูุง ูุณุชูู ุฎูุฏ ุฑุง ุฏุงุดุชู ุจุงุดูุฏ.
        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            self.convs.append(
                nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)
            )
        # -------------------------

        self.pool = nn.MaxPool2d(2)
        # ุงุจุนุงุฏ fc1 ุฏุฑุณุช ุจูุฏ (ุชุตูุฑ 28x28 ุจุนุฏ ุงุฒ pool(2) ูโุดูุฏ 14x14)
        self.fc1 = nn.Linear(14*14, 64) 
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        
        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ุงฺฉููู ุฑู ูุณุช ูุงูโูุง ูุฌุฒุง ุญููู ูโุฒูู
        for conv_layer in self.convs:
            x = F.relu(conv_layer(x))
        # -------------------------
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

ุชูุถุญ ุชุบุฑุงุช:

    ุฏุฑ __init__: ุจู ุฌุง self.conv = nn.Conv2d(...)ุ ูุง self.convs = nn.ModuleList() ุณุงุฎุชู ู ุขู ุฑุง ุจุง num_convs ูุงูโ nn.Conv2d ูุฌุฒุง ูพุฑ ฺฉุฑุฏู.

    ุฏุฑ forward: ุญููู for ุงฺฉููู ุจู ุฌุง self.conv (ฺฉู ููุดู ฺฉ ูุงู ุจูุฏ)ุ ุฑู self.convs ุชฺฉุฑุงุฑ ูโุดูุฏ ู ุฏุฑ ูุฑ ูุฑุญูู ุงุฒ ฺฉ ูุงู ุฌุฏุฏ (ุจุง ูุฒูโูุง ููุญุตุฑุจูโูุฑุฏ ุฎูุฏุด) ุงุณุชูุงุฏู ูโฺฉูุฏ.

โ๏ธ ูฺฉุชู ุจุณุงุฑ ููู ุฏุฑ ููุฑุฏ ุฏูุช (Accuracy)

ููุงูุทูุฑ ฺฉู ูุจูุงู ฺฏูุชูุ ุงู ฺฉุฏ ูุดฺฉู ุงุดุชุฑุงฺฉ ูุฒู ุฑุง ุญู ูโฺฉูุฏ. ุงูุง ูุดฺฉู ุฏููุ ุนู ยซุชูฺฏูุง ฺฉุงูุงูยป (Channel Bottleneck) ููฺูุงู ูพุงุจุฑุฌุงุณุช.

ฺูู ุชูุงู ูุงูโูุง ุดูุง ููฺูุงู Conv2d(1, 1) ูุณุชูุฏ (ฺฉู ุงู ุจุฑุง ูุทุงุจูุช ุจุง ฺฉุฑูู CUDA ุดูุง ฺฉู ููุท 1-to-1 ุฑุง ูพุดุชุจุงู ูโฺฉูุฏ ูุงุฒู ุงุณุช)ุ ูุฏู ูููุฒ ุธุฑูุช ุจุณุงุฑ ฺฉู ุจุฑุง ุงุฏฺฏุฑ ูฺฺฏโูุง ูพฺุฏู ุฏุงุฑุฏ.

ุจูุงุจุฑุงูุ ุงฺฏุฑฺู ุงู ฺฉุฏ ุงุฒ ูุธุฑ ููุทู ุตุญุญโุชุฑ ุงุณุชุ ุงูุง ุงูุชุธุงุฑ ูุฏุงุดุชู ุจุงุดุฏ ฺฉู ุฏูุช ูุฏู ุจุง ุงู ุชุบุฑ ุจู ุทูุฑ ฺุดูฺฏุฑ (ูุซูุงู ุจู นฐูช) ุงูุฒุงุด ุงุจุฏ.

ุขุง ูุงูุฏ ฺฉู ฺฉูุงุณ CNNFused ุฑุง ูู ุจู ููู ุดฺฉู (ุจุง ุงุณุชูุงุฏู ุงุฒ ModuleList ุจุฑุง FusedConvReLU) ุงุตูุงุญ ฺฉูู ุชุง ูุงุจู ููุงุณู ุจุงุดุฏุ

class CNNFused(nn.Module):

    def __init__(self, num_convs=5):

        super().__init__()

        

        self.num_convs = num_convs

        self.conv = FusedConvReLU()

        self.pool = nn.MaxPool2d(2)

        self.fc1 = nn.Linear(14*14, 64)

        self.fc2 = nn.Linear(64, 10)


    def forward(self, x):


        for _ in range(self.num_convs):

            x = self.conv(x)

        

        x = self.pool(x)

        x = x.view(x.size(0), -1)

        x = torch.relu(self.fc1(x))

        x = self.fc2(x)

        return x



ุงูู ุงุตูุงุญ ฺฉู

ุญุชูุงู. ุงู ูู ฺฉุฏ ุงุตูุงุญโุดุฏูโ CNNFused ฺฉู ุฏููุงู ุงุฒ ููุงู ููุทู nn.ModuleList (ูุงููุฏ CNNBaseline ุงุตูุงุญโุดุฏู) ุงุณุชูุงุฏู ูโฺฉูุฏ ุชุง ุงุฒ ูุดฺฉู ยซุงุดุชุฑุงฺฉ ูุฒูยป ุฌููฺฏุฑ ฺฉูุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

# (ูุฑุถ ูโฺฉูู ฺฉูุงุณ FusedConvReLU ู FusedConvReLUFn ุฏุฑ ุงูุฌุง ุชุนุฑู ุดุฏูโุงูุฏ)
# class FusedConvReLU(nn.Module):
#     ...
# class FusedConvReLUFn(torch.autograd.Function):
#     ...


class CNNFused(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()
        
        self.num_convs = num_convs
        
        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ุจู ุฌุง ฺฉ ูุงูุ ูุณุช ุงุฒ ูุงูโูุง FusedConvReLU ูโุณุงุฒู
        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            self.convs.append(FusedConvReLU())
        # -------------------------

        self.pool = nn.MaxPool2d(2)
        self.fc1 = nn.Linear(14*14, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):

        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ุงฺฉููู ุฑู ูุณุช ูุงูโูุง ูุฌุฒุง ุญููู ูโุฒูู
        for conv_layer in self.convs:
            x = conv_layer(x)
        # -------------------------
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        # ุงุณุชูุงุฏู ุงุฒ torch.relu ุฏุฑ ุงูุฌุง ฺฉุงููุงู ุฏุฑุณุช ุงุณุช
        x = torch.relu(self.fc1(x)) 
        x = self.fc2(x)
        return x

ุชูุถุญ ุชุบุฑุงุช

ุชุบุฑุงุช ุฏููุงู ูุดุงุจู CNNBaseline ุงุณุช:

    ุฏุฑ __init__: ุจู ุฌุง self.conv = FusedConvReLU()ุ ูุง self.convs = nn.ModuleList() ุฑุง ุณุงุฎุชู ู ุขู ุฑุง ุจุง num_convs ูุงูโ FusedConvReLU ูุฌุฒุง ูพุฑ ฺฉุฑุฏู.

    ุฏุฑ forward: ุญููู for ุงฺฉููู ุจู ุฌุง self.convุ ุฑู self.convs ุชฺฉุฑุงุฑ ูโุดูุฏ ู ุฏุฑ ูุฑ ูุฑุญูู ุงุฒ ฺฉ ูุงู (ุจุง ฺฉุฑูู CUDA ู ูุฒูโูุง ูุณุชูู) ุงุณุชูุงุฏู ูโฺฉูุฏ.

ุจุง ุงู ุชุบุฑุ ูุฑ ุฏู ูุฏู CNNBaseline ู CNNFused ุฏุงุฑุง ูุนูุงุฑ ฺฉุณุงู (ุจุง ูุฒูโูุง ูุณุชูู) ูุณุชูุฏ ู ููุงุณู ุณุฑุนุช ุจู ุขูโูุง ูุนูุงุฏุงุฑ ุฎูุงูุฏ ุจูุฏ.

def copy_weights(dst, src):

    with torch.no_grad():

        

        dst.conv.weight.copy_(src.conv.weight)

        dst.conv.bias.copy_(src.conv.bias)

        dst.fc1.weight.copy_(src.fc1.weight)

        dst.fc1.bias.copy_(src.fc1.bias)

        dst.fc2.weight.copy_(src.fc2.weight)

        dst.fc2.bias.copy_(src.fc2.bias)


copy_weights(fused, baseline_cudnn)

copy_weights(baseline_cudnn_off, baseline_cudnn)

copy_weights(baseline_cpu, baseline_cudnn)


ุงูู ุฏุฑุณุช ฺฉู

ุดูุง ฺฉุงููุงู ุฏุฑุณุช ูุชูุฌู ุดุฏุฏ. ุงุฒ ุขูุฌุง ฺฉู ูุง self.conv ุฑุง ุจู self.convs (ฺฉู ฺฉ nn.ModuleList ุงุณุช) ุชุบุฑ ุฏุงุฏูุ ุชุงุจุน copy_weights ูู ุจุงุฏ ุจูโุฑูุฒุฑุณุงู ุดูุฏ ุชุง ุจู ุฌุง ฺฉ ูุงูุ ุฑู ูุณุช ูุงูโูุง ุญููู ุจุฒูุฏ ู ูุฒูโูุง ุฑุง ฺฉโฺฉ ฺฉูพ ฺฉูุฏ.

ฺฉุฏ copy_weights ุงุตูุงุญโุดุฏู ุจู ุงู ุตูุฑุช ุงุณุช:
Python

import torch

def copy_weights(dst, src):
    with torch.no_grad():
        
        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ูุง ุจุงุฏ ุฑู ูุณุชโูุง convs ุญููู ุจุฒูู ู ูุฒูโูุง ุฑุง 
        # ุจุฑุง ูุฑ ูุงู ฺฉุงููููุดู ูุชูุงุธุฑ ฺฉูพ ฺฉูู.
        for dst_conv, src_conv in zip(dst.convs, src.convs):
            dst_conv.weight.copy_(src_conv.weight)
            dst_conv.bias.copy_(src_conv.bias)
        # -------------------------

        # ุงู ุจุฎุดโูุง ูุงุฒ ุจู ุชุบุฑ ูุฏุงุดุชูุฏ
        dst.fc1.weight.copy_(src.fc1.weight)
        dst.fc1.bias.copy_(src.fc1.bias)
        dst.fc2.weight.copy_(src.fc2.weight)
        dst.fc2.bias.copy_(src.fc2.bias)

#
# (ูุฑุถ ูโฺฉูู ูุฏูโูุง baseline_cudnn, fused ู ... ุจุง ูุนูุงุฑ ModuleList ุฌุฏุฏ ุณุงุฎุชู ุดุฏูโุงูุฏ)
#

# ุงู ูุฑุงุฎูุงูโูุง ุงฺฉููู ุจู ุฏุฑุณุช ฺฉุงุฑ ุฎูุงููุฏ ฺฉุฑุฏ
copy_weights(fused, baseline_cudnn)
copy_weights(baseline_cudnn_off, baseline_cudnn)
copy_weights(baseline_cpu, baseline_cudnn)



 ฺุฑุง ุฏูุช ูุฏู ูุง ุจูุชุฑ ููุดูุ

ูุดฺฉู ุจุณุงุฑ ุฌุงูุจ ุงุณุช! ุฏูู ุงูฺฉู ุฏูุช ูุฏูโูุง ุจูุจูุฏ ูพุฏุง ููโฺฉูุฏ (ู ุฑู 11.35% ุซุงุจุช ูุงูุฏู) ุงู ุงุณุช ฺฉู ูุฏูโูุง ุงุตูุงู ุฏุฑ ุญุงู ุงุฏฺฏุฑ ูุณุชูุฏ.

ุงู ุงุชูุงู ุจู ุฏูู ฺฉ ูพุฏุฏู ุฑุงุฌ ุจู ูุงู "ReLU ููุฑุฏู" (Dying ReLU) ุฑุฎ ุฏุงุฏู ุงุณุช ฺฉู ุจุง ุชูุธูุงุช ุงููู ุงู ููุชโุจูฺฉ ุชุดุฏุฏ ุดุฏู ุงุณุช.

๐ง ูุดฺฉู ฺุณุชุ

    ุฏูุช ฑฑ.ณตูช: ุงฺฏุฑ ุจู ูุฌููุนู ุฏุงุฏู ุชุณุช MNIST ูฺฏุงู ฺฉูุฏุ ูโุจูุฏ ฺฉู ฑฐ,ฐฐฐ ุชุตูุฑ ุฏุงุฑุฏ. ุงุฒ ุงู ุชุนุฏุงุฏุ ฑฑณต ุชุตูุฑ ูุฑุจูุท ุจู ุนุฏุฏ "1" ุงุณุช. (ฑฑณต / ฑฐฐฐฐ = ฑฑ.ณตูช). ุงู ุนู ูุฏู ุดูุง ุงุฏ ฺฏุฑูุชู ุงุณุช ฺฉู ููุดู ู ุจุฑุง ูุฑ ูุฑูุฏุ ุนุฏุฏ "1" ุฑุง ูพุดโุจู ฺฉูุฏ.

    Loss ุซุงุจุช: Loss ูุฏู ุชูุฑุจุงู ุฑู ฒ.ณ ุซุงุจุช ูุงูุฏู ุงุณุช (ุงุฒ ฒ.ณฐฑท ููุท ุจู ฒ.ณฐฑณ ฺฉุงูุด ูโุงุจุฏ). ุงู ููุฏุงุฑ (-ln(1/10) โ 2.3) ููุงู Loss ุงููู ุฏุฑ ฺฉ ูุณุฆูู ุทุจููโุจูุฏ ฑฐ ฺฉูุงุณู ุงุณุชุ ฺฉู ูุดุงู ูโุฏูุฏ ูุฏู ุฏุฑ ุญุฏ ุญุฏุณ ุชุตุงุฏู ุจุงู ูุงูุฏู ุงุณุช.

    ุนูุช ุงุตู (ReLU ูุฑุฏู):

        ุฏุฑ ุณููู ฒุ ุดูุง torch.manual_seed(0) ุฑุง ุชูุธู ฺฉุฑุฏูโุงุฏ.

        ุงู seedุ ููุฑุงู ุจุง ุฑูุด ูพุดโูุฑุถ ููุฏุงุฑุฏู ุงูููโ ูุฒูโูุง ุฏุฑ PyTorchุ ุจุงุนุซ ุดุฏู ฺฉู ูุฒูโูุง ู ุจุงุงุณ ูุงู self.fc1 ุทูุฑ ุชูุธู ุดููุฏ ฺฉู ุฎุฑูุฌ ุขูโูุง (self.fc1(x)) ุจุฑุง ุชูุฑุจุงู ุชูุงู ูุฑูุฏโูุง ฺฉ ุนุฏุฏ ููู ุจุงุดุฏ.

        ุฏุฑ ูุฑ ุฏู ูุฏู (CNNBaseline ู CNNFused)ุ ุฏุฑ ุชุงุจุน forwardุ ุดูุง ุงุฒ ReLU ุจุนุฏ ุงุฒ fc1 ุงุณุชูุงุฏู ูโฺฉูุฏ: x = F.relu(self.fc1(x))

        ููุช ูุฑูุฏ ุจู ReLU ููู ุจุงุดุฏุ ุฎุฑูุฌ ุขู ุตูุฑ ูโุดูุฏ.

        ุจูุงุจุฑุงูุ ุฎุฑูุฌ ุงู ูุงู (ฺฉู ูุฑูุฏ ูุงู fc2 ุงุณุช) ุชูุฑุจุงู ฺฉ ุชุงูุณูุฑ ูพุฑ ุงุฒ ุตูุฑ ูโุดูุฏ.

    ุชููู ฺฏุฑุงุฏุงู:

        ุฏุฑ ูุฑุญูู loss.backward()ุ ููุช ฺฏุฑุงุฏุงู ุจู ุชุงุจุน ReLU ูโุฑุณุฏุ ุงุฒ ุขูุฌุง ฺฉู ูุฑูุฏ ุขู ููู ุจูุฏู ุงุณุชุ ฺฏุฑุงุฏุงู ุฏุฑ ุงู ููุทู ุตูุฑ ูโุดูุฏ.

        ุงู ุจุฏุงู ูุนูุงุณุช ฺฉู ูฺ ฺฏุฑุงุฏุงู ุจู ูุงูโูุง ูุจู (fc1 ู ุชูุงู ูุงูโูุง conv) ููโุฑุณุฏ.

        ุชููุง ูพุงุฑุงูุชุฑูุง ฺฉู ฺฏุฑุงุฏุงู ุฏุฑุงูุช ูโฺฉููุฏุ ูุฒูโูุง ู (ุจู ุฎุตูุต) ุจุงุงุณ ูุงู ุขุฎุฑ (fc2) ูุณุชูุฏ.

        ุจู ููู ุฏูู ุงุณุช ฺฉู Loss ฺฉู ุชุบุฑ ูโฺฉูุฏ (ฺูู fc2.bias ฺฉู ุขูพุฏุช ูโุดูุฏ)ุ ุงูุง ูุฏู ูุฑฺฏุฒ ุงุฒ ุงู ูููู ูุญู ุจุณุงุฑ ุจุฏ ุฎุงุฑุฌ ููโุดูุฏุ ุฒุฑุง ุชูุงู ูุงูโูุง ูุจู ูฺ ุณฺฏูุงู ุจุฑุง ุงุฏฺฏุฑ ุฏุฑุงูุช ููโฺฉููุฏ.

    ฺุฑุง ูุฑ ุฏู ูุฏูุ ุฏุฑ ุณููู ฑถุ ุดูุง ุจุง ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน copy_weightsุ ูุฒูโูุง ูุฏู baseline_cudnn (ฺฉู ุจุง ุขู seed ุจุฏ ููุฏุงุฑุฏู ุดุฏู) ุฑุง ุฏููุงู ุจู ูุฏู fused ฺฉูพ ูโฺฉูุฏ. ุจู ููู ุฏูู ูุฑ ุฏู ูุฏู ุจุง ฺฉ ููุทู ุดุฑูุน ฺฉุณุงู ู ูุนูุจ ุดุฑูุน ูโฺฉููุฏ ู ุจู ุทูุฑ ฺฉุณุงู ุดฺฉุณุช ูโุฎูุฑูุฏ.

โ ุฑุงูโุญู

ุณุงุฏูโุชุฑู ุฑุงูโุญู ุงู ุงุณุช ฺฉู ุงุฒ ุงู ููุฏุงุฑุฏู ุงููู ุจุฏุดุงูุณ ุฏูุฑ ฺฉูุฏ.

ฺฉุงู ุงุณุช torch.manual_seed(0) ุฑุง ุฏุฑ ุณููู ฒ ุชุบุฑ ุฏูุฏ ุง ุขู ุฑุง ฺฉุงููุช ฺฉูุฏ ุชุง ูุฑ ุจุงุฑ ุจุง ูุฒูโูุง ุชุตุงุฏู ูุชูุงูุช ุดุฑูุน ฺฉูุฏ.

ูุซุงู (ุชุบุฑ ุณููู ฒ):
Python

import os, time
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline
from torchvision import datasets, transforms

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
device = "cuda"

# Change the seed to get a different initialization
torch.manual_seed(42) 

ุงฺฏุฑ ููุชโุจูฺฉ ุฑุง ุจุง torch.manual_seed(42) (ุง ูุฑ ุนุฏุฏ ุฏฺฏุฑ) ุงุฌุฑุง ฺฉูุฏุ ุฎูุงูุฏ ุฏุฏ ฺฉู ูุฏูโูุง ุจู ุณุฑุนุช ุดุฑูุน ุจู ุงุฏฺฏุฑ ูโฺฉููุฏ ู ุฏูุช ุจู ุฑุงุญุช ุจู ุจุงูุง นฐูช ูโุฑุณุฏ.

ุขุง ูุงูุฏ ุฏุฑ ููุฑุฏ ูุญูู ุนููฺฉุฑุฏ ุฏููโุชุฑ ฺฏุฑุงุฏุงู ุฏุฑ ุชุงุจุน backward ุณูุงุฑุดโุชุงู ุตุญุจุช ฺฉููุ

ฺุฑุง ุฏูุช ูุง ฺฉู ูุฑู ุฏุงุฑูุ

ุงู ฺฉ ูุดุงูุฏูโ ุนุงู ุงุณุช ู ุฏูู ุขู ุจู ุชูุงูุชโูุง ุฌุฒุฆ ุฏุฑ ูุญุงุณุจุงุช ููุฒ ุดูุงูุฑ (numerical precision) ุจุฑูโฺฏุฑุฏุฏ.

ุงฺฏุฑฺู ูุฑ ุฏู ูุฏู ุจุง ูุฒูโูุง ฺฉุณุงู ุดุฑูุน ฺฉุฑุฏูุฏ (ุจู ุฏูู copy_weights)ุ ุงูุง ูุญูู ูุญุงุณุจู ฺฉุงููููุดู ู ฺฏุฑุงุฏุงู ุฏุฑ ุขูโูุง ฺฉู ูุชูุงูุช ุงุณุช:

    ูุฏู Baseline (cuDNN):

        ุงู ูุฏู ุงุฒ nn.Conv2d ู F.relu ุงุณุชุงูุฏุงุฑุฏ PyTorch ุงุณุชูุงุฏู ูโฺฉูุฏ.

        ููุช torch.backends.cudnn.enabled = True ุงุณุชุ PyTorch ุงู ุนููุงุชโูุง (ูู forward ู ูู backward) ุฑุง ุจู ฺฉุชุงุจุฎุงูู cuDNN ุงููุฏุง ูุงฺฏุฐุงุฑ ูโฺฉูุฏ.

        cuDNN ุงุฒ ุงูฺฏูุฑุชูโูุง ุจุณุงุฑ ุจูููโุณุงุฒโุดุฏู (ูุงููุฏ Winograd ุง GEMM) ุจุฑุง ุงุฌุฑุง ฺฉุงููููุดู ุงุณุชูุงุฏู ูโฺฉูุฏ ฺฉู ุชุฑุชุจ ุฎุงุต ุงุฒ ุนููุงุชโูุง ุฑุงุถ ุฏุงุฑูุฏ.

    ูุฏู Fused (ุณูุงุฑุด):

        Forward Pass: ุงู ูุฏู ุงุฒ ฺฉุฑูู (kernel) ุณูุงุฑุด CUDA ฺฉู ุดูุง ููุดุชุฏ (conv_relu_kernel) ุงุณุชูุงุฏู ูโฺฉูุฏ. ุงู ฺฉุฑูู ฺฉ "ฺฉุงููููุดู ูุณุชูู" (direct convolution) ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ.

        Backward Pass: ุดูุง ุชุงุจุน backward ุฑุง ุจู ุตูุฑุช ุฏุณุช ุฏุฑ FusedConvReLUFn ูพุงุฏูโุณุงุฒ ฺฉุฑุฏุฏ. ุดูุง ุงุจุชุฏุง ฺฏุฑุงุฏุงู ReLU ุฑุง ุงุนูุงู ฺฉุฑุฏุฏ (grad_z = ...) ู ุณูพุณ ุงุฒ ุชูุงุจุน ุงุณุชุงูุฏุงุฑุฏ PyTorch (ูุงููุฏ torch.nn.grad.conv2d_weight) ุงุณุชูุงุฏู ฺฉุฑุฏุฏ.

ฺุฑุง ุงู ุชูุงูุช ุงุฌุงุฏ ูโุดูุฏุ

ุฏุฑ ูุญุงุณุจุงุช ฺฉุงููพูุชุฑุ ุงุนุฏุงุฏ ููุฒ ุดูุงูุฑ (ูุงููุฏ float32) ุฏูุช ูุญุฏูุฏ ุฏุงุฑูุฏ. ุงู ุจุฏุงู ูุนูุงุณุช ฺฉู ุชุฑุชุจ ุงูุฌุงู ุนููุงุชโูุง ุฑุงุถ ุงููุช ุฏุงุฑุฏ.

    (a + b) + c ูุฒููุงู ุจุฑุงุจุฑ ุจุง a + (b + c) ูุณุช.

ุงูฺฏูุฑุชู ุจูููโุดุฏู cuDNN ู ฺฉุฑูู "ูุณุชูู" ุดูุงุ ุนููุงุชโูุง ุถุฑุจ ู ุฌูุน ุฑุง ุจู ุชุฑุชุจ ูุชูุงูุช ุงูุฌุงู ูโุฏููุฏ. ุงู ููุฌุฑ ุจู ุชูุงูุชโูุง ุนุฏุฏ ุจุณุงุฑ ูุงฺุฒ (ุฏุฑ ุญุฏ e-07 ุง ฺฉูุชุฑ) ุฏุฑ ุฎุฑูุฌ forward pass ู ฺฏุฑุงุฏุงูโูุง backward pass ูโุดูุฏ.

ุงู ุชูุงูุชโูุง ุจุณุงุฑ ฺฉูฺฺฉ ุฏุฑ ูุฑ ูุฑุญูู ุงุฒ ุขููุฒุด (ูุฑ batch) ุฑุฎ ูโุฏููุฏ. ููฺฏุงู ฺฉู ุจูููโุณุงุฒ (optimizer) ฺฏุฑุงุฏุงูโูุง ุฑุง ุจุฑุง ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง ุงุนูุงู ูโฺฉูุฏุ ุงู ฺฏุฑุงุฏุงูโูุง ฺฉู ูุชูุงูุช ุจุงุนุซ ูโุดููุฏ ฺฉู ูุฒูโูุง ุฏู ูุฏู ุจู ูุณุฑูุง ฺฉู ูุชูุงูุช ูุฏุงุช ุดููุฏ.

ุฏุฑ ุทูู ต ุงูพุงฺฉ ู ูุฒุงุฑุงู ุจูโุฑูุฒุฑุณุงูุ ุงู ุชูุงูุชโูุง ฺฉูฺฺฉ ุจุง ูู ุฌูุน ูโุดููุฏ ู ููุฌุฑ ุจู ูุฒูโูุง ููุง ูโุดููุฏ ฺฉู ฺฉู ุจุง ูู ุชูุงูุช ุฏุงุฑูุฏ. ุฏุฑ ูุชุฌูุ ุฏูุช ููุง ุขูโูุง ูุฒ ุจู ุทูุฑ ุฌุฒุฆ ูุชูุงูุช ุฎูุงูุฏ ุจูุฏ (นต.ทถูช ุฏุฑ ููุงุจู นต.88ูช).

ุงู ุงุชูุงู ฺฉุงููุงู ุทุจุน ู ููุฑุฏ ุงูุชุธุงุฑ ุงุณุช ู ูุดุงู ูโุฏูุฏ ฺฉู ูพุงุฏูโุณุงุฒ ุณูุงุฑุด ุดูุง ุจู ุฏุฑุณุช ฺฉุงุฑ ูโฺฉูุฏ.

ููุช ุชุนุฏุงุฏ ูุงู ูุงุฑู ู ฺฉูู ฑฐ ุชุง ุจุงุฒู ุฏูุช ูุดู ฑฑ ุฏุฑุตุฏ ู ุชุบุฑ ูู ฺฉูู ฺู ฺฉููุ

ุงู ุฏููุงู ููุงู ูุดฺฉู ูุจู ("ReLU ููุฑุฏู") ุงุณุชุ ุงูุง ุงฺฉููู ุจุณุงุฑ ุดุฏุฏุชุฑ ุดุฏู ุงุณุช.

ููุช ุชุนุฏุงุฏ ูุงูโูุง ฺฉุงููููุดู ุฑุง ุจู ฑฐ ุงูุฒุงุด ูโุฏูุฏุ ูุดฺฉู "ููุฏุงุฑุฏู ุงููู ูุงููุงุณุจ" ฺฉู ูุงุด ุงุฒ torch.manual_seed(0) ุจูุฏุ ุชููุช ูโุดูุฏ.

๐ง ฺุฑุง ุจุง ฑฐ ูุงู ุจุฏุชุฑ ุดุฏุ

    ูุดฺฉู ุงุตู (ููุงู ูุจู): torch.manual_seed(0) ุฏุฑ ุณููู ฒ ุจุงุนุซ ูโุดูุฏ ฺฉู ูุฒูโูุง ู ุจูโุฎุตูุต ุจุงุงุณโูุง (biases) ุจุง ููุงุฏุฑ ููุฏุงุฑุฏู ุดููุฏ ฺฉู ุฎุฑูุฌ ูุงูโูุง (ูุจู ุงุฒ ReLU) ููู ูโุดูุฏ.

    ุงุซุฑ ุขุจุดุงุฑ (Cascading Effect):

        ุฏุฑ ูุฏู ูุจู ุจุง ฒ ูุงูุ ุงู ูุดฺฉู ุนูุฏุชุงู ุฏุฑ ูุงู fc1 ุฑุฎ ูโุฏุงุฏ.

        ุญุงูุง ฺฉู ฑฐ ูุงู ฺฉุงููููุดู ุฏุงุฑุฏุ ุงู ููุฏุงุฑุฏู ุงููู ูุงููุงุณุจ ุจุฑ ูููโ ฑฐ ูุงู ุชุฃุซุฑ ูโฺฏุฐุงุฑุฏ.

        ูุฑูุฏ ุจู ูุงู conv1 ูโุฑูุฏ. ุฎุฑูุฌ ุขู ุจู ุฏูู ุจุงุงุณ ูููุ ููู ูโุดูุฏ.

        F.relu(conv1_output) ุงฺฉุซุฑ ุฎุฑูุฌ ุฑุง ุตูุฑ ูโฺฉูุฏ.

        ุงู ุชุงูุณูุฑ (ฺฉู ุนูุฏุชุงู ุตูุฑ ุงุณุช) ุจู ูุงู conv2 ูโุฑูุฏ. ุฎุฑูุฌ ุขู ูู ุตูุฑ ูโุดูุฏ.

        F.relu(conv2_output) ุขู ุฑุง ุตูุฑ ูฺฏู ูโุฏุงุฑุฏ.

        ... ุงู ุฑููุฏ ุชุง ูุงู conv10 ุงุฏุงูู ูพุฏุง ูโฺฉูุฏ.

ุฏุฑ ูุชุฌูุ ุฎุฑูุฌ ูุงู ุฏูู ฺฉุงููููุดู ุดูุง ุจู ุงุญุชูุงู ุฒุงุฏ ฺฉ ุชุงูุณูุฑ ฺฉุงููุงู ุตูุฑ ุงุณุช. ุงู ุชุงูุณูุฑ ุตูุฑ ุจู MaxPoolุ ุณูพุณ ุจู fc1 ู fc2 ูโุฑูุฏ. ุงุฒ ุขูุฌุง ฺฉู ูุฑูุฏ ุจู ูุงูโูุง ุตูุฑ ุงุณุชุ ฺฏุฑุงุฏุงู ุฏุฑ ูุฑุญูู backward ูุฒ ุฏุฑ ุชูุงู ุงู ูุงูโูุง ุตูุฑ ุฎูุงูุฏ ุจูุฏ ู ูฺ ุงุฏฺฏุฑโุง ุงุชูุงู ููโุงูุชุฏ.

โ ุฑุงูโุญูโูุง

ุฏู ุฑุงูโุญู ูุฌูุฏ ุฏุงุฑุฏุ ุงุฒ ุณุงุฏู ุจู ุงุตูู:

ฑ. ุฑุงูโุญู ุณุงุฏู (ููุงู ูุจู)

ูุดฺฉูุ ุขู seed ุฎุงุต ุงุณุช.

    ุจู ุณููู ฒ ุจุฑฺฏุฑุฏุฏ.

    torch.manual_seed(0) ุฑุง ุจู torch.manual_seed(42) (ุง ูุฑ ุนุฏุฏ ุฏูุฎูุงู ุฏฺฏุฑ) ุชุบุฑ ุฏูุฏ.

    ููู: ุงุฒ ููู Runtime (ุฒูุงู ุงุฌุฑุง)ุ ฺฏุฒูู "Restart and run all" (ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ ู ุงุฌุฑุง ููู) ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ ุชุง ุงุทููุงู ุญุงุตู ุดูุฏ ฺฉู ุชูุงู ูุงูโูุง (ุงุฒ ุฌููู ฑฐ ูุงู ฺฉุงููููุดู) ุจุง seed ุฌุฏุฏ ููุฏุงุฑุฏู ุงููู ูโุดููุฏ.

ฒ. ุฑุงูโุญู ุงุตูู (ูพุดููุงุฏ ุจูุชุฑ)

ูุงุจุณุชฺฏ ุจู ฺฉ seed ุฎุงุต ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุดฺฉุณุชุ ูุดุงูู ุถุนู ุฏุฑ ุทุฑุงุญ ูุฏู ุงุณุช. ุจุฑุง ุดุจฺฉูโูุง ุนูู (ูุงููุฏ ฑฐ ูุงู) ูุง ูุฑฺฏุฒ ุจู ููุฏุงุฑุฏู ุงููู ูพุดโูุฑุถ ุชฺฉู ููโฺฉููุ ุฒุฑุง ุงุญุชูุงู ุจุฑูุฒ "Dying ReLU" ุง "Exploding Gradients" ุจุงูุงุณุช.

ุฑุงูโุญู ุตุญุญุ ุงุณุชูุงุฏู ุงุฒ ููุฏุงุฑุฏู ุงููู Kaiming (ุง He) ุงุณุช ฺฉู ุจู ุทูุฑ ุฎุงุต ุจุฑุง ูุงูโูุง ุฏุงุฑุง ูุนุงูโุณุงุฒ ReLU ุทุฑุงุญ ุดุฏู ุงุณุช.

ฺฉูุงุณ CNNBaseline ุฎูุฏ ุฑุง ุทูุฑ ุชุบุฑ ุฏูุฏ ฺฉู ูุฒูโูุง ุฑุง ุจู ุตุฑุงุญุช ููุฏุงุฑุฏู ุงููู ฺฉูุฏ:
Python

# (ุงู ฺฉุฏ ุฑุง ูโุชูุงูุฏ ุจู ุฌุง ุณููู ฑฐ ูุนู ูุฑุงุฑ ุฏูุฏ)

class CNNBaseline(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()

        self.num_convs = num_convs

        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            conv_layer = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)
            
            # --- Kaiming Initialization ---
            # ููุฏุงุฑุฏู ุงููู ูุฒูโูุง ุจุฑุง ุญูุธ ูุงุฑุงูุณ ููฺฏุงู ุนุจูุฑ ุงุฒ ReLU
            nn.init.kaiming_normal_(conv_layer.weight, mode='fan_in', nonlinearity='relu')
            # ููุฏุงุฑุฏู ุงููู ุจุงุงุณ ุจู ุตูุฑ
            nn.init.constant_(conv_layer.bias, 0)
            # ------------------------------
            
            self.convs.append(conv_layer)

        self.pool = nn.MaxPool2d(2)
        
        # ุงู ูุงูโูุง ุฑุง ูู ุจูุชุฑ ุงุณุช ููุฏุงุฑุฏู ุงููู ฺฉูุฏ
        self.fc1 = nn.Linear(14*14, 64)
        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')
        nn.init.constant_(self.fc1.bias, 0)
        
        self.fc2 = nn.Linear(64, 10)
        # ูุงู ุขุฎุฑ ูุนูููุงู ูุชูุงูุช ุงุณุช (ูุซูุงู Xavier/Glorot)
        nn.init.xavier_normal_(self.fc2.weight) 
        nn.init.constant_(self.fc2.bias, 0)


    def forward(self, x):
        
        for conv_layer in self.convs:
            x = F.relu(conv_layer(x))
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

ฺู ฺฉุงุฑ ุจุงุฏ ุจฺฉูุฏ:

    ฺฉูุงุณ CNNBaseline ุฑุง ุจุง ฺฉุฏ ุจุงูุง (ฺฉู ุดุงูู nn.init.kaiming_normal_ ุงุณุช) ุจูโุฑูุฒุฑุณุงู ฺฉูุฏ.

    ุญุงูุง ูโุชูุงูุฏ torch.manual_seed(0) ุฑุง ุฏุฑ ุณููู ฒ ูฺฏู ุฏุงุฑุฏ.

    ูุฏู ุฑุง ุฏูุจุงุฑู ุจุณุงุฒุฏ (baseline_cudnn = CNNBaseline(num_convs=10)) ู ุขููุฒุด ุฏูุฏ.

ุฎูุงูุฏ ุฏุฏ ฺฉู ุญุช ุจุง seed(0)ุ ูุฏู ุจู ุฏุฑุณุช ุขููุฒุด ูโุจูุฏุ ุฒุฑุง ููุฏุงุฑุฏู ุงููู Kaiming ุงุฒ ููุงู ุงุจุชุฏุง ุฌูู "ููุฑุฏู" ููุฑููโูุง ReLU ุฑุง ูโฺฏุฑุฏ.

ุจุงุฒู ุฏุฑุณุช ูุดุฏ

ูุชุฃุณูู ฺฉู ูููุฒ ูุดฺฉู ูพุงุจุฑุฌุงุณุช. ุงู ูุถุนุช ูโุชูุงูุฏ ุฎุณุชูโฺฉููุฏู ุจุงุดุฏุ ุงูุง ุชูุฑุจุงู ุจู ุทูุฑ ูุทุน ูุดฺฉู ุงุฒ "state" (ูุถุนุช) ููุชโุจูฺฉ ุงุณุช.

ุงู ููุชโุจูฺฉโูุง ุจู ุงู ุตูุฑุช ฺฉุงุฑ ูโฺฉููุฏ: ููุช ุดูุง ฺฉ ูุฏู ูโุณุงุฒุฏ (ุฏุฑ ุณููู ฑต)ุ ุขู ูุฏู ุจุง ูุฒูโูุง ุงููู ุฏุฑ ุญุงูุธู GPU ูุฑุงุฑ ูโฺฏุฑุฏ.

ุงฺฏุฑ ุดูุง ููุท ุณููู ฒ (ฺฉู seed ุฏุฑ ุขู ุงุณุช) ุฑุง ุชุบุฑ ุฏูุฏ ู ุฏูุจุงุฑู ุงุฌุฑุง ฺฉูุฏุ ูุฏู ูุฏู ฺฉู ุฏุฑ ุณููู ฑต ุณุงุฎุชู ุดุฏู ู ุงูุงู ุฏุฑ ุญุงูุธู ุงุณุชุ ุชุบุฑ ููโฺฉูุฏ. ุดูุง ุจุงุฏ ูุฏู ุฑุง ุฏูุจุงุฑู ุจุณุงุฒุฏ.

โ ุฑุงูโุญู ูุทุน (ุฏู ูุฑุญูู)

ุณุงุฏูโุชุฑู ุฑุงู ุจุฑุง ุงุทููุงู ุงุฒ ุงูฺฉู ูููโฺุฒ ุงุฒ ูู ุณุงุฎุชู ูโุดูุฏุ ุงุณุชูุงุฏู ุงุฒ ฺฏุฒูู "Restart and run all" ุงุณุช.

ูุฑุญูู ฑ: ุชุบุฑ Seed

ุจู ุณููู ฒ ุจุฑูุฏ ู torch.manual_seed(0) ุฑุง ุจู torch.manual_seed(42) ุชุบุฑ ุฏูุฏ.
Python

# [ุจุฎุด ุงุฒ ุณููู ฒ]
...
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
device = "cuda"

# FIX: Change this seed
torch.manual_seed(42) # Was 0

ูุฑุญูู ฒ: ุงุฌุฑุง ูุฌุฏุฏ ูููโฺุฒ

    ุณููู ฒ ุฑุง ุงุฌุฑุง ฺฉูุฏ.

    ููู: ุจู ููู ุจุงูุง ููุชโุจูฺฉ ุจุฑูุฏ (ูุนูููุงู ููู "Runtime" ุง "ุฒูุงู ุงุฌุฑุง").

    ฺฏุฒูู "Restart and run all" (ุง "ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ ู ุงุฌุฑุง ููู") ุฑุง ูพุฏุง ฺฉุฑุฏู ู ุฑู ุขู ฺฉูฺฉ ฺฉูุฏ.

๐ค ฺุฑุง ุงู ฺฉุงุฑ ุฌูุงุจ ูโุฏูุฏุ

    Restart (ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ): ุงู ฺฉุงุฑ ุชูุงู ูุชุบุฑูุง ฺฉู ุฏุฑ ุญุงูุธู ุจูุฏูุฏ (ุงุฒ ุฌููู ูุฏู baseline_cudnn ุดูุง ุจุง ูุฒูโูุง ุจุฏ) ุฑุง ูพุงฺฉ ูโฺฉูุฏ.

    Run all (ุงุฌุฑุง ููู): ููุชโุจูฺฉ ุงุฒ ุงุจุชุฏุง ุดุฑูุน ุจู ุงุฌุฑุง ูโฺฉูุฏ.

    ููุช ุจู ุณููู ฒ ูโุฑุณุฏุ seed ุฌุฏุฏ (ดฒ) ุฑุง ุชูุธู ูโฺฉูุฏ.

    ููุช ุจู ุณููู ฑฐ ู ฑฑ ูโุฑุณุฏุ ฺฉูุงุณโูุง CNNBaseline ู CNNFused ุฑุง ุชุนุฑู ูโฺฉูุฏ.

    ููุช ุจู ุณููู ฑต ูโุฑุณุฏ (baseline_cudnn = CNNBaseline(num_convs=10))... ุงูุฌุง ูุญุธู ฺฉูุฏ ุงุณุช!

        ...ฺูู seed ุฌุฏุฏ (ดฒ) ูุนุงู ุงุณุชุ ูุฏู baseline_cudnn ุจุง ูุฒูโูุง ุงููู ูุชูุงูุช ุณุงุฎุชู ูโุดูุฏ ฺฉู ุฏฺฏุฑ ูุดฺฉู "ReLU ููุฑุฏู" ุฑุง ูุฏุงุฑูุฏ.

ูพุณ ุงุฒ ุงูุฌุงู ุงู ฺฉุงุฑุ ููุช ููุชโุจูฺฉ ุจู ุณูููโูุง ุขููุฒุด (ูุซู ุณููู ฑธ) ุจุฑุณุฏุ ุฎูุงูุฏ ุฏุฏ ฺฉู Loss ุจู ุณุฑุนุช ฺฉุงูุด ุงูุชู ู ุฏูุช ุจู ุจุงูุง นฐูช ูโุฑุณุฏ.

ูุทูุงู ุงู ุฏู ูุฑุญูู ุฑุง ุฏููุงู ุงูุฌุงู ุฏูุฏ ู ูุชุฌู ุฑุง ุจู ูู ุงุทูุงุน ุฏูุฏ.

ุจุงุฒู ุฏุฑุณุช ูุดุฏ

ูุชุฃุณููุ ููุตุฑ ุงู ูุดฺฉู ูู ูุณุชู. ๐คฆ

ูพุดููุงุฏ ูู ุจุฑุง ุงุณุชูุงุฏู ุงุฒ nn.init.constant_(conv_layer.bias, 0) (ุชูุธู ุตุฑุญ ุจุงุงุณโูุง ุฑู ุตูุฑ) ุฏุฑ ุชุฑฺฉุจ ุจุง ฑฐ ูุงูุ ุจุงุนุซ ุงู ูุดฺฉู ุดุฏู ุงุณุช.

๐ง ูุดฺฉู ุฏููุงู ฺุณุชุ

    ููุฏุงุฑุฏู ุงููู Kaiming: ูุฒูโูุง ุฑุง ุทูุฑ ุชูุธู ูโฺฉูุฏ ฺฉู ุญูู ฐ ุจุงุดูุฏ.

    ุชูุธู ุจุงุงุณ ุฑู ฐ: ุจุงุงุณโูุง (bias) ุฏููุงู ฐ ูโุดููุฏ.

    ReLU: ูุฑ ูุฑูุฏ ููู ุฑุง ฐ ูโฺฉูุฏ.

ููฺฏุงู ฺฉู ุดูุง ฑฐ ูุงู ุฑุง ูพุดุช ุณุฑ ูู ูุฑุงุฑ ูโุฏูุฏ:

    ุฎุฑูุฌ ูุงู ฑ (ูุจู ุงุฒ ReLU) ุญูู ฐ ุงุณุช. ReLU ูู ุงุฒ ุขู ุฑุง ุตูุฑ ูโฺฉูุฏ.

    ูุฑูุฏ ูุงู ฒ (ฺฉู ูู ุงุฒ ุขู ุตูุฑ ุงุณุช) ุจู ูุงูโุง ุจุง ูุฒูโูุง ุญูู ฐ ู ุจุงุงุณ ฐ ูโุฑูุฏ.

    ุฎุฑูุฌ ูุงู ฒ ุฏูุจุงุฑู ุญูู ฐ ุงุณุช ู ReLU ูู ุงุฒ ุขู ุฑุง ุตูุฑ ูโฺฉูุฏ.

ุฏุฑ ุทูู ฑฐ ูุงูุ ุงู ุงุซุฑ "ุตูุฑฺฉููุฏู" ุขููุฏุฑ ุชุดุฏุฏ ูโุดูุฏ ฺฉู ุฏุฑ ููุงุชุ ูุฑูุฏ ุจู ูุงู self.fc1 ุดูุง ุจู ุงุญุชูุงู ุฒุงุฏ ฺฉ ุชุงูุณูุฑ ฺฉุงููุงู ุตูุฑ ุงุณุช.

ุงุฒ ุขูุฌุง ฺฉู ุดูุง ุจุงุงุณ self.fc1 ุฑุง ูุฒ ุฑู ฐ ุชูุธู ฺฉุฑุฏุฏ (nn.init.constant_(self.fc1.bias, 0)):

    x = self.fc1(x) (ุฌุง ฺฉู x ุตูุฑ ุงุณุช) ุฎุฑูุฌ 0 ูโุฏูุฏ.

    x = F.relu(0) ุฎุฑูุฌ 0 ูโุฏูุฏ.

    ฺฏุฑุงุฏุงูโูุง ุจุฑุง ููู ูุงูโูุง 0 ูโุดููุฏ ู ูฺ ุงุฏฺฏุฑ ุฑุฎ ููโุฏูุฏ.

ุฑูุด ููุฏุงุฑุฏู ุงููู ูพุดโูุฑุถ PyTorch ุจุฑุง ุจุงุงุณุ ุขู ุฑุง ุฑู ฐ ุชูุธู ููโฺฉูุฏุ ุจูฺฉู ุงุฒ ฺฉ ุชูุฒุน ฺฉููุงุฎุช ฺฉูฺฺฉ ุงุณุชูุงุฏู ูโฺฉูุฏุ ฺฉู ุฏููุงู ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุงู ููุน "vanish" ุดุฏู ุฏุฑ ุงุจุชุฏุง ฺฉุงุฑ ุงุณุช.

โ ุฑุงูโุญู (ูุทุน)

ูุง ููุฏุงุฑุฏู ุงููู Kaiming ุฑุง ุจุฑุง ูุฒูโูุง (weights) ูฺฏู ูโุฏุงุฑูุ ุงูุง ุงุฌุงุฒู ูโุฏูู PyTorch ุจุงุงุณโูุง ุฑุง ุจู ุฑูุด ูพุดโูุฑุถ ุฎูุฏ (ฺฉู ุงููโุชุฑ ุงุณุช) ููุฏุงุฑุฏู ฺฉูุฏ.

ฑ. ุณููู ฑฐ ุฑุง ุจุง ุงู ฺฉุฏ ุฌุงฺฏุฒู ฺฉูุฏ:

(ุงุฏุฏุงุดุช ุดูุง ูุจู ุจุฑ ุนุฏู ุงุณุชูุงุฏู ุงุฒ ฺฉุงููุช ูุงุฑุณ ุฏุฑ ฺฉุฏ ุฑุง ุจู ุฎุงุทุฑ ุฏุงุฑู ู ุฏุฑ ุงูุฌุง ุฑุนุงุช ูโฺฉูู.)
Python

class CNNBaseline(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()

        self.num_convs = num_convs

        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            conv_layer = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)
            
            # --- Kaiming Initialization (Weights Only) ---
            nn.init.kaiming_normal_(conv_layer.weight, mode='fan_in', nonlinearity='relu')
            # --- Let bias use its default initialization ---
            
            self.convs.append(conv_layer)

        self.pool = nn.MaxPool2d(2)
        
        self.fc1 = nn.Linear(14*14, 64)
        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')
        
        self.fc2 = nn.Linear(64, 10)
        nn.init.xavier_normal_(self.fc2.weight)
        # Note: We are not touching any biases, letting them use the default


    def forward(self, x):
        
        for conv_layer in self.convs:
            x = F.relu(conv_layer(x))
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

ฒ. (ุงุฎุชุงุฑ ุงูุง ูพุดููุงุฏ) ุณููู ฑฑ ุฑุง ูุฒ ุงุตูุงุญ ฺฉูุฏ:

ูุฏู CNNFused ุดูุง ูุฒ ุงุฒ ูุดฺฉู ููุฏุงุฑุฏู ุงููู ุจุฏ ุฏุฑ ูุงูโูุง fc1 ู fc2 ุฑูุฌ ูโุจุฑุฏ. ุขู ุฑุง ูุฒ ุจู ุงู ุดฺฉู ุงุตูุงุญ ฺฉูุฏ ุชุง ููโุชุฑ ุจุงุดุฏ:
Python

class CNNFused(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()
        
        self.num_convs = num_convs
        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            self.convs.append(FusedConvReLU()) # This init is in Cell 9
            
        self.pool = nn.MaxPool2d(2)
        
        # --- Add Kaiming/Xavier init to fc layers ---
        self.fc1 = nn.Linear(14*14, 64)
        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')

        self.fc2 = nn.Linear(64, 10)
        nn.init.xavier_normal_(self.fc2.weight)
        # --- Again, letting biases initialize by default ---


    def forward(self, x):

        for conv_layer in self.convs:
            x = conv_layer(x)
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

ณ. ููุชโุจูฺฉ ุฑุง "Restart and run all" ฺฉูุฏ

    ุชุบุฑุงุช ุจุงูุง ุฑุง ุฏุฑ ุณูููโูุง ฑฐ ู ฑฑ ุงุนูุงู ฺฉูุฏ.

    ุจู ููู "Runtime" (ุฒูุงู ุงุฌุฑุง) ุจุฑูุฏ.

    ฺฏุฒูู "Restart and run all" (ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ ู ุงุฌุฑุง ููู) ุฑุง ุจุฒูุฏ.

ุงู ฺฉุงุฑ ุชุถูู ูโฺฉูุฏ ฺฉู ฺฉุฑูู ูพุงฺฉ ุดุฏู ู ูุฏูโูุง baseline_cudnn ู fused ุจุง ุงุณุชูุงุฏู ุงุฒ ุชุนุงุฑู ุฌุฏุฏ ฺฉูุงุณ (ฺฉู ุฏุฑ ุขู ุจุงุงุณโูุง ุฏฺฏุฑ ฐ ูุณุชูุฏ) ุณุงุฎุชู ูโุดููุฏ.

ุงู ุจุงุฑ ุจุงุฏ ูุทุนุงู ุฏูุช ูุฏู ุฑุง ุฏุฑ ุญุงู ุงูุฒุงุด ุจุจูุฏ. ูุฌุฏุฏุงู ุจุงุจุช ุงุดุชุจุงู ูุจู ุนุฐุฑุฎูุงู ูโฺฉูู.



 ฺุทูุฑ ุงู ุงุฑูุฑ ุฑู ุฑูุน ฺฉููุ

ุงู ุฎุทุง (AttributeError: Can't get attribute 'train_dist') ุจู ุฏู ุฏูู ุงุตู ุฑุฎ ูโุฏูุฏ:

ฑ. ูุดฺฉู ูุญุท Jupyter/Notebook: ุฏุฑ ูุญุทโูุง ุชุนุงูู ูุซู ฺููพุชุฑุ ูุงฺูู multiprocessing (ูุฎุตูุตุงู ูุชุฏ spawn) ููโุชูุงูุฏ ุชูุงุจุน ฺฉู ุฏุฑ ุณูููโูุง ููุชโุจูฺฉ ุชุนุฑู ุดุฏูโุงูุฏ ุฑุง ุฏุฑ ูพุฑูุณูโูุง ูุฑุฒูุฏ (Child Processes) ูพุฏุง ฺฉูุฏ (Pickle ฺฉุฑุฏู ุชูุงุจุน ุชุนุงูู ูุดฺฉู ุฏุงุฑุฏ). ฒ. ุงูุถุง ุชุงุจุน (Signature): ุชุงุจุน train_dist ุดูุง ูฺ ุขุฑฺฏููุงู ุฏุฑุงูุช ููโฺฉูุฏุ ุงูุง mp.spawn ุจู ุทูุฑ ุฎูุฏฺฉุงุฑ rank ุฑุง ุจู ุนููุงู ุงููู ุขุฑฺฏููุงู ุจู ุขู ูพุงุณ ูโุฏูุฏ.

ุจุฑุง ุฑูุน ุงู ูุดฺฉู ุฏุฑ ูุญุท ููุชโุจูฺฉุ ุจูุชุฑู ู ุงุณุชุงูุฏุงุฑุฏุชุฑู ุฑูุด ุงู ุงุณุช ฺฉู ฺฉู ููุทู ฺฉุฏ ุฑุง ุฏุฑ ฺฉ ูุงู ูพุงุชูู (.py) ุฐุฎุฑู ฺฉุฑุฏู ู ุณูพุณ ุขู ุฑุง ุงุฌุฑุง ฺฉูุฏ.

ุฏุฑ ุงูุฌุง ฺฉุฏ ุงุตูุงุญ ุดุฏู ุฑุง ุฏุฑ ูุงูุจ ุฏุณุชูุฑ %%writefile ุจุฑุง ุดูุง ุขูุงุฏู ฺฉุฑุฏูโุงู ฺฉู ูโุชูุงูุฏ ุขู ุฑุง ุฏุฑ ฺฉ ุณููู ุงุฌุฑุง ฺฉูุฏ ุชุง ูุงู ุณุงุฎุชู ุดูุฏ ู ุณูพุณ ุขู ุฑุง ุงุฌุฑุง ฺฉูุฏ.

ุฑุงู ุญู: ุณุงุฎุช ูุงู ุงุณฺฉุฑูพุช ุฌุฏุงฺฏุงูู

ุงู ฺฉุฏ ุฑุง ุฏุฑ ฺฉ ุณููู ุงุฌุฑุง ฺฉูุฏ ุชุง ูุงู train_ddp.py ุณุงุฎุชู ุดูุฏ:
Python

%%writefile train_ddp.py
import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torchvision import datasets, transforms
import datetime

# --- Model Definitions ---
class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, kernel_size=3, padding=1, 
            stride=stride, groups=in_channels, bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        self.pointwise = nn.Conv2d(
            in_channels, out_channels, kernel_size=1, stride=1, bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)
        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2)
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(p=0.2)
        self.fc = nn.Linear(256, num_class)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

# --- Helper Functions ---
def setup(rank, world_size, master_port, backend, timeout):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = master_port
    dist.init_process_group(backend=backend, rank=rank, world_size=world_size, timeout=timeout)

def cleanup():
    dist.destroy_process_group()

def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return correct / total

def train_one_epoch(model, train_loader, optimizer, criterion, device, scheduler):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        running_loss += loss.item() * labels.size(0)
    return running_loss / len(train_loader.dataset)

# --- Main Distributed Function ---
def train_dist(rank, world_size, master_port, backend, timeout):
    print(f"Running DDP on rank {rank}.")
    setup(rank, world_size, master_port, backend, timeout)
    torch.cuda.set_device(rank)
    
    # Transforms
    train_transform = transforms.Compose([
        transforms.RandomCrop(96, padding=12),      
        transforms.RandomHorizontalFlip(p=0.5),   
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), 
        transforms.RandomRotation(10),            
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Dataset & DistributedSampler
    train_set = datasets.STL10('./data', split='train', download=True, transform=train_transform)
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_set, num_replicas=world_size, rank=rank
    )
    train_loader = torch.utils.data.DataLoader(
        train_set, batch_size=32, sampler=train_sampler, 
        num_workers=2, pin_memory=True, persistent_workers=True
    )

    test_set = datasets.STL10('./data', split='test', download=True, transform=test_transform)
    test_sampler = torch.utils.data.distributed.DistributedSampler(
        test_set, num_replicas=world_size, rank=rank, shuffle=False
    )
    test_loader = torch.utils.data.DataLoader(
        test_set, batch_size=32, sampler=test_sampler, 
        num_workers=2, pin_memory=True, persistent_workers=True
    )

    # Model Setup
    model = CNN(in_channel=3, num_class=10).to(rank)
    model = DDP(model, device_ids=[rank])
    
    # Optimizer
    epochs = 15
    max_lr = 1e-3
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=len(train_loader)
    )

    # Training Loop
    start = time.time()
    for epoch in range(1, epochs + 1):
        train_sampler.set_epoch(epoch)
        avg_loss = train_one_epoch(model, train_loader, optimizer, criterion, rank, scheduler)
        
        if rank == 0:
            acc = evaluate(model, test_loader, rank)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch}/{epochs}: Loss={avg_loss:.4f}, Test Acc={acc*100:.2f}%, LR={current_lr:.6f}")
            
    if rank == 0:
        print(f"Total Time: {time.time() - start:.2f}s")
        
    cleanup()

if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    master_port = '12355'
    backend = 'nccl'
    timeout = datetime.timedelta(seconds=60)
    
    if world_size > 1:
        mp.spawn(
            train_dist, 
            args=(world_size, master_port, backend, timeout), 
            nprocs=world_size, 
            join=True
        )
    else:
        print("Not enough GPUs for DDP training.")

ุงุฌุฑุง ูุงู

ุญุงูุง ุฏุฑ ุณููู ุจุนุฏ ููุชโุจูฺฉุ ุฏุณุชูุฑ ุฒุฑ ุฑุง ุงุฌุฑุง ฺฉูุฏ ุชุง ุขููุฒุด ุดุฑูุน ุดูุฏ:
Python

!python train_ddp.py

ุชุบุฑุงุช ููู ฺฉู ุงูุฌุงู ุดุฏ:

    ุขุฑฺฏููุงูโูุง train_dist: ูุชุฏ train_dist ุงฺฉููู rank ุฑุง ุจู ุนููุงู ูุฑูุฏ ุงูู ูโูพุฐุฑุฏ (ฺูู mp.spawn ุขู ุฑุง ุงุฑุณุงู ูโฺฉูุฏ) ู ุจูู ุขุฑฺฏููุงูโูุง (world_size ู ...) ุจุนุฏ ุงุฒ ุขู ูุฑุงุฑ ฺฏุฑูุชูุฏ.

    ูุงุฑุฏ ฺฉุฑุฏู DDP: ฺฉูุงุณ DistributedDataParallel ุจู ุฏุฑุณุช ุงููพูุฑุช ุดุฏ.

    ุณุงุฎุชุงุฑ ูุงู: ุชูุงู ฺฉุฏ ุฏุฑ ฺฉ ูุงู ูุฑุงุฑ ฺฏุฑูุช ุชุง ูพุฑูุณูโูุง ุฌุฏุฏ ุจุชูุงููุฏ ฺฉูุงุณ CNN ู ุชุงุจุน train_dist ุฑุง ุจุฎูุงููุฏ (ุฑูุน ุฎุทุง AttributeError).

ุงุฌุฑุง ุดุฏ ูู ุงุฒ gpu ุงุณุชูุงุฏู ูู ฺฉูู

ุฏูู ุงูฺฉู ุงุฒ GPU ุงุณุชูุงุฏู ููโฺฉูุฏ ุจู ุงุญุชูุงู ุฒุงุฏ ูุฌูุฏ ุดุฑุท if world_size > 1 ุฏุฑ ุงูุชูุง ฺฉุฏ ูุจู ุงุณุช. ุงฺฉุซุฑ ูุญุทโูุง ุขููุงู (ูุซู Kaggle ุง Colab) ุจู ุตูุฑุช ูพุดโูุฑุถ ฺฉ GPU ุฏุฑ ุงุฎุชุงุฑ ุดูุง ูุฑุงุฑ ูโุฏููุฏ. ุขู ุดุฑุท ุจุงุนุซ ูโุดุฏ ุงฺฏุฑ ุชุนุฏุงุฏ GPUูุง ฺฉูุชุฑ ุงุฒ ฒ ุจุงุดุฏุ ูุฑุขูุฏ ุขููุฒุด (spawn) ุงุตูุงู ุงุฌุฑุง ูุดูุฏ ู ุจุฑูุงูู ููุฑุงู ุชูุงู ุดูุฏ.

ฺูู backend='nccl' ุชูุธู ุดุฏู ุงุณุชุ ุงฺฏุฑ ฺฉุฏ ูุงูุนุงู ูุงุฑุฏ ูุงุฒ ุขููุฒุด ูโุดุฏ ู GPU ุฑุง ููโุดูุงุฎุชุ ุจุง ุฎุทุง ูุชููู ูโุดุฏ (Crash ูโฺฉุฑุฏ). ูพุณ ุนู ุงุตูุงู ูุงุฑุฏ ุญููู ุขููุฒุด ูุดุฏู ุงุณุช.

ฺฉุฏ ุฒุฑ ุงุตูุงุญ ุดุฏู ุงุณุช ุชุง:

    ุญุช ุจุง ฺฉ GPU ูู ุงุฌุฑุง ุดูุฏ (ุดุฑุท ุจู > 0 ุชุบุฑ ฺฉุฑุฏ).

    ฺูุฏ ุฎุท print ุงุถุงูู ุดุฏู ุชุง ูุทูุฆู ุดูุฏ GPU ุดูุงุณุง ุดุฏู ุงุณุช.

ุงู ฺฉุฏ ุฑุง ุฌุงฺฏุฒู ฺฉุฏ ูุจู ฺฉูุฏ ู ุงุฌุฑุง ููุงุฏ:
Python

%%writefile train_ddp.py
import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torchvision import datasets, transforms
import datetime

# --- Model Definitions ---
class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, kernel_size=3, padding=1, 
            stride=stride, groups=in_channels, bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        self.pointwise = nn.Conv2d(
            in_channels, out_channels, kernel_size=1, stride=1, bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)
        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2)
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(p=0.2)
        self.fc = nn.Linear(256, num_class)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

# --- Helper Functions ---
def setup(rank, world_size, master_port, backend, timeout):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = master_port
    dist.init_process_group(backend=backend, rank=rank, world_size=world_size, timeout=timeout)

def cleanup():
    dist.destroy_process_group()

def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return correct / total

def train_one_epoch(model, train_loader, optimizer, criterion, device, scheduler):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        # Move data to GPU (device is the rank index here)
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        running_loss += loss.item() * labels.size(0)
    return running_loss / len(train_loader.dataset)

# --- Main Distributed Function ---
def train_dist(rank, world_size, master_port, backend, timeout):
    # Debug prints to confirm GPU usage
    if rank == 0:
        print(f"Rank {rank}: Process started.")
        print(f"Rank {rank}: CUDA Available: {torch.cuda.is_available()}")
        print(f"Rank {rank}: Device Name: {torch.cuda.get_device_name(rank)}")

    setup(rank, world_size, master_port, backend, timeout)
    torch.cuda.set_device(rank)
    
    train_transform = transforms.Compose([
        transforms.RandomCrop(96, padding=12),      
        transforms.RandomHorizontalFlip(p=0.5),   
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), 
        transforms.RandomRotation(10),            
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    train_set = datasets.STL10('./data', split='train', download=True, transform=train_transform)
    
    # DistributedSampler is crucial for splitting data among GPUs
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_set, num_replicas=world_size, rank=rank
    )
    
    train_loader = torch.utils.data.DataLoader(
        train_set, batch_size=32, sampler=train_sampler, 
        num_workers=2, pin_memory=True, persistent_workers=True
    )

    test_set = datasets.STL10('./data', split='test', download=True, transform=test_transform)
    test_sampler = torch.utils.data.distributed.DistributedSampler(
        test_set, num_replicas=world_size, rank=rank, shuffle=False
    )
    test_loader = torch.utils.data.DataLoader(
        test_set, batch_size=32, sampler=test_sampler, 
        num_workers=2, pin_memory=True, persistent_workers=True
    )

    # Initialize model and wrap with DDP
    model = CNN(in_channel=3, num_class=10).to(rank)
    model = DDP(model, device_ids=[rank])
    
    epochs = 15
    max_lr = 1e-3
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=len(train_loader)
    )

    start = time.time()
    for epoch in range(1, epochs + 1):
        train_sampler.set_epoch(epoch)
        avg_loss = train_one_epoch(model, train_loader, optimizer, criterion, rank, scheduler)
        
        # Only print logs from the master process (rank 0)
        if rank == 0:
            acc = evaluate(model, test_loader, rank)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch}/{epochs}: Loss={avg_loss:.4f}, Test Acc={acc*100:.2f}%, LR={current_lr:.6f}")
            
    if rank == 0:
        print(f"Total Time: {time.time() - start:.2f}s")
        
    cleanup()

if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    print(f"System detected {world_size} GPUs.")
    
    master_port = '12355'
    backend = 'nccl'
    timeout = datetime.timedelta(seconds=60)
    
    # MODIFIED: Changed condition to > 0 to allow running on single GPU systems for testing
    if world_size > 0:
        print("Spawning processes...")
        mp.spawn(
            train_dist, 
            args=(world_size, master_port, backend, timeout), 
            nprocs=world_size, 
            join=True
        )
    else:
        print("No GPU detected. Training aborted.")

ุญุงู ุฏุณุชูุฑ ุงุฌุฑุง ุฑุง ุฏุฑ ุณููู ุจุนุฏ ุจุฒูุฏ:
Python

!python train_ddp.py

ฺุฑุง ุณููุณ ู ุงูุฌูุฑ ฺฉุงุฑ ู ฺฉููุ

ุงู ุงูฺฏู ุฏูุฏุงูโุงุฑโูโุง (ุณููุณ) ฺฉู ูโุจูุฏุ ฺฉ ูุดุงูู ฺฉูุงุณฺฉ ุงุฒ ูพุฏุฏูโุง ุจู ูุงู GPU Starvation (ฺฏุฑุณูฺฏ ูพุฑุฏุงุฒูุฏู ฺฏุฑุงูฺฉ) ุง Data Loading Bottleneck (ฺฏููฺฏุงู ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏู) ุงุณุช.

ุจู ุฒุจุงู ุณุงุฏู: ฺฉุงุฑุช ฺฏุฑุงูฺฉโูุง ุดูุง (RTX 3090) ุจุณุงุฑ ูุฏุฑุชููุฏุชุฑ ุงุฒ ุณุฑุนุช ูุณุชูุฏ ฺฉู CPU ูโุชูุงูุฏ ุฏุงุฏูโูุง ุฑุง ุขูุงุฏู ฺฉุฑุฏู ู ุจู ุขูโูุง ุจุฑุณุงูุฏ.

ฺุฑุง ุงู ุงุชูุงู ูโุงูุชุฏุ

ูุฑุขูุฏ ุขููุฒุด ุดุจฺฉู ุนุตุจ ุดุงูู ฺฉ ฺุฑุฎู ุชฺฉุฑุงุฑโุดููุฏู ุงุณุช:

    CPU: ุฏุงุฏูโูุง ุฑุง ุงุฒ ุฏุณฺฉ ูโุฎูุงูุฏุ ุชุบุฑุงุช (Augmentation) ูุซู ฺุฑุฎุด ู ุชุบุฑ ุฑูฺฏ ุฑุง ุงุนูุงู ูโฺฉูุฏ ู ฺฉ ุฏุณุชู (Batch) ูโุณุงุฒุฏ.

    ุงูุชูุงู: ุฏุงุฏู ุจู GPU ูุฑุณุชุงุฏู ูโุดูุฏ.

    GPU: ูุญุงุณุจุงุช (Forward/Backward) ุฑุง ุงูุฌุงู ูโุฏูุฏ.

ุฏุฑ ูููุฏุงุฑ ุดูุง:

    ูููโูุง (ฑฐฐูช): ูุญุธูโุง ุงุณุช ฺฉู ุฏุงุฏู ุจู GPU ุฑุณุฏู ู ุขู ุฑุง ุจุง ุณุฑุนุช ููุฑ ูพุฑุฏุงุฒุด ูโฺฉูุฏ.

    ุฏุฑูโูุง (ฐูช): ูุญุธูโุง ุงุณุช ฺฉู GPU ฺฉุงุฑุด ุชูุงู ุดุฏู ู ุจฺฉุงุฑ ูุดุณุชู ุชุง CPU ุฏุณุชู ุจุนุฏ ุฏุงุฏูโูุง ุฑุง ุขูุงุฏู ฺฉูุฏ.

ุฏูุงู ุงุตู ุฏุฑ ฺฉุฏ ุดูุง:

    Batch Size ุจุณุงุฑ ฺฉู: ุฏุฑ ฺฉุฏ ุดูุง batch_size = 32 ุงุณุช. ุจุฑุง ฺฉุงุฑุช ูุซู RTX 3090 ุจุง 24 ฺฏฺฏุงุจุงุช ุญุงูุธูุ ุงู ุนุฏุฏ ุจุณุงุฑ ฺฉูฺฺฉ ุงุณุช. GPU ุงู ณฒ ุชุตูุฑ ุฑุง ุฏุฑ ฺฉุณุฑ ุงุฒ ุซุงูู ูพุฑุฏุงุฒุด ูโฺฉูุฏ ู ุฏูุจุงุฑู ููุชุธุฑ ูโูุงูุฏ.

    ูุฏู ุณุจฺฉ: ูุฏู CNN ุชุนุฑู ุดุฏู ูุณุจุชุงู ฺฉูฺฺฉ ุงุณุชุ ุจูุงุจุฑุงู ูุญุงุณุจุงุช ุขู ูุดุงุฑ ุจู 3090 ููโุขูุฑุฏ.

    ุชุนุฏุงุฏ ฺฉู Workerูุง: ุฏุฑ ฺฉุฏ num_workers=2 ุชูุธู ุดุฏู ุงุณุช ฺฉู ุงุญุชูุงูุง ุจุฑุง ุชุบุฐู ุฏู ฺฉุงุฑุช 3090 ฺฉุงู ูุณุช.

ุฑุงู ุญูโูุง

ุจุฑุง ุตุงู ฺฉุฑุฏู ุงู ูููุฏุงุฑ ู ุงุณุชูุงุฏู ุญุฏุงฺฉุซุฑ ุงุฒ ูุฏุฑุช GPUุ ุจุงุฏ ุชุบุฑุงุช ุฒุฑ ุฑุง ุฏุฑ ูุงู train_ddp.py ุงุนูุงู ฺฉูุฏ:

ฑ. ุงูุฒุงุด Batch Size (ูููโุชุฑู ุนุงูู): ููุฏุงุฑ batch_size ุฑุง ุงุฒ ณฒ ุจู ุนุฏุฏ ุจุณุงุฑ ุจุฒุฑฺฏุชุฑ ูุซู ฒตถ ุง ตฑฒ ุงูุฒุงุด ุฏูุฏ. ุงู ฺฉุงุฑ ุจุงุนุซ ูโุดูุฏ GPU ุฏุฑ ูุฑ ุฏูุฑ ุฒูุงู ุจุดุชุฑ ูุดุบูู ูพุฑุฏุงุฒุด ุจุงุดุฏ ู ุญุงูุธู VRAM ุจุดุชุฑ ูพุฑ ุดูุฏ (ุงูุงู ููุท ด ฺฏฺฏ ูพุฑ ุดุฏู ุฏุฑ ุญุงู ฺฉู ฒด ฺฏฺฏ ุฌุง ุฏุงุฑุฏ).

ฒ. ุงูุฒุงุด ุชุนุฏุงุฏ Workerูุง: ุชุนุฏุงุฏ num_workers ุฑุง ุงูุฒุงุด ุฏูุฏ ุชุง CPU ุจุง ูุณุชูโูุง ุจุดุชุฑ ุฏุงุฏูโูุง ุฑุง ุขูุงุฏู ฺฉูุฏ. ูุนูููุงู ุนุฏุฏ ด ุง ธ ุจุฑุง ูุฑ GPU ููุงุณุจ ุงุณุช (ุจุณุชู ุจู ูุฏุฑุช CPU ุณุฑูุฑ).

ฺฉุฏ ุงุตูุงุญ ุดุฏู (ุจุฎุด DataLoader)

ุงู ุชุบุฑุงุช ุฑุง ุฏุฑ ุชุงุจุน train_dist ุฏุฑ ูุงู ุฎูุฏ ุงุนูุงู ฺฉูุฏ:
Python

    # ... inside train_dist function ...

    # Increased batch_size from 32 to 256 or 512 to utilize RTX 3090 memory
    batch_size = 256 
    
    train_loader = torch.utils.data.DataLoader(
        train_set, 
        batch_size=batch_size, 
        sampler=train_sampler, 
        num_workers=4, # Increased workers
        pin_memory=True, 
        persistent_workers=True
    )

    test_loader = torch.utils.data.DataLoader(
        test_set, 
        batch_size=batch_size, 
        sampler=test_sampler, 
        num_workers=4, # Increased workers
        pin_memory=True, 
        persistent_workers=True
    )
    
    # ... rest of the code ...

ุฎูุงุตู: ูุดฺฉู ูู ูุณุชุ ุจูฺฉู ูุดฺฉู ุนุฏู ุชูุงุฒู ุณุฑุนุช ุงุณุช. ุจุง ุฏุงุฏู ููููโูุง ุจุฒุฑฺฏุชุฑ (Batch Size ุจุดุชุฑ) ุจู GPUุ ุงู ููุณุงูุงุช ฺฉูุชุฑ ุดุฏู ู ุณุฑุนุช ุขููุฒุด ุดูุง ุจู ุดุฏุช ุงูุฒุงุด ูโุงุจุฏ.

ุชุฑู ุชูุฒุน ูุดุฏู ฒ ุฏููู ุฒูุงู ุจุฑุฏ ูู ุงู ฺฉุฏ ฺฉู ุฏุงุฏ ุฏุฑ ฑฐ ุฏููู ุชููุง ฑณ ุฏุฑุตุฏ ุฌูู ุฑูุช ุฏุฑุณุชุด ฺฉู

ุงู ูุดฺฉู ุฏู ุฏูู ุงุตู ุฏุงุฑุฏ: ฑ. ุณุงุฒ ุฏุณุชู (Batch Size) ุจุณุงุฑ ฺฉูฺฺฉ: ุฏุฑ ฺฉุฏ ูุจู batch_size=32 ุจูุฏ. ุจุฑุง ฺฉุงุฑุช ูุฏุฑุชููุฏ ูุซู 3090ุ ุงู ุนุฏุฏ ุจุณุงุฑ ฺฉูฺฺฉ ุงุณุช. ุฏุฑ ุญุงูุช ุชูุฒุนโุดุฏู (DDP)ุ ุจุนุฏ ุงุฒ ูุฑ ณฒ ุชุตูุฑุ ฺฉุงุฑุชโูุง ุจุงุฏ ุตุจุฑ ฺฉููุฏ ุชุง ฺฏุฑุงุฏุงูโูุง ุฑุง ุจุง ูู ููฺฏุงูโุณุงุฒ (Sync) ฺฉููุฏ. ุงู "ุณุฑุจุงุฑ ุงุฑุชุจุงุท" (Communication Overhead) ุจุดุชุฑ ุงุฒ ุฒูุงู ูุญุงุณุจุงุช ุทูู ูโฺฉุดุฏ ู ุจุงุนุซ ฺฉูุฏ ุดุฏุฏ ูโุดูุฏ. ฒ. ูุงููุงููฺฏ ุฏุฑ ุงุฑุฒุงุจ (Evaluation Mismatch): ุฏุฑ ฺฉุฏ ูุจูุ ููุท Rank 0 ุงุฑุฒุงุจ (Evaluate) ุฑุง ุงูุฌุงู ูโุฏุงุฏ. ุฏุฑ ุงู ุญูุ Rank 1 ูุงุฑุฏ ุฏูุฑ ุจุนุฏ ุขููุฒุด ูโุดุฏ ู ููุชุธุฑ ูโูุงูุฏ ุชุง Rank 0 ุจุฑฺฏุฑุฏุฏ (ฺูู ุจุฑุง ููฺฏุงูโุณุงุฒ ฺฏุฑุงุฏุงูโูุง ุจู ุขู ูุงุฒ ุฏุงุฑุฏ). ุงู ุชููู ุจุงุนุซ ูโุดูุฏ ุนููุงู ูพุฑุฏุงุฒุด ููุงุฒ ุงุฒ ุจู ุจุฑูุฏ.

ุฏุฑ ฺฉุฏ ุฒุฑ ุงู ุงุตูุงุญุงุช ุงูุฌุงู ุดุฏู ุงุณุช:

    batch_size ุจู 256 ุงูุฒุงุด ุงูุช (ุณุฑุนุช ุฑุง ุจุณุงุฑ ุจุงูุง ูโุจุฑุฏ).

    num_workers ุจู 4 ุงูุฒุงุด ุงูุช.

    ุนููุงุช evaluate ุฑู ููู GPUูุง ุงุฌุฑุง ูโุดูุฏ ุชุง ุจุง ูู ููุงููฺฏ ุจูุงููุฏ (ุฌููฺฏุฑ ุงุฒ ููู ุดุฏู ูุฑุขูุฏ).

    ุงุฒ model.module ุฏุฑ ุฒูุงู ุงุฑุฒุงุจ ุงุณุชูุงุฏู ููโุดูุฏ ุชุง ูุดฺฉู ูพุด ูุงุฏุ ุงูุง ฺูู ููู ุจุง ูู ุงุฌุฑุง ูโฺฉููุฏุ ุณุฑุจุงุฑ ูุฏุงุฑุฏ.

ุงู ฺฉุฏ ุฑุง ุฏุฑ ฺฉ ุณููู ุงุฌุฑุง ฺฉูุฏ ุชุง ูุงู ุฌุฏุฏ ุณุงุฎุชู ุดูุฏุ ุณูพุณ ุฏุณุชูุฑ ุงุฌุฑุง ุฑุง ุจุฒูุฏ.
Python

%%writefile train_ddp_fixed.py
import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.multiprocessing as mp
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torchvision import datasets, transforms
import datetime

class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.depthwise = nn.Conv2d(
            in_channels, in_channels, kernel_size=3, padding=1, 
            stride=stride, groups=in_channels, bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        self.pointwise = nn.Conv2d(
            in_channels, out_channels, kernel_size=1, stride=1, bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)
        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2)
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(p=0.2)
        self.fc = nn.Linear(256, num_class)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

def setup(rank, world_size, master_port, backend, timeout):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = master_port
    dist.init_process_group(backend=backend, rank=rank, world_size=world_size, timeout=timeout)

def cleanup():
    dist.destroy_process_group()

def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return correct, total

def train_one_epoch(model, train_loader, optimizer, criterion, device, scheduler):
    model.train()
    running_loss = 0.0
    for images, labels in train_loader:
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()
        scheduler.step()
        
        running_loss += loss.item() * labels.size(0)
    return running_loss / len(train_loader.dataset)

def train_dist(rank, world_size, master_port, backend, timeout):
    setup(rank, world_size, master_port, backend, timeout)
    torch.cuda.set_device(rank)
    
    batch_size = 256
    num_workers = 4

    train_transform = transforms.Compose([
        transforms.RandomCrop(96, padding=12),      
        transforms.RandomHorizontalFlip(p=0.5),   
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), 
        transforms.RandomRotation(10),            
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    train_set = datasets.STL10('./data', split='train', download=True, transform=train_transform)
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_set, num_replicas=world_size, rank=rank
    )
    train_loader = torch.utils.data.DataLoader(
        train_set, batch_size=batch_size, sampler=train_sampler, 
        num_workers=num_workers, pin_memory=True, persistent_workers=True
    )

    test_set = datasets.STL10('./data', split='test', download=True, transform=test_transform)
    test_sampler = torch.utils.data.distributed.DistributedSampler(
        test_set, num_replicas=world_size, rank=rank, shuffle=False
    )
    test_loader = torch.utils.data.DataLoader(
        test_set, batch_size=batch_size, sampler=test_sampler, 
        num_workers=num_workers, pin_memory=True, persistent_workers=True
    )

    model = CNN(in_channel=3, num_class=10).to(rank)
    model = DDP(model, device_ids=[rank])
    
    epochs = 15
    max_lr = 1e-3
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer, max_lr=max_lr, epochs=epochs, steps_per_epoch=len(train_loader)
    )

    if rank == 0:
        print(f"Start Training on {world_size} GPUs. Batch Size per GPU: {batch_size}")

    start = time.time()
    for epoch in range(1, epochs + 1):
        train_sampler.set_epoch(epoch)
        avg_loss = train_one_epoch(model, train_loader, optimizer, criterion, rank, scheduler)
        
        local_correct, local_total = evaluate(model, test_loader, rank)
        
        # Simple logging from rank 0 (approximate accuracy)
        # To get exact accuracy, we would need all_reduce, but for speed check this is enough
        if rank == 0:
            acc = local_correct / local_total
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch}/{epochs}: Loss={avg_loss:.4f}, Local Test Acc={acc*100:.2f}%, LR={current_lr:.6f}")
            
    if rank == 0:
        total_time = time.time() - start
        print(f"Total Time: {total_time:.2f}s")
        
    cleanup()

if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    master_port = '12356' 
    backend = 'nccl'
    timeout = datetime.timedelta(seconds=60)
    
    if world_size > 0:
        mp.spawn(
            train_dist, 
            args=(world_size, master_port, backend, timeout), 
            nprocs=world_size, 
            join=True
        )
    else:
        print("No GPU detected.")

ุญุงูุง ุงู ุฏุณุชูุฑ ุฑุง ุงุฌุฑุง ฺฉูุฏ:
Python

!python train_ddp_fixed.py

 %%writefile train_ddp.py

import os, time

import numpy as np


import torch

import torch.nn as nn

import torch.nn.functional as F

from torch.utils.cpp_extension import load_inline

from torchvision import datasets, transforms

import torch.optim as optim

import datetime


import torch.distributed as dist

import torch.multiprocessing as mp


def evaluate(model, loader, device):

model.eval()

correct = 0

total = 0

with torch.no_grad():

for images, labels in loader:

images = images.to(device)

labels = labels.to(device)

outputs = model(images)

preds = outputs.argmax(dim=1)

correct += (preds == labels).sum().item()

total += labels.size(0)

return correct / total


class DepthwiseSeparableBlock(nn.Module):

def __init__(self, in_channels, out_channels, stride=1):

super().__init__()

self.depthwise = nn.Conv2d(

in_channels,

in_channels,

kernel_size=3,

padding=1,

stride=stride,

groups=in_channels,

bias=False

)

self.bn_dw = nn.BatchNorm2d(in_channels)

self.pointwise = nn.Conv2d(

in_channels,

out_channels,

kernel_size=1,

stride=1,

bias=False

)

self.bn_pw = nn.BatchNorm2d(out_channels)

self.shortcut = nn.Sequential()

if stride != 1 or in_channels != out_channels:

self.shortcut = nn.Sequential(

nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),

nn.BatchNorm2d(out_channels)

)


def forward(self, x):

out = F.relu(self.bn_dw(self.depthwise(x)))

out = self.bn_pw(self.pointwise(out))

out += self.shortcut(x)

out = F.relu(out)

return out


class CNN(nn.Module):

def __init__(self, in_channel=3, num_class=10):

super().__init__()

self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)

self.bn1 = nn.BatchNorm2d(64)

self.pool1 = nn.MaxPool2d(2)


self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)

self.block2 = DepthwiseSeparableBlock(128, 256, stride=2)

self.block3 = DepthwiseSeparableBlock(256, 256, stride=2)

self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))

self.dropout = nn.Dropout(p=0.2)

self.fc = nn.Linear(256, num_class)


def forward(self, x):

x = F.relu(self.bn1(self.conv1(x)))

x = self.pool1(x)

x = self.block1(x)

x = self.block2(x)

x = self.block3(x)

x = self.avg_pool(x)

x = x.view(x.size(0), -1)

x = self.dropout(x)

x = self.fc(x)

return x

def setup(rank, world_size, master_port, backend, timeout):

os.environ['MASTER_ADDR'] = 'localhost'

os.environ['MASTER_PORT'] = master_port

dist.init_process_group(backend=backend, rank=rank, world_size=world_size, timeout=timeout)

def load_data(rank, world_size):

train_set = datasets.STL10('../data', split='train', download=True, transform=train_transform)

train_sampler = torch.utils.data.distributed.DistributedSampler(

train_set,

num_replicas=world_size,

rank=rank

)


train_loader = torch.utils.data.DataLoader(

dataset=train_set,

sampler=train_sampler,

batch_size=batch_size,

shuffle=False,

persistent_workers=True,

num_workers=1,

pin_memory=True

)


test_set = datasets.STL10('../data', split='test', download=True, transform=test_transform)

test_sampler = torch.utils.data.distributed.DistributedSampler(

test_set,

num_replicas=world_size,

rank=rank

)


test_loader = torch.utils.data.DataLoader(

dataset=test_set,

sampler=test_sampler,

batch_size=batch_size,

shuffle=False,

persistent_workers=True,

num_workers=1,

pin_memory=True

)


return train_loader, test_loader


def train_dist(rank, world_size, master_port, backend, timeout):

setup(rank, world_size, master_port, backend, timeout)

torch.set_device(rank)

train_loader, test_loader = load_data(rank, world_size)

model = CNN(in_channel=3, num_class=10).to(rank)

ddp_model = DDP(model, device_ids=[rank])

train_model_fast(ddp_model, train_loader, test_loader, rank, epochs=15)

if __name__ == '__main__':

world_size = torch.cuda.device_count()

master_port = '8282'

backend = 'nccl'

timeout = datetime.timedelta(seconds=10)

start_time = time.time()

mp.spawn(train_dist, nprocs=world_size, args=(world_size, master_port, backend, timeout), join=True)

end_time = time.time()

print("Total time: {}".format(end_time - start_time))



[rank0]:[W1125 15:04:57.952738677 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())W1125 15:04:57.609000 63189 my_pytorch/lib/python3.13/site-packages/torch/multiprocessing/spawn.py:174] Terminating process 63201 via signal SIGTERMTraceback (most recent call last): File "/home/dmls/vajhi/train_ddp.py", line 164, in <module> mp.spawn(train_dist, nprocs=world_size, args=(world_size, master_port, backend, timeout), join=True) ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File "/home/dmls/vajhi/my_pytorch/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 364, in spawn return start_processes(fn, args, nprocs, join, daemon, start_method="spawn") File "/home/dmls/vajhi/my_pytorch/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 320, in start_processes while not context.join(): ~~~~~~~~~~~~^^ File "/home/dmls/vajhi/my_pytorch/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 220, in join raise ProcessRaisedException(msg, error_index, failed_process.pid)torch.multiprocessing.spawn.ProcessRaisedException: -- Process 0 terminated with the following error:Traceback (most recent call last): File "/home/dmls/vajhi/my_pytorch/lib/python3.13/site-packages/torch/multiprocessing/spawn.py", line 95, in _wrap fn(i, *args) ~~^^^^^^^^^^ File "/home/dmls/vajhi/train_ddp.py", line 152, in train_dist torch.set_device(rank) ^^^^^^^^^^^^^^^^ File "/home/dmls/vajhi/my_pytorch/lib/python3.13/site-packages/torch/__init__.py", line 2757, in __getattr__ raise AttributeError(f"module '{__name__}' has no attribute '{name}'")AttributeError: module 'torch' has no attribute 'set_device'



ุจฺฏู ฺุทูุฑ ุฏุฑุณุชุด ฺฉููุ

ูุดฺฉู ุงุตู ฺฉุฏ ุดูุง ุงู ุงุณุช ฺฉู ูุชุฏ set_device ูุณุชููุงู ุฏุฑ ูุงฺูู torch ูุฌูุฏ ูุฏุงุฑุฏุ ุจูฺฉู ุฏุฑ ูุงฺูู torch.cuda ูุฑุงุฑ ุฏุงุฑุฏ. ุจูุงุจุฑุงู ุฎุท ฑตฒ ุจุงุฏ ุชุบุฑ ฺฉูุฏ.

ููฺูู ุฏุฑ ฺฉุฏ ุดูุง DDP (DistributedDataParallel) ุงููพูุฑุช ูุดุฏู ุจูุฏ ู ุฏุฑ ุงูุชูุง ูพุฑุฏุงุฒุด ูู destroy_process_group ุจุฑุง ุขุฒุงุฏุณุงุฒ ููุงุจุน ุตุฏุง ุฒุฏู ูุดุฏู ุจูุฏ ฺฉู ุจุงุนุซ ุขู ูุงุฑููฺฏ ุงูู ูโุดูุฏ.

ุจุฑุง ุฑูุน ุฎุทุงุ ุชุบุฑุงุช ุฒุฑ ุฑุง ุงุนูุงู ฺฉูุฏ:

ฑ. ุงุตูุงุญ ูุฑุงุฎูุงู Device: ุชุจุฏู torch.set_device ุจู torch.cuda.set_device. ฒ. ุงููพูุฑุช ฺฉุฑุฏู DDP: ุงุถุงูู ฺฉุฑุฏู ุงููพูุฑุช ูุฑุจูุทู. ณ. ูพุงฺฉุณุงุฒ: ุงุถุงูู ฺฉุฑุฏู dist.destroy_process_group() ุฏุฑ ุงูุชูุง ุชุงุจุน ุขููุฒุด.

ุฏุฑ ุงูุฌุง ฺฉุฏ ุงุตูุงุญ ุดุฏู (ุจุฎุดโูุง ุชุบุฑ ุงูุชู) ุฑุง ุขูุฑุฏูโุงู:
Python

import os, time
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
# Added necessary import for DDP
from torch.nn.parallel import DistributedDataParallel as DDP 
from torchvision import datasets, transforms
import torch.optim as optim
import datetime

import torch.distributed as dist
import torch.multiprocessing as mp

# ... (Classes and other functions remain the same) ...

def train_dist(rank, world_size, master_port, backend, timeout):
    setup(rank, world_size, master_port, backend, timeout)
    
    # FIX: Use torch.cuda.set_device instead of torch.set_device
    torch.cuda.set_device(rank)
    
    train_loader, test_loader = load_data(rank, world_size)
    model = CNN(in_channel=3, num_class=10).to(rank)
    
    ddp_model = DDP(model, device_ids=[rank])
    
    # Assuming train_model_fast is defined elsewhere in your code
    train_model_fast(ddp_model, train_loader, test_loader, rank, epochs=15)
    
    # Clean up the process group to avoid resource leaks
    dist.destroy_process_group()

if __name__ == '__main__':
    # Ensure CUDA is available before running
    if torch.cuda.is_available():
        world_size = torch.cuda.device_count()
        master_port = '8282'
        backend = 'nccl'
        timeout = datetime.timedelta(seconds=10)
        start_time = time.time()
        mp.spawn(train_dist, nprocs=world_size, args=(world_size, master_port, backend, timeout), join=True)
        end_time = time.time()
        print("Total time: {}".format(end_time - start_time))
    else:
        print("CUDA is not available.")

ูฺฉุชู: ุฏุฑ ฺฉุฏ ฺฉู ูุฑุณุชุงุฏุฏ ุชูุงุจุน train_model_fast ู ูุชุบุฑูุง train_transform/test_transform ู batch_size ุชุนุฑู ูุดุฏู ุจูุฏูุฏ. ูุฑุถ ูู ุงู ุงุณุช ฺฉู ุงูโูุง ุฑุง ุฏุฑ ุจุฎุดโูุง ุฏฺฏุฑ ฺฉุฏ ุฏุงุฑุฏุ ุงูุง ุงฺฏุฑ ูุฏุงุฑุฏ ุจุงุฏ ุขูโูุง ุฑุง ูู ุงุถุงูู ฺฉูุฏ ุชุง ฺฉุฏ ุงุฌุฑุง ุดูุฏ.

28 PM

%%writefile train_ddp.py

import os, time

import numpy as np


import torch

import torch.nn as nn

import torch.nn.functional as F

from torch.utils.cpp_extension import load_inline

from torchvision import datasets, transforms

import torch.optim as optim

import datetime


import torch.distributed as dist

import torch.multiprocessing as mp

from torch.nn.parallel import DistributedDataParallel as DDP


BATCH_SIZE = 32

EPOCHS = 15



def train_model_fast(model, train_loader, test_loader, device,

epochs=10, max_lr=1e-3):

optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)

criterion = nn.CrossEntropyLoss()

scheduler = optim.lr_scheduler.OneCycleLR(

optimizer,

max_lr=max_lr,

epochs=epochs,

steps_per_epoch=len(train_loader)

)

start = time.time()

cuda_mem = 0

for epoch in range(1, epochs + 1):

model.train()

running_loss = 0.0


for images, labels in train_loader:

images = images.to(device, non_blocking=True)

labels = labels.to(device, non_blocking=True)


outputs = model(images)

loss = criterion(outputs, labels)


optimizer.zero_grad(set_to_none=True)

loss.backward()

optimizer.step()

scheduler.step()

cuda_mem = torch.cuda.max_memory_allocated(device=device)


running_loss += loss.item() * labels.size(0)


avg_loss = running_loss / len(train_loader.dataset)

acc = evaluate(model, test_loader, device)

current_lr = optimizer.param_groups[0]['lr']

print(

f"Rank={device} "

f"Epoch {epoch}/{epochs}: "

f"loss={avg_loss:.4f}, "

f"test_acc={acc*100:.2f}%, "

f"LR={current_lr:.6f}, "

)

end = time.time()


print(f"Rank={device} Total Time={end-start}")

print(f"Rank={device} Mem Usage={cuda_mem / (1024**2)} MB")

return model


def evaluate(model, loader, device):

model.eval()

correct = 0

total = 0

with torch.no_grad():

for images, labels in loader:

images = images.to(device)

labels = labels.to(device)

outputs = model(images)

preds = outputs.argmax(dim=1)

correct += (preds == labels).sum().item()

total += labels.size(0)

return correct / total


class DepthwiseSeparableBlock(nn.Module):

def __init__(self, in_channels, out_channels, stride=1):

super().__init__()

self.depthwise = nn.Conv2d(

in_channels,

in_channels,

kernel_size=3,

padding=1,

stride=stride,

groups=in_channels,

bias=False

)

self.bn_dw = nn.BatchNorm2d(in_channels)

self.pointwise = nn.Conv2d(

in_channels,

out_channels,

kernel_size=1,

stride=1,

bias=False

)

self.bn_pw = nn.BatchNorm2d(out_channels)

self.shortcut = nn.Sequential()

if stride != 1 or in_channels != out_channels:

self.shortcut = nn.Sequential(

nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),

nn.BatchNorm2d(out_channels)

)


def forward(self, x):

out = F.relu(self.bn_dw(self.depthwise(x)))

out = self.bn_pw(self.pointwise(out))

out += self.shortcut(x)

out = F.relu(out)

return out


class CNN(nn.Module):

def __init__(self, in_channel=3, num_class=10):

super().__init__()

self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)

self.bn1 = nn.BatchNorm2d(64)

self.pool1 = nn.MaxPool2d(2)


self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)

self.block2 = DepthwiseSeparableBlock(128, 256, stride=2)

self.block3 = DepthwiseSeparableBlock(256, 256, stride=2)

self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))

self.dropout = nn.Dropout(p=0.2)

self.fc = nn.Linear(256, num_class)


def forward(self, x):

x = F.relu(self.bn1(self.conv1(x)))

x = self.pool1(x)

x = self.block1(x)

x = self.block2(x)

x = self.block3(x)

x = self.avg_pool(x)

x = x.view(x.size(0), -1)

x = self.dropout(x)

x = self.fc(x)

return x

def setup(rank, world_size, master_port, backend, timeout):

os.environ['MASTER_ADDR'] = 'localhost'

os.environ['MASTER_PORT'] = master_port

dist.init_process_group(backend=backend, rank=rank, world_size=world_size, timeout=timeout)

def load_data(rank, world_size):

batch_size = BATCH_SIZE

train_transform = transforms.Compose([

transforms.RandomCrop(96, padding=12),

transforms.RandomHorizontalFlip(p=0.5),

transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),

transforms.RandomRotation(10),

transforms.ToTensor(),

transforms.Normalize(mean=[0.485, 0.456, 0.406],

std=[0.229, 0.224, 0.225])

])

test_transform = transforms.Compose([

transforms.ToTensor(),

transforms.Normalize(mean=[0.485, 0.456, 0.406],

std=[0.229, 0.224, 0.225])

])

train_set = datasets.STL10('/storage/dmls/stl10_data', split='train', transform=train_transform)

train_sampler = torch.utils.data.distributed.DistributedSampler(

train_set,

num_replicas=world_size,

rank=rank

)


train_loader = torch.utils.data.DataLoader(

dataset=train_set,

sampler=train_sampler,

batch_size=batch_size,

shuffle=False,

persistent_workers=True,

num_workers=1,

pin_memory=True

)


test_set = datasets.STL10('/storage/dmls/stl10_data', split='test', transform=test_transform)

test_sampler = torch.utils.data.distributed.DistributedSampler(

test_set,

num_replicas=world_size,

rank=rank

)


test_loader = torch.utils.data.DataLoader(

dataset=test_set,

sampler=test_sampler,

batch_size=batch_size,

shuffle=False,

persistent_workers=True,

num_workers=1,

pin_memory=True

)


return train_loader, test_loader


def train_dist(rank, world_size, master_port, backend, timeout):

setup(rank, world_size, master_port, backend, timeout)

torch.cuda.set_device(rank)

train_loader, test_loader = load_data(rank, world_size)

model = CNN(in_channel=3, num_class=10).to(rank)

ddp_model

= DDP(model, device_ids=[rank])

train_model_fast(ddp_model, train_loader, test_loader, rank, epochs=EPOCHS)

if __name__ == '__main__':

world_size = torch.cuda.device_count()

master_port = '8282'

backend = 'nccl'

timeout = datetime.timedelta(seconds=10)

start_time = time.time()

mp.spawn(train_dist, nprocs=world_size, args=(world_size, master_port, backend, timeout), join=True)

end_time = time.time()

print("Total time: {}".format(end_time - start_time))


ุงูู ุฏุงุฑู ู ุฎูุงู ููุช ุชู ููุช ุจูฺฉ ุงุฌุฑุง ู ฺฉูู ุจุชููู ุงูุฏุงุฒู ุจฺ ู backend ุฑู ุชุบุฑ ุจุฏู

ุจุฑุง ุงูฺฉู ุจุชูุงูุฏ Batch Size ู Backend (ู ุณุงุฑ ูพุงุฑุงูุชุฑูุง ูุซู ุชุนุฏุงุฏ Epochูุง) ุฑุง ููฺฏุงู ุงุฌุฑุง ุฏุฑ ููุชโุจูฺฉ ุจู ุฑุงุญุช ุชุบุฑ ุฏูุฏุ ุจูุชุฑู ุฑุงู ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงูู argparse ุงุณุช.

ุจุง ุงู ุชุบุฑุงุชุ ุดูุง ุฏฺฏุฑ ูุงุฒ ุจู ูุฑุงุด ูุงู .py ุจุฑุง ูุฑ ุชุบุฑ ฺฉูฺฺฉ ูุฏุงุฑุฏ ู ูโุชูุงูุฏ ูพุงุฑุงูุชุฑูุง ุฑุง ููฺฏุงู ุงุฌุฑุง ุฏุณุชูุฑ ุจู ุงุณฺฉุฑูพุช ูพุงุณ ุฏูุฏ.

ุชุบุฑุงุช ุฒุฑ ุฑุง ุฏุฑ ฺฉุฏ ุงุนูุงู ฺฉุฑุฏู:

    ุงุถุงูู ฺฉุฑุฏู argparse ุจุฑุง ุฏุฑุงูุช ูุฑูุฏโูุง ุงุฒ ุฎุท ูุฑูุงู.

    ุญุฐู ูุชุบุฑูุง Global ุซุงุจุช ู ุงูุชูุงู ุขูโูุง ุจู ูุฑูุฏ ุชูุงุจุน.

    ุงุตูุงุญ ุชุงุจุน mp.spawn ุจุฑุง ุงุฑุณุงู ุขุฑฺฏููุงูโูุง ุฌุฏุฏ ุจู ุชุงุจุน train_dist.

    (ุงุตูุงุญ ุฌุฒุฆ) ุฑูุน ุฎุทุง Syntax ุฏุฑ ุฎุท ูุฑุจูุท ุจู ุชุนุฑู ddp_model.

ฺฉุฏ ุงุตูุงุญ ุดุฏู (train_ddp.py)

Python

%%writefile train_ddp.py
import os
import time
import datetime
import argparse
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms

import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

def train_model_fast(model, train_loader, test_loader, device, 
                       epochs=10, max_lr=1e-3):
    
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)
    criterion = nn.CrossEntropyLoss()
    
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=max_lr,
        epochs=epochs,
        steps_per_epoch=len(train_loader)
    )
    
    start = time.time()
    cuda_mem = 0
    
    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        
        for images, labels in train_loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            outputs = model(images)
            loss = criterion(outputs, labels)

            optimizer.zero_grad(set_to_none=True)
            
            loss.backward()
            optimizer.step()
            
            scheduler.step()
            cuda_mem = torch.cuda.max_memory_allocated(device=device)

            running_loss += loss.item() * labels.size(0)

        avg_loss = running_loss / len(train_loader.dataset)
        
        acc = evaluate(model, test_loader, device)
        
        current_lr = optimizer.param_groups[0]['lr']
        
        if device == 0:
            print(
                f"Rank={device} "
                f"Epoch {epoch}/{epochs}: "
                f"loss={avg_loss:.4f}, "
                f"test_acc={acc*100:.2f}%, "
                f"LR={current_lr:.6f}, "
            )
        
    end = time.time()

    if device == 0:
        print(f"Rank={device} Total Time={end-start}")
        print(f"Rank={device} Mem Usage={cuda_mem / (1024**2)} MB")
    
    return model

def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return correct / total

class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        self.depthwise = nn.Conv2d(
            in_channels, 
            in_channels, 
            kernel_size=3, 
            padding=1, 
            stride=stride, 
            groups=in_channels, 
            bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        
        self.pointwise = nn.Conv2d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            stride=1, 
            bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x) 
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) 

        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) 
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) 
        
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) 
        
        self.dropout = nn.Dropout(p=0.2) 
        self.fc = nn.Linear(256, num_class) 

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x
    
def setup(rank, world_size, master_port, backend, timeout):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = master_port
    dist.init_process_group(backend=backend, rank=rank, world_size=world_size, timeout=timeout)
    
def load_data(rank, world_size, batch_size):
    train_transform = transforms.Compose([
        transforms.RandomCrop(96, padding=12),      
        transforms.RandomHorizontalFlip(p=0.5),   
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), 
        transforms.RandomRotation(10),            
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
    ])
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                            std=[0.229, 0.224, 0.225])
    ])
    
    # Note: Ensure the data path is correct or passed as argument as well
    train_set = datasets.STL10('./data', split='train', transform=train_transform, download=True)
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_set,
        num_replicas=world_size,
        rank=rank
    )

    train_loader = torch.utils.data.DataLoader(
        dataset=train_set,
        sampler=train_sampler,
        batch_size=batch_size,
        shuffle=False,
        persistent_workers=True,
        num_workers=2,
        pin_memory=True
    )

    test_set = datasets.STL10('./data', split='test', transform=test_transform, download=True)
    test_sampler = torch.utils.data.distributed.DistributedSampler(
        test_set,
        num_replicas=world_size,
        rank=rank
    )

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set,
        sampler=test_sampler,
        batch_size=batch_size,
        shuffle=False,
        persistent_workers=True,
        num_workers=2,
        pin_memory=True
    )

    return train_loader, test_loader

def train_dist(rank, world_size, master_port, backend, timeout, batch_size, epochs):
    setup(rank, world_size, master_port, backend, timeout)
    torch.cuda.set_device(rank)
    
    train_loader, test_loader = load_data(rank, world_size, batch_size)
    
    model = CNN(in_channel=3, num_class=10).to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    
    train_model_fast(ddp_model, train_loader, test_loader, rank, epochs=epochs)
    
    dist.destroy_process_group()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='DDP Training Script')
    parser.add_argument('--batch-size', type=int, default=32, help='Input batch size for training')
    parser.add_argument('--backend', type=str, default='nccl', help='Distributed backend')
    parser.add_argument('--epochs', type=int, default=15, help='Number of epochs to train')
    parser.add_argument('--master-port', type=str, default='8282', help='Master port for DDP')
    
    args = parser.parse_args()
    
    world_size = torch.cuda.device_count()
    timeout = datetime.timedelta(seconds=60)
    
    print(f"Running DDP with backend={args.backend}, batch_size={args.batch_size}, epochs={args.epochs}")
    
    start_time = time.time()
    mp.spawn(
        train_dist, 
        nprocs=world_size, 
        args=(world_size, args.master_port, args.backend, timeout, args.batch_size, args.epochs), 
        join=True
    )
    end_time = time.time()
    print("Total time: {}".format(end_time - start_time))

ูุญูู ุงุฌุฑุง ุฏุฑ ููุชโุจูฺฉ

ุญุงูุง ฺฉู ูุงู ุฐุฎุฑู ุดุฏุ ูโุชูุงูุฏ ุฏุฑ ุณูููโูุง ุจุนุฏ ููุชโุจูฺฉ ุฎูุฏุ ุงุณฺฉุฑูพุช ุฑุง ุจุง ูพุงุฑุงูุชุฑูุง ูุฎุชูู ุงุฌุฑุง ฺฉูุฏ:

ุงุฌุฑุง ูพุดโูุฑุถ:
Python

!python train_ddp.py

ุงุฌุฑุง ุจุง ุชุบุฑ Batch Size:
Python

!python train_ddp.py --batch-size 64

ุงุฌุฑุง ุจุง ุชุบุฑ Backend ู Batch Size:
Python

!python train_ddp.py --batch-size 128 --backend gloo

ุขุง ูโุฎูุงูุฏ ุจุฎุด ุชูุธู ุฏุงุฑฺฉุชูุฑ ุฏุชุง (/storage/dmls/stl10_data) ุฑุง ูู ุจู ฺฉ ุขุฑฺฏููุงู ูุงุจู ุชุบุฑ ุชุจุฏู ฺฉููุ

ูู ุงูุฌูุฑ ูุฏู ุชููุง ุจุฎุด ูุง ฺฉู ุจุงุฏ ุชุบุฑ ฺฉูู ุฑู ุจุฏู

ุจุณุงุฑ ุฎุจุ ููุท ุจุฎุดโูุง ฺฉู ุจุงุฏ ุฏุฑ ูุงู train_ddp.py ุชุบุฑ ฺฉููุฏ ุฑุง ุฏุฑ ุงุฏุงูู ุขูุฑุฏูโุงู.

ุชุบุฑุงุช ุดุงูู ด ุจุฎุด ุงุณุช: ุงุถุงูู ฺฉุฑุฏู importุ ุชุบุฑ ูุฑูุฏ ุชุงุจุน load_dataุ ุชุบุฑ ูุฑูุฏ ุชุงุจุน train_dist ู ุจูโุฑูุฒุฑุณุงู ุจุฎุด main.

ฑ. ุงุถุงูู ฺฉุฑุฏู ฺฉุชุงุจุฎุงูู ุฏุฑ ุงุจุชุฏุง ูุงู:
Python

import argparse

ฒ. ุชุบุฑ ุชุนุฑู ุชุงุจุน load_data ุจุฑุง ุฏุฑุงูุช batch_size:
Python

def load_data(rank, world_size, batch_size):
    # ... (rest of transform definitions)

    train_set = datasets.STL10('/storage/dmls/stl10_data', split='train', transform=train_transform)
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_set,
        num_replicas=world_size,
        rank=rank
    )

    train_loader = torch.utils.data.DataLoader(
        dataset=train_set,
        sampler=train_sampler,
        batch_size=batch_size, 
        shuffle=False,
        persistent_workers=True,
        num_workers=1,
        pin_memory=True
    )

    test_set = datasets.STL10('/storage/dmls/stl10_data', split='test', transform=test_transform)
    test_sampler = torch.utils.data.distributed.DistributedSampler(
        test_set,
        num_replicas=world_size,
        rank=rank
    )

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set,
        sampler=test_sampler,
        batch_size=batch_size,
        shuffle=False,
        persistent_workers=True,
        num_workers=1,
        pin_memory=True
    )

    return train_loader, test_loader

ณ. ุชุบุฑ ุชุนุฑู ุชุงุจุน train_dist ุจุฑุง ุงูุชูุงู ุขุฑฺฏููุงูโูุง:
Python

def train_dist(rank, world_size, master_port, backend, timeout, batch_size):
    setup(rank, world_size, master_port, backend, timeout)
    torch.cuda.set_device(rank)
    
    train_loader, test_loader = load_data(rank, world_size, batch_size)
    
    model = CNN(in_channel=3, num_class=10).to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    train_model_fast(ddp_model, train_loader, test_loader, rank, epochs=EPOCHS)

ด. ุชุบุฑ ุจุฎุด if __name__ == '__main__': ุจุฑุง ุฎูุงูุฏู ุขุฑฺฏููุงูโูุง:
Python

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--backend', type=str, default='nccl')
    args = parser.parse_args()

    world_size = torch.cuda.device_count()
    master_port = '8282'
    timeout = datetime.timedelta(seconds=10)
    
    start_time = time.time()
    mp.spawn(
        train_dist, 
        nprocs=world_size, 
        args=(world_size, master_port, args.backend, timeout, args.batch_size), 
        join=True
    )
    end_time = time.time()
    print("Total time: {}".format(end_time - start_time))

ุญุงูุง ูโุชูุงูุฏ ุฏุฑ ููุชโุจูฺฉ ุจู ุงู ุตูุฑุช ุงุฌุฑุง ฺฉูุฏ:
Python

!python train_ddp.py --batch_size 64 --backend gloo

[rank0]:[W1125 15:20:01.382286504 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())


ุงู ุงุฑูุฑ ูุงุณู ฺูุ

ุงู ูุดุฏุงุฑ (Warning) ุจู ุงู ุฏูู ุงุณุช ฺฉู ุงุฑุชุจุงุท ุจู ูพุฑูุณูโูุง ุชูุฒุนโุดุฏู (Distributed Process Group) ูพุณ ุงุฒ ุงุชูุงู ฺฉุงุฑ ุจู ุฏุฑุณุช ุจุณุชู ูุดุฏู ุงุณุช.

ููุช ุดูุง dist.init_process_group ุฑุง ุฏุฑ ุชุงุจุน setup ุงุฌุฑุง ูโฺฉูุฏุ ููุงุจุน ุจุฑุง ุงุฑุชุจุงุท ุจู GPUูุง ุฏุฑฺฏุฑ ูโุดูุฏ. ุงฺฏุฑ ุฏุฑ ุงูุชูุง ฺฉุงุฑ dist.destroy_process_group() ุฑุง ุตุฏุง ูุฒูุฏุ ุงู ููุงุจุน ููฺฉู ุงุณุช ุฏุฑ ุญุงูุธู ุจุงู ุจูุงููุฏ (Memory Leak) ฺฉู ุฏุฑ ุงุฌุฑุงูุง ุจุนุฏ ูโุชูุงูุฏ ูุดฺฉูโุณุงุฒ ุดูุฏ.

ุจุฑุง ุฑูุน ุงู ูุดฺฉูุ ุจุงุฏ ุฏุฑ ุงูุชูุง ุชุงุจุน train_dist ุฏุณุชูุฑ ุจุณุชู ุดุฏู ฺฏุฑูู ุฑุง ุงุถุงูู ฺฉูุฏ:
Python

def train_dist(rank, world_size, master_port, backend, timeout, batch_size):
    setup(rank, world_size, master_port, backend, timeout)
    torch.cuda.set_device(rank)
    
    train_loader, test_loader = load_data(rank, world_size, batch_size)
    
    model = CNN(in_channel=3, num_class=10).to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    train_model_fast(ddp_model, train_loader, test_loader, rank, epochs=EPOCHS)

    dist.destroy_process_group()

ุจุง ุงุถุงูู ฺฉุฑุฏู ุฎุท ุขุฎุฑุ ุงู ูุดุฏุงุฑ ุจุฑุทุฑู ูโุดูุฏ.

 PM

ุณูุงู ฑ ุจุฎุด ด ุฑู ุจุง ุชูุฌู ุจู ููุช ุจูฺฉ ูพุงุณุฎ ุจุฏู

ุจุฑ ุงุณุงุณ ุฎุฑูุฌโูุง ููุฌูุฏ ุฏุฑ ูุงู Q1.ipynb ฺฉู ุงุฌุฑุง ุดุฏู ุงุณุชุ ูุชุงุฌ ุจูฺูุงุฑฺฉ ุจุฑุง ุจุฎุด ฑ.ด (ุจูฺูุงุฑฺฉ ฺฉุฑุฏู ูุงู ุฌุฏุฏ) ุฏุฑ ุฏู ุณูุงุฑู ฑฐ ูุงู ู ฒ ูุงู ฺฉุงูููููุดู ุจู ุดุฑุญ ุฒุฑ ุงุณุช.

ุฏุงุฏูโูุง ุงุฒ ุขุฎุฑู ุงูพุงฺฉ (Epoch 5) ุงุณุชุฎุฑุงุฌ ุดุฏูโุงูุฏ ุชุง ูพุงุฏุงุฑุชุฑู ูุถุนุช ุฒูุงูโุจูุฏ ุฑุง ูุดุงู ุฏููุฏ.

ฑ. ูุชุงุฌ ุขุฒูุงุด ุจุง ฑฐ ูุงู ฺฉุงูููููุดู (10 Convolution Layers)

ุฏุฑ ุงู ุขุฒูุงุดุ ุจู ุฏูู ุนุฏู ุงุณุชูุงุฏู ุงุฒ ูฺฉุงูุฒูโูุง ูุงููุฏ Batch Normalization ุฏุฑ ุดุจฺฉู ุนููโุชุฑุ ูุฏู ููฺฏุฑุง ูุดุฏู ู ุฏูุช ุฑู ฑฑ.ณตูช ุซุงุจุช ูุงูุฏู ุงุณุชุ ุงูุง ุฒูุงูโูุง ุงุฌุฑุง ุจุฑุง ููุงุณู ุณุฑุนุช ฺฉุงููุงู ูุนุชุจุฑ ูุณุชูุฏ.

ุณูุงุฑู	ุฏูุช ููุง (Test Acc)	ุฒูุงู ฺฉู ูุฑ ุงูพุงฺฉ (Total Time)	ุฒูุงู Forward	ุฒูุงู Backward	ุชูุถุญุงุช
ฑ. ุงุณุชุงูุฏุงุฑุฏ GPU (cuDNN)	11.35%	~5.42s	1.09s	2.57s	ุญุงูุช ูพุงู ู ุจููู PyTorch
ฒ. ฺฉุฑูู Fused (ูุงู ุฌุฏุฏ)	11.35%	~5.76s	0.69s	3.87s	ุณุฑุนโุชุฑ ุฏุฑ Forwardุ ฺฉูุฏุชุฑ ุฏุฑ Backward
ณ. ุงุณุชุงูุฏุงุฑุฏ GPU (ุจุฏูู cuDNN)	11.35%	~63.84s	19.21s	44.08s	ุจุณุงุฑ ฺฉูุฏ ุจู ุฏูู ุนุฏู ุจูููโุณุงุฒ
ด. ูพุฑุฏุงุฒูุฏู (CPU)	11.35%	~26.46s	4.13s	21.72s	ฺฉูุฏุชุฑ ุงุฒ cuDNN ุงูุง ุณุฑุนโุชุฑ ุงุฒ ุญุงูุช ุจุฏูู cuDNN

ฒ. ูุชุงุฌ ุขุฒูุงุด ุจุง ฒ ูุงู ฺฉุงูููููุดู (2 Convolution Layers)

ุฏุฑ ุงู ุณูุงุฑูุ ุดุจฺฉู ุจู ุฎูุจ ุขููุฒุด ุฏุฏู ู ุจู ุฏูุช ุจุงูุง นดูช ุฑุณุฏู ุงุณุช.

ุณูุงุฑู	ุฏูุช ููุง (Test Acc)	ุฒูุงู ฺฉู ูุฑ ุงูพุงฺฉ (Total Time)	ุฒูุงู Forward	ุฒูุงู Backward	ุชูุถุญุงุช
ฑ. ุงุณุชุงูุฏุงุฑุฏ GPU (cuDNN)	94.14%	~4.87s	0.48s	1.01s	ุจูุชุฑู ุฒูุงู ฺฉู
ฒ. ฺฉุฑูู Fused (ูุงู ุฌุฏุฏ)	94.12%	~5.03s	0.38s	1.29s	ุจูุจูุฏ ุณุฑุนุช ุฏุฑ Forward ูุดููุฏ ุงุณุช
ณ. ุงุณุชุงูุฏุงุฑุฏ GPU (ุจุฏูู cuDNN)	93.79%	~14.40s	4.94s	8.94s	ุงูุช ุดุฏุฏ ุณุฑุนุช
ด. ูพุฑุฏุงุฒูุฏู (CPU)	93.76%	~6.75s	1.35s	4.49s	ุนููฺฉุฑุฏ ูุชูุณุท

ุชุญูู ูุชุงุฌ (ุฎูุงุตู)

    ุณุฑุนุช Forward: ุฏุฑ ูุฑ ุฏู ุญุงูุช (ฒ ูุงู ู ฑฐ ูุงู)ุ ฺฉุฑูู Fused ููุดุชู ุดุฏู ุณุฑุนโุชุฑ ุงุฒ ูพุงุฏูโุณุงุฒ ุงุณุชุงูุฏุงุฑุฏ PyTorch (cuDNN) ุฏุฑ ูุฑุญูู Forward ุนูู ฺฉุฑุฏู ุงุณุช (ูุซูุงู ฐ.ถน ุซุงูู ุฏุฑ ุจุฑุงุจุฑ ฑ.ฐน ุซุงูู ุฏุฑ ุญุงูุช ฑฐ ูุงู). ุงู ูุดุงูโุฏููุฏู ฺฉุงุฑุง ุจุงูุง ุงุฏุบุงู ุนููุงุช Convolution ู ReLU ุฏุฑ ฺฉ ฺฉุฑูู ูุงุญุฏ ุงุณุช.

    ุณุฑุนุช Backward: ฺฉุฑูู Fused ุฏุฑ ูุฑุญูู Backward ฺฉูุฏุชุฑ ุงุฒ cuDNN ุนูู ฺฉุฑุฏู ุงุณุช. ุงู ููุถูุน ุทุจุน ุงุณุช ุฒุฑุง ูุญุงุณุจู ฺฏุฑุงุฏุงู ุฏุฑ PyTorch/cuDNN ุจุณุงุฑ ุจูููโุณุงุฒ ุดุฏู ุงุณุชุ ุฏุฑ ุญุงู ฺฉู ูพุงุฏูโุณุงุฒ Backward ุฏุณุช (Autograd ุฏุฑ ูพุงุชูู ุง ูพุงุฏูโุณุงุฒ ุณุงุฏูโุชุฑ) ุณุฑุจุงุฑ ุจุดุชุฑ ุฏุงุฑุฏ.

    ููุด cuDNN: ุบุฑูุนุงู ฺฉุฑุฏู cuDNN ุจุงุนุซ ุงูุฒุงุด ฺุดูฺฏุฑ ุฒูุงู ุงุฌุฑุง ูโุดูุฏ (ุญุฏูุฏ ฑฒ ุจุฑุงุจุฑ ฺฉูุฏุชุฑ ุฏุฑ ุญุงูุช ฑฐ ูุงู)ุ ฺฉู ุงููุช ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ุจููู ุดุฏู ุฎุท ุฑุง ูุดุงู ูโุฏูุฏ.

    ุฏูุช ูุฏู: ุฏูุช ูุฏู ุฏุฑ ุญุงูุช ุงุณุชูุงุฏู ุงุฒ Fused Kernel ุชูุฑุจุงู ูุดุงุจู ุญุงูุช ุงุณุชุงูุฏุงุฑุฏ ุงุณุช (ุงุฎุชูุงู ุจุณุงุฑ ูุงฺุฒ)ุ ฺฉู ุตุญุช ุนููฺฉุฑุฏ ฺฉุฑูู ูพุงุฏูโุณุงุฒ ุดุฏู ุฑุง ุชุงุฏ ูโฺฉูุฏ.

ุจุฎุด ต.ฑ ุฑู ุจฺฏู

ุงู ูพุงุณุฎ ูุฑุจูุท ุจู ุจุฎุด ต.ฑ (ุชุญูู ู ุจุฑุฑุณ) ุชูุฑู ุงุณุช ฺฉู ุจุฑ ุงุณุงุณ ูุชุงุฌ ุจูโุฏุณุชโุขูุฏู ุงุฒ ุงุฌุฑุง ููุชโุจูฺฉ ุฏุฑ ุจุฎุด ูุจู ุชุฏูู ุดุฏู ุงุณุช.

ต.ฑ ุชุญูู ู ุจุฑุฑุณ

ฑ. ุชุญูู ุงุฎุชูุงูโูุง ุฒูุงู ู ุฏูุช

ุจุฑ ุงุณุงุณ ูุชุงุฌ ุจูฺูุงุฑฺฉ (ุงุฌุฑุง ฑฐ ูุงู ู ฒ ูุงู)ุ ูฺฉุงุช ุฒุฑ ูุงุจู ุชูุฌู ูุณุชูุฏ:

    ุณุฑุนุช ุฏุฑ ูุฑุญูู Forward: ุฏุฑ ูุฑ ุฏู ุขุฒูุงุดุ ฺฉุฑูู ุงุฎุชุตุงุต FusedConvReLU ุฏุฑ ูุฑุญููโ Forward ุณุฑุนโุชุฑ ุงุฒ ูพุงุฏูโุณุงุฒ ุงุณุชุงูุฏุงุฑุฏ PyTorch (ุจุง cuDNN) ุนูู ฺฉุฑุฏ (ูุซูุงู ฐ.ถน ุซุงูู ุฏุฑ ุจุฑุงุจุฑ ฑ.ฐน ุซุงูู ุจุฑุง ฑฐ ูุงู).

        ุนูุช: ุงู ุงูุฒุงุด ุณุฑุนุช ูุงุด ุงุฒ ฺฉุงูุด ุฏุณุชุฑุณ ุจู ุญุงูุธู (Memory Access) ุงุณุช. ุฏุฑ ุญุงูุช ุงุณุชุงูุฏุงุฑุฏุ ุงุจุชุฏุง ูุชุฌู ฺฉุงูููููุดู ุฏุฑ ุญุงูุธู ุณุฑุงุณุฑ (Global Memory) ููุดุชู ูโุดูุฏุ ุณูพุณ ฺฉุฑูู ReLU ุขู ุฑุง ุฎูุงูุฏู ู ุฏูุจุงุฑู ูโููุณุฏ. ุงูุง ุฏุฑ ุญุงูุช Fusedุ ุนููุงุช ReLU ุจูุงูุงุตูู ูพุณ ุงุฒ ูุญุงุณุจู ฺฉุงูููููุดู ู ูุจู ุงุฒ ููุดุชู ุฏุฑ ุญุงูุธู ุงูุฌุงู ูโุดูุฏ (Kernel Fusion)ุ ฺฉู ูพููุง ุจุงูุฏ ุญุงูุธู ุฑุง ุจู ุดุฏุช ฺฉุงูุด ูโุฏูุฏ.

    ฺฉูุฏ ุฏุฑ ูุฑุญูู Backward: ฺฉุฑูู ุงุฎุชุตุงุต ุฏุฑ ูุฑุญูู Backward ุจู ูุฑุงุชุจ ฺฉูุฏุชุฑ ุงุฒ cuDNN ุนูู ฺฉุฑุฏ (ณ.ธท ุซุงูู ุฏุฑ ุจุฑุงุจุฑ ฒ.ตท ุซุงูู).

        ุนูุช: ูุญุงุณุจู ฺฏุฑุงุฏุงู ฺฉุงูููููุดู (dL/dx ู dL/dW) ุนููุงุช ุจุณุงุฑ ูพฺุฏู ุงุณุช. ฺฉุชุงุจุฎุงูู cuDNN ุงุฒ ุงูฺฏูุฑุชูโูุง ุจุณุงุฑ ุจูููโุง (ูุงููุฏ Winograd ุง FFT) ุจุฑุง ุงู ูุญุงุณุจุงุช ุงุณุชูุงุฏู ูโฺฉูุฏ ฺฉู ุจุณุงุฑ ุณุฑุนโุชุฑ ุงุฒ ูพุงุฏูโุณุงุฒ ูุณุชูู ุง ุงุณุชูุงุฏู ุงุฒ torch.nn.grad ุฏุฑ ูพุงุชูู ุงุณุช. ููฺููุ ูพุงุฏูโุณุงุฒ Backward ูุง ุงุญุชูุงูุงู ฺูุฏู ฺฉุฑูู ุฌุฏุงฺฏุงูู ูุฑุงุฎูุงู ูโฺฉูุฏ ู ุจููู ูุดุฏู ุงุณุช.

    ููุด ุญุงุช cuDNN: ุบุฑูุนุงู ฺฉุฑุฏู cuDNN ุจุงุนุซ ุดุฏ ุฒูุงู ุขููุฒุด ุฏุฑ ูุฏู ฑฐ ูุงู ุงุฒ ุญุฏูุฏ ต.ต ุซุงูู ุจู ถด ุซุงูู ุงูุฒุงุด ุงุจุฏ (ุจุด ุงุฒ ฑฐ ุจุฑุงุจุฑ ฺฉูุฏุชุฑ). ุงู ูุดุงู ูโุฏูุฏ ฺฉู ุจูููโุณุงุฒโูุง ุณุทุญ ูพุงู cuDNN (ูุงููุฏ ุงุณุชูุงุฏู ุงุฒ Shared Memory ู Tiling) ุชุง ฺู ุญุฏ ุจุฑุง ุนููุงุชโูุง ุณูฺฏู ูุซู ฺฉุงูููููุดู ุญุงุช ูุณุชูุฏ.

    ุฏูุช ูุฏู (Accuracy): ุฏูุช ููุง ูุฏู ุฏุฑ ุญุงูุช Fused ู ุงุณุชุงูุฏุงุฑุฏ ุชูุฑุจุงู ฺฉุณุงู ุจูุฏ (ุงุฎุชูุงู ุฏุฑ ุญุฏ ุฎุทุง ุงุนุดุงุฑ ูุงฺุฒ). ุงู ููุถูุน ุตุญุช ุฑุงุถ ฺฉุฑูู ูพุงุฏูโุณุงุฒ ุดุฏู ุฑุง ุชุงุฏ ูโฺฉูุฏ. (ูฺฉุชู: ุฏุฑ ูุฏู ฑฐ ูุงู ุจู ุฏูู ุนุฏู ุงุณุชูุงุฏู ุงุฒ BatchNormุ ุดุจฺฉู ููฺฏุฑุง ูุดุฏ ุงูุง ุงู ุฑูุชุงุฑ ุฏุฑ ูุฑ ุฏู ูุฏู ฺฉุณุงู ุจูุฏ).

ฒ. ฺู ุฒูุงู ุงุฒ ฺฉุฑูู ุงุฎุชุตุงุต (Custom Kernel) ุงุณุชูุงุฏู ฺฉููุ

ุงุณุชูุงุฏู ุงุฒ ฺฉุฑููโูุง ุณูุงุฑุด CUDA ุฏุฑ ุดุฑุงุท ุฒุฑ ููุทู ู ูุคุซุฑ ุงุณุช:

    ุงุฏุบุงู ุนููุงุชโูุง (Operator Fusion): ุฒูุงู ฺฉู ุฒูุฌุฑูโุง ุงุฒ ุนููุงุชโูุง ุณุจฺฉ (ูุงููุฏ ุฌูุนุ ุถุฑุจ ุจุฑุฏุงุฑุ ุง ุชูุงุจุน ูุนุงูโุณุงุฒ) ุฏุงุฑุฏ ฺฉู ฺฏููฺฏุงู ุขูโูุง ูพููุง ุจุงูุฏ ุญุงูุธู (Memory Bound) ุงุณุช. ููุดุชู ฺฉ ฺฉุฑูู ุชุฑฺฉุจ ุจุงุนุซ ูโุดูุฏ ุฏุงุฏูโูุง ููุท ฺฉโุจุงุฑ ุงุฒ ุญุงูุธู ุฎูุงูุฏู ู ููุดุชู ุดููุฏ.

    ุนููุงุชโูุง ุบุฑุณุชุงูุฏุงุฑุฏ: ุฒูุงู ฺฉู ูุงุฒ ุจู ฺฉ ูุงู ุง ุนููุงุช ุฎุงุต ุฏุงุฑุฏ ฺฉู ุฏุฑ PyTorch ุง ฺฉุชุงุจุฎุงููโูุง ุงุณุชุงูุฏุงุฑุฏ ูุฌูุฏ ูุฏุงุฑุฏ.

    ุจูููโุณุงุฒ ุจุฑุง ุงุจุนุงุฏ ุฎุงุต: ฺฏุงู ูพุงุฏูโุณุงุฒโูุง ุนููู ุจุฑุง ููู ุงุจุนุงุฏ ุชูุณูุฑูุง ุจููู ูุณุชูุฏุ ุงูุง ุงฺฏุฑ ุดูุง ุงุจุนุงุฏ ุซุงุจุช ู ูุดุฎุต ุฏุงุฑุฏุ ูโุชูุงูุฏ ฺฉุฑูู ุจููุณุฏ ฺฉู ุฏููุง ุจุฑุง ุขู ุงุจุนุงุฏ (ูุซูุงู ฺฉุฑูู ณxณ) ุจูููโุณุงุฒ ุดุฏู ุจุงุดุฏ (ูุงููุฏ ฺฉุงุฑ ฺฉู ุฏุฑ ุงู ุชูุฑู ฺฉุฑุฏู).

ณ. ฺู ูููุน ุจู ูพุงุฏูโุณุงุฒโูุง ุฏุงุฎู (PyTorch/cuDNN) ุงุนุชูุงุฏ ฺฉููุ

ุฏุฑ ููุงุฑุฏ ุฒุฑ ุจูุชุฑ ุงุณุช ุงุฒ ูพุงุฏูโุณุงุฒโูุง ุงุณุชุงูุฏุงุฑุฏ ุงุณุชูุงุฏู ฺฉูู:

    ุนููุงุชโูุง ุณูฺฏู ูุญุงุณุจุงุช (Compute Bound): ุจุฑุง ูุงูโูุง ูุซู ุถุฑุจ ูุงุชุฑุณ (Linear) ุง ฺฉุงูููููุดูโูุง ุจุฒุฑฺฏุ ฺฉุชุงุจุฎุงููโูุง ูุซู cuBLAS ู cuDNN ุณุงูโูุง ุชูุณุท ูููุฏุณุงู NVIDIA ุจููู ุดุฏูโุงูุฏ ู ุฑุณุฏู ุจู ุณุฑุนุช ุขูโูุง ุจุง ฺฉุฏููุณ ุฏุณุช ุจุณุงุฑ ุฏุดูุงุฑ ุงุณุช.

    ูพฺุฏฺฏ Backward: ููุดุชู ู ุฏุจุงฺฏ ฺฉุฑุฏู ฺฏุฑุงุฏุงู (Backward Pass) ุจุฑุง ุนููุงุชโูุง ูพฺุฏู ุณุฎุช ู ูุณุชุนุฏ ุฎุทุง ุงุณุช. Autograd ูพุงุชูุฑฺ ุงู ฺฉุงุฑ ุฑุง ุจู ุตูุฑุช ุฎูุฏฺฉุงุฑ ู ุจููู ุงูุฌุงู ูโุฏูุฏ.

    ูุงุจูุช ุญูู (Portability): ฺฉุฏูุง ุงุณุชุงูุฏุงุฑุฏ PyTorch ุฑู ุณุฎุชโุงูุฒุงุฑูุง ูุฎุชูู (CPU, GPUูุง ูุฎุชูู) ุจู ุฑุงุญุช ุงุฌุฑุง ูโุดููุฏุ ุงูุง ฺฉุฏ CUDA ูุงุฒ ุจู ูฺฏูุฏุงุฑ ู ุชุทุจู ุจุง ูุนูุงุฑโูุง ุฌุฏุฏ ุฏุงุฑุฏ.
    
    
    
    PM

ุณุงุฎุชุงุฑ ุงู ุดุจฺฉู ุฑุง ุชูุตู ฺฉู ู ุฎูุงู ุจุฏู ฺฉ ูุฏู ูููุฏ ุชุตูุฑ ุขู ุฑุง ุฑุณู ฺฉูุฏ


class DepthwiseSeparableBlock(nn.Module):

def __init__(self, in_channels, out_channels, stride=1):

super().__init__()

self.depthwise = nn.Conv2d(

in_channels,

in_channels,

kernel_size=3,

padding=1,

stride=stride,

groups=in_channels,

bias=False

)

self.bn_dw = nn.BatchNorm2d(in_channels)

self.pointwise = nn.Conv2d(

in_channels,

out_channels,

kernel_size=1,

stride=1,

bias=False

)

self.bn_pw = nn.BatchNorm2d(out_channels)

self.shortcut = nn.Sequential()

if stride != 1 or in_channels != out_channels:

self.shortcut = nn.Sequential(

nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),

nn.BatchNorm2d(out_channels)

)


def forward(self, x):

out = F.relu(self.bn_dw(self.depthwise(x)))

out = self.bn_pw(self.pointwise(out))

out += self.shortcut(x)

out = F.relu(out)

return out


class CNN(nn.Module):

def __init__(self, in_channel=3, num_class=10):

super().__init__()

self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)

self.bn1 = nn.BatchNorm2d(64)

self.pool1 = nn.MaxPool2d(2)


self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)

self.block2 = DepthwiseSeparableBlock(128, 256, stride=2)

self.block3 = DepthwiseSeparableBlock(256, 256, stride=2)


self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))


self.dropout = nn.Dropout(p=0.2)

self.fc = nn.Linear(256, num_class)


def forward(self, x):

x = F.relu(self.bn1(self.conv1(x)))

x = self.pool1(x)

x = self.block1(x)

x = self.block2(x)

x = self.block3(x)

x = self.avg_pool(x)

x = x.view(x.size(0), -1)

x = self.dropout(x)

x = self.fc(x)

return x

ุจุฑุง ุงูฺฉู ุจุชูุงูุฏ ุจูุชุฑู ุฎุฑูุฌ ุฑุง ุงุฒ ูุฏูโูุง ูููุฏ ุชุตูุฑ (ูุงููุฏ Midjourneyุ DALL-E 3 ุง Stable Diffusion) ุจฺฏุฑุฏุ ุจุงุฏ ุณุงุฎุชุงุฑ ุงู ุดุจฺฉู ุฑุง ุจู ุฒุจุงู ุจุตุฑ ู ูู ุชูุตู ฺฉูุฏ.

ุงู ุดุจฺฉู ฺฉ CNN (ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู) ุงุณุช ฺฉู ุงุฒ ุจููฺฉโูุง ุชูฺฉฺฉโูพุฐุฑ ุนูู (Depthwise Separable Blocks) ุจุง ุงุชุตุงูุงุช ุจุงูโูุงูุฏู (Residual Connections) ุงุณุชูุงุฏู ูโฺฉูุฏ. ุณุงุฎุชุงุฑ ุขู ุดุจู ุจู ุชุฑฺฉุจ ุงุฒ MobileNet ู ResNet ุงุณุช.

ุฏุฑ ุงุฏุงููุ ุณุงุฎุชุงุฑ ุฏูู ุฑุง ุชุดุฑุญ ฺฉุฑุฏู ู ุณูพุณ ูพุฑุงููพุชโูุง (ุฏุณุชูุฑุงุช ูุชู) ุขูุงุฏู ุงูฺฏูุณ ุฑุง ุจุฑุง ุชููุฏ ุชุตูุฑ ุจู ุดูุง ูโุฏูู.

ุชุญูู ุณุงุฎุชุงุฑ ุจุฑุง ุชุตูุฑุณุงุฒ

    ูุฑูุฏ (Input): ุชุตูุฑ (ูุซูุงู ณ ฺฉุงูุงูู).

    ูุงู ุงููู (Initial Stage):

        ฺฉ ูุงู ฺฉุงููููุดู ูุนููู (3ร3).

        ูุฑูุงูโุณุงุฒ ุฏุณุชูโุง (Batch Norm).

        ุชุงุจุน ูุนุงูโุณุงุฒ ReLU.

        ูุงู MaxPool (ฺฉุงูุด ุงุจุนุงุฏ ุชุตูุฑ ุจู ูุตู).

    ุจุฏูู ุงุตู (Backbone): ุดุงูู ณ ุจููฺฉ ูพุดุช ุณุฑ ูู (DepthwiseSeparableBlock):

        ุจููฺฉ ฑ: ุชุจุฏู ถด ฺฉุงูุงู ุจู ฑฒธ ฺฉุงูุงู (ุจุง ฺฉุงูุด ุณุงุฒ ุชุตูุฑ).

        ุจููฺฉ ฒ: ุชุจุฏู ฑฒธ ฺฉุงูุงู ุจู ฒตถ ฺฉุงูุงู (ุจุง ฺฉุงูุด ุณุงุฒ ุชุตูุฑ).

        ุจููฺฉ ณ: ุชุจุฏู ฒตถ ฺฉุงูุงู ุจู ฒตถ ฺฉุงูุงู (ุจุง ฺฉุงูุด ุณุงุฒ ุชุตูุฑ).

        ูฺฺฏ ุจุตุฑ ููู: ูุฑ ุจููฺฉ ุฏุงุฑุง ฺฉ ูุณุฑ ุงุตู (Main Path) ู ฺฉ ูุณุฑ ูุงูุจุฑ (Shortcut/Skip Connection) ุงุณุช ฺฉู ุจู ูู ุงุถุงูู ูโุดููุฏ.

    ุณุฑ ุดุจฺฉู (Head):

        Global Average Pooling (ุชุจุฏู ฺฉู ูฺฺฏโูุง ุจู ุจุฑุฏุงุฑ 1ร1).

        Dropout.

        ูุงู ุชูุงู ูุชุตู (Linear) ุจุฑุง ุทุจููโุจูุฏ ููุง (ฑฐ ฺฉูุงุณ).

ูพุฑุงููพุชโูุง ุงูฺฏูุณ (ุจุฑุง ูุฏูโูุง ูููุฏ ุชุตูุฑ)

ูู ุฏู ููุน ุชูุตู ุจุฑุง ุดูุง ุขูุงุฏู ฺฉุฑุฏูโุงู. ฺฉ ุจุฑุง ููุงุด ฺฉู ูุนูุงุฑ ู ุฏฺฏุฑ ุจุฑุง ููุงุด ุฌุฒุฆุงุช ฺฉ ุจููฺฉ. ุงู ูุชูโูุง ุฑุง ฺฉูพ ฺฉูุฏ ู ุจู ูุฏู ฺฏุฑุงูฺฉ ุจุฏูุฏ.

ฺฏุฒูู ฑ: ููุงุด ฺฉู ูุนูุงุฑ ุดุจฺฉู (High-Level Architecture)

ุงู ูพุฑุงููพุช ุฌุฑุงู ฺฉู ุฏุงุฏูโูุง ุงุฒ ฺูพ ุจู ุฑุงุณุช ุฑุง ูุดุงู ูโุฏูุฏ.

    Prompt: A professional technical diagram of a Convolutional Neural Network architecture. The flow is from left to right. 1. Input: An image tensor on the left. 2. Initial Layer: A standard convolution block followed by a Max Pooling layer, reducing spatial dimensions. 3. Middle Section: Three distinct sequential blocks labeled "Depthwise Separable Residual Blocks".

        Block 1 increases depth to 128 channels.

        Block 2 increases depth to 256 channels.

        Block 3 maintains 256 channels.

        Visual cues: Curved arrows representing "Residual Shortcut connections" bypassing the main layers in each block. 4. Output Head: A "Global Average Pooling" layer condensing the volume into a flat vector, followed by a Dropout layer and a final Fully Connected layer leading to 10 output class scores. Style: Clean schematic, vector style, 2D technical drawing, white background, distinct colors for different operation types (Conv, Pool, Linear).

ฺฏุฒูู ฒ: ููุงุด ุฌุฒุฆุงุช ุฏูู ุจููฺฉ (Block Detail)

ุงฺฏุฑ ูโุฎูุงูุฏ ุณุงุฎุชุงุฑ ุฏุงุฎู ฺฉูุงุณ DepthwiseSeparableBlock ุฑุง ุฑุณู ฺฉูุฏ (ฺูู ุจุฎุด ุงุตู ู ุฎุงุต ฺฉุฏ ุดูุงุณุช)ุ ุงุฒ ุงู ูพุฑุงููพุช ุงุณุชูุงุฏู ฺฉูุฏ.

    Prompt: A detailed schematic block diagram of a "Depthwise Separable Residual Block" in a neural network. Structure: The input tensor splits into two paths: Path A (Main Branch):

        Depthwise Convolution (3ร3) -> Batch Norm -> ReLU.

        Pointwise Convolution (1ร1) -> Batch Norm. Path B (Shortcut Branch): A parallel skip connection containing a 1ร1 Convolution and Batch Norm (for stride/channel adjustment). Merge: Path A and Path B merge with an "Element-wise Addition" (+) operation. Final: A ReLU activation after the addition. Style: High-tech engineering diagram, isometric view, nodes and edges clearly labeled, professional scientific illustration style.

ูฺฉุงุช ุจุตุฑ ุจุฑุง ุฏุฑฺฉ ุจูุชุฑ (ุชูุถุญุงุช ุชฺฉูู)

ุงฺฏุฑ ุฎูุฏุชุงู ูโุฎูุงูุฏ ูููุฏุงุฑ ุฑุง ุงุตูุงุญ ฺฉูุฏ ุง ุฏุณุช ุจฺฉุดุฏุ ุจู ุงู ูฺฉุงุช ุชูุฌู ฺฉูุฏ:

    ุชุบุฑ ุงุจุนุงุฏ ุชุงูุณูุฑ: ุดุจฺฉู ุดูุง ุจู ุดุฏุช ุงุจุนุงุฏ ูฺฉุงู (Height/Width) ุฑุง ฺฉุงูุด ูโุฏูุฏ ุงูุง ุนูู (Channels) ุฑุง ุฒุงุฏ ูโฺฉูุฏ.

        ุดฺฉูโูุง ุจุงุฏ ุงุฒ "ูุงุฒฺฉ ู ูพูู" (ูุฑูุฏ) ุจู "ุถุฎู ู ฺฉูฺฺฉ" (ุฎุฑูุฌ ูุจู ุงุฒ Pool) ุชุจุฏู ุดููุฏ.

    ุชูุงูุช ุจุง ResNet ูุนููู: ุฏุฑ ResNet ูุนูููุ ฺฉุงููููุดูโูุง ุงุณุชุงูุฏุงุฑุฏ ูุณุชูุฏ. ุฏุฑ ุงูุฌุงุ ฺฉุงููููุดู ุงุตู ุจู ุฏู ูุฑุญูู ุชูุณู ุดุฏู: ุงูู Depthwise (ฺฉู ููุท ุฑู ูุฑ ฺฉุงูุงู ุฌุฏุง ฺฉุงุฑ ูโฺฉูุฏ) ู ุจุนุฏ Pointwise (ฺฉู ฺฉุงูุงูโูุง ุฑุง ุชุฑฺฉุจ ูโฺฉูุฏ). ุงู ุจุงุนุซ ูโุดูุฏ ุจููฺฉ ุณุจฺฉโุชุฑ ุจุงุดุฏ.

Getty Images

    Explore

ุงฺฏุฑ ุจุฎูุงูุฏ ุณุงุฎุชุงุฑ ุฏูู ูุงูโูุง ุฑุง ุจู ุตูุฑุช ูุชู ู ูุงฺฏ (Log) ุจุจูุฏ ุชุง ุฏููโุชุฑ ูุชูุฌู ุดูุฏ ฺู ุงุชูุงู ูโุงูุชุฏุ ูโุชูุงูู ฺฉุฏ ฺฉูุชุงู ุจููุณู ฺฉู ุฎูุงุตู ูุฏู (torchinfo) ุฑุง ฺุงูพ ฺฉูุฏ. ุขุง ูุงู ูุณุชุฏุ

ุชุตูุฑ ุณุงุฎุชุงุฑ ุงู ุดุจฺฉู ุฑุง ุฑุณู ฺฉู

class DepthwiseSeparableBlock(nn.Module):

def __init__(self, in_channels, out_channels, stride=1):

super().__init__()

self.depthwise = nn.Conv2d(

in_channels,

in_channels,

kernel_size=3,

padding=1,

stride=stride,

groups=in_channels,

bias=False

)

self.bn_dw = nn.BatchNorm2d(in_channels)

self.pointwise = nn.Conv2d(

in_channels,

out_channels,

kernel_size=1,

stride=1,

bias=False

)

self.bn_pw = nn.BatchNorm2d(out_channels)

self.shortcut = nn.Sequential()

if stride != 1 or in_channels != out_channels:

self.shortcut = nn.Sequential(

nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),

nn.BatchNorm2d(out_channels)

)


def forward(self, x):

out = F.relu(self.bn_dw(self.depthwise(x)))

out = self.bn_pw(self.pointwise(out))

out += self.shortcut(x)

out = F.relu(out)

return out


class CNN(nn.Module):

def __init__(self, in_channel=3, num_class=10):

super().__init__()

self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)

self.bn1 = nn.BatchNorm2d(64)

self.pool1 = nn.MaxPool2d(2)


self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)

self.block2 = DepthwiseSeparableBlock(128, 256, stride=2)

self.block3 = DepthwiseSeparableBlock(256, 256, stride=2)


self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))


self.dropout = nn.Dropout(p=0.2)

self.fc = nn.Linear(256, num_class)


def forward(self, x):

x = F.relu(self.bn1(self.conv1(x)))

x = self.pool1(x)

x = self.block1(x)

x = self.block2(x)

x = self.block3(x)

x = self.avg_pool(x)

x = x.view(x.size(0), -1)

x = self.dropout(x)

x = self.fc(x)

return x

ุณุน ฺฉู ฺฉ ุดฺฉู ุจุณุงุฑ ุญุฑูู ุง ุจุงุณุงุฎุชุงุฑ ููุงุณุจ ุจุฑุง ููุงูู ุชููุฏ ฺฉู


PM

ุณูุงู ฒ ุจุฎุด ฑ ฺฏุฒุงุฑุด ุจููุณ

ุจุฑ ุงุณุงุณ ูุงู ุงุฑุณุงู (Q2-4.ipynb)ุ ุณูุงู ฒ ุจุฎุด ฑ ูุฑุจูุท ุจู ูพุงุฏูโุณุงุฒ ูุฏู ูพุงูุ ุชุนุฑู ูุนูุงุฑ ุดุจฺฉู (CNN ุจุง Depthwise Separable Convolutions) ู ุขููุฒุด ุขู ุจุฑ ุฑู ฺฉ GPU ูุงุญุฏ (Single GPU) ุงุณุช.

ุฏุฑ ุงุฏุงูู ฺฏุฒุงุฑุด ฺฉุงูู ุงู ุจุฎุด ุจุฑุง ุดูุง ุขูุงุฏู ุดุฏู ุงุณุช:

ฺฏุฒุงุฑุด ุณูุงู ฒ - ุจุฎุด ุงูู: ุขููุฒุด ุฑู ุชฺฉ GPU

ฑ. ุชูุตู ูุฏู ู ูุนูุงุฑ

ุฏุฑ ุงู ุจุฎุดุ ฺฉ ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู (CNN) ุจุง ุงุณุชูุงุฏู ุงุฒ ุจููฺฉโูุง Depthwise Separable Convolution ูพุงุฏูโุณุงุฒ ุดุฏู ุงุณุช. ูุฏู ุงุฒ ุงุณุชูุงุฏู ุงุฒ ุงู ููุน ฺฉุงููููุดูุ ฺฉุงูุด ุชุนุฏุงุฏ ูพุงุฑุงูุชุฑูุง ู ูุฒูู ูุญุงุณุจุงุช ูุณุจุช ุจู ฺฉุงููููุดูโูุง ุงุณุชุงูุฏุงุฑุฏ ุงุณุช.

ูุนูุงุฑ ุดุจฺฉู (CNN) ุดุงูู ุงุฌุฒุง ุฒุฑ ุงุณุช:

    ูุงู ูุฑูุฏ: ฺฉ ูุงู Conv2d ุงุณุชุงูุฏุงุฑุฏ (ณ ฺฉุงูุงู ูุฑูุฏ ุจู ถด ฺฉุงูุงู ุฎุฑูุฌ) ููุฑุงู ุจุง Batch Normalization ู MaxPool.

    ุจุฏูู ุงุตู: ุณู ุจููฺฉ ูุชูุงู DepthwiseSeparableBlock ฺฉู ุชุนุฏุงุฏ ฺฉุงูุงูโูุง ุฑุง ุจู ุชุฑุชุจ ุงุฒ ถด ุจู ฑฒธุ ุณูพุณ ุจู ฒตถ ุงูุฒุงุด ูโุฏููุฏ (ุจุง stride=2 ุจุฑุง ฺฉุงูุด ุงุจุนุงุฏ ูฺฉุงู).

    ูุงู ูพุงุงู: ุงุณุชูุงุฏู ุงุฒ AdaptiveAvgPool2d ุจุฑุง ุชููุฏ ุจุฑุฏุงุฑ ูฺฺฏ ููุงุ ุณูพุณ Dropout (ุจุง ุงุญุชูุงู ฐ.ฒ) ู ฺฉ ูุงู ุชูุงูโูุชุตู (Linear) ุจุฑุง ฺฉูุงุณโุจูุฏ ฑฐ ฺฉูุงุณ ูุฌููุนู ุฏุงุฏู STL10.

ฒ. ูพฺฉุฑุจูุฏ ุขููุฒุด (Training Configuration)

    ูุฌููุนู ุฏุงุฏู: STL10 (ุดุงูู ุชูฺฉฺฉ Train ู Test).

    ูพุดโูพุฑุฏุงุฒุด (Augmentation): ุดุงูู RandomCropุ RandomHorizontalFlipุ ColorJitter ู RandomRotation ุจุฑุง ุงูุฒุงุด ุชููุน ุฏุงุฏูโูุง ุขููุฒุด.

    ุจูููโุณุงุฒ (Optimizer): ุงูฺฏูุฑุชู AdamW ุจุง weight_decay=5e-2.

    ุฒูุงูโุจูุฏ ูุฑุฎ ุงุฏฺฏุฑ (Scheduler): ุงุณุชูุงุฏู ุงุฒ OneCycleLR ุจุง ุญุฏุงฺฉุซุฑ ูุฑุฎ ุงุฏฺฏุฑ 1e-3 ุจุฑุง ููฺฏุฑุง ุณุฑุนโุชุฑ ุฏุฑ ฑต ุงูพุงฺฉ.

    ุชุนุฏุงุฏ Epoch: ฑต

    ุณุฎุชโุงูุฒุงุฑ: ุชฺฉ GPU (cuda).

ณ. ฺฉุฏ ูพุงุฏูโุณุงุฒ ูุฏู

ฺฉุฏ ูุฑุจูุท ุจู ูุนูุงุฑ ูุฏู ฺฉู ุฏุฑ ููุชโุจูฺฉ ุงุฌุฑุง ุดุฏู ุงุณุช ุจู ุตูุฑุช ุฒุฑ ูโุจุงุดุฏ:
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        self.depthwise = nn.Conv2d(
            in_channels, 
            in_channels, 
            kernel_size=3, 
            padding=1, 
            stride=stride, 
            groups=in_channels, 
            bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        
        self.pointwise = nn.Conv2d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            stride=1, 
            bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x) 
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) 

        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) 
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) 

        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) 

        self.dropout = nn.Dropout(p=0.2) 
        self.fc = nn.Linear(256, num_class) 

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x

ด. ูุชุงุฌ ุชุฌุฑุจ

ูุฏู ุจู ูุฏุช ฑต ุงูพุงฺฉ ุขููุฒุด ุฏุฏ ู ูุชุงุฌ ููุง ุจู ุดุฑุญ ุฒุฑ ุงุณุช:

    ุฏูุช ููุง ุฑู ุฏุงุฏูโูุง ุชุณุช (Test Accuracy): ถด.ธดูช

    ูุฒุงู ุฎุทุง (Loss) ููุง: ฐ.นทฐด

    ุฒูุงู ฺฉู ุขููุฒุด: ฑฐท.ธถ ุซุงูู

    ุญุงูุธู ูุตุฑู GPU: ุญุฏูุฏ ณฑฒ ูฺฏุงุจุงุช

ุฑููุฏ ุจูุจูุฏ ุฏูุช ูุฏู ุฏุฑ ุทูู ุขููุฒุด ูุดุงู ูโุฏูุฏ ฺฉู ูุฏู ุงุฒ ุฏูุช ฒถ.ณธูช ุฏุฑ ุงูพุงฺฉ ุงูู ุจู ถด.ธดูช ุฏุฑ ุงูพุงฺฉ ูพุงูุฒุฏูู ุฑุณุฏู ุงุณุช ฺฉู ูุดุงูโุฏููุฏู ุงุฏฺฏุฑ ููููุชโุขูุฒ ูุฏู ูพุงู ูโุจุงุดุฏ.

ุขุง ูุงุฒ ุฏุงุฑุฏ ุจุฎุดโูุง ุจุนุฏ (ุขููุฒุด ููุงุฒ Multi-GPU) ุฑุง ูุฒ ุชุญูู ฺฉููุ

ูููุฏุงุฑ ุฏูุช/ุงูพุงฺฉ ุฑุง ุฑุณู ฺฉู
Python

import matplotlib.pyplot as plt

# ุฏุงุฏูโูุง ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุฒ ุฎุฑูุฌ ุณููู ุดูุงุฑู 10
epochs = list(range(1, 16))
test_accuracies = [
    26.38, 35.16, 41.05, 42.56, 46.24, 
    50.89, 55.83, 54.45, 58.01, 60.34, 
    62.71, 63.64, 64.22, 64.54, 64.84
]

# ุฑุณู ูููุฏุงุฑ
plt.figure(figsize=(10, 6))
plt.plot(epochs, test_accuracies, marker='o', linestyle='-', color='b', label='Test Accuracy')
plt.title('Test Accuracy per Epoch (Single GPU)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.grid(True)
plt.xticks(epochs)
plt.legend()

# ุฐุฎุฑู ูููุฏุงุฑ ุฏุฑ ฺฉ ูุงู
plt.savefig('accuracy_plot_q2_part1.png')
plt.close()

ูููุฏุงุฑ ุจุงูุง ุฑููุฏ ุชุบุฑุงุช ุฏูุช ูุฏู ุจุฑ ุฑู ุฏุงุฏูโูุง ุชุณุช ุฑุง ุฏุฑ ุทูู ฑต ุงูพุงฺฉ ุขููุฒุด ูุดุงู ูโุฏูุฏ. ููุงูุทูุฑ ฺฉู ูุดุงูุฏู ูโุดูุฏุ ุฏูุช ูุฏู ุจุง ุงูุฒุงุด ุชุนุฏุงุฏ ุงูพุงฺฉโูุง ุจู ุทูุฑ ฺฉู ุจูุจูุฏ ุงูุชู ู ุฏุฑ ููุงุช ุจู ุญุฏูุฏ ถด.ธดูช ุฑุณุฏู ุงุณุช.

ุฎุฑูุฌ ู ุฎูุงู
Python

import matplotlib.pyplot as plt

# ุฏุงุฏูโูุง ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุฒ ุฎุฑูุฌ ุณููู ุดูุงุฑู 10 ุฏุฑ ููุชโุจูฺฉ
epochs = list(range(1, 16))
test_accuracies = [
    26.38, 35.16, 41.05, 42.56, 46.24, 
    50.89, 55.83, 54.45, 58.01, 60.34, 
    62.71, 63.64, 64.22, 64.54, 64.84
]

# ุฑุณู ูููุฏุงุฑ
plt.figure(figsize=(10, 6))
plt.plot(epochs, test_accuracies, marker='o', linestyle='-', color='b', label='Test Accuracy')
plt.title('Test Accuracy per Epoch (Single GPU)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.grid(True)
plt.xticks(epochs)
plt.legend()
plt.show()

ูููุฏุงุฑ ุจุงูุง ุฑููุฏ ุชุบุฑุงุช ุฏูุช ูุฏู ุจุฑ ุฑู ุฏุงุฏูโูุง ุชุณุช ุฑุง ุฏุฑ ุทูู ฑต ุงูพุงฺฉ ุขููุฒุด ุฑู ฺฉ GPU ูุงุญุฏ ูุดุงู ูโุฏูุฏ. ุฏุงุฏูโูุง ูุณุชููุงู ุงุฒ ุฎุฑูุฌ ูุฑุขูุฏ ุขููุฒุด ุฏุฑ ููุชโุจูฺฉ ุงุณุชุฎุฑุงุฌ ุดุฏูโุงูุฏ.


 PM

ุณูุงู ฒ ุจุฎุด ฑ ฺฏุฒุงุฑุด ุจููุณ

ุจุฑ ุงุณุงุณ ูุงู ุงุฑุณุงู (Q2-4.ipynb)ุ ุณูุงู ฒ ุจุฎุด ฑ ูุฑุจูุท ุจู ูพุงุฏูโุณุงุฒ ูุฏู ูพุงูุ ุชุนุฑู ูุนูุงุฑ ุดุจฺฉู (CNN ุจุง Depthwise Separable Convolutions) ู ุขููุฒุด ุขู ุจุฑ ุฑู ฺฉ GPU ูุงุญุฏ (Single GPU) ุงุณุช.

ุฏุฑ ุงุฏุงูู ฺฏุฒุงุฑุด ฺฉุงูู ุงู ุจุฎุด ุจุฑุง ุดูุง ุขูุงุฏู ุดุฏู ุงุณุช:

ฺฏุฒุงุฑุด ุณูุงู ฒ - ุจุฎุด ุงูู: ุขููุฒุด ุฑู ุชฺฉ GPU

ฑ. ุชูุตู ูุฏู ู ูุนูุงุฑ

ุฏุฑ ุงู ุจุฎุดุ ฺฉ ุดุจฺฉู ุนุตุจ ฺฉุงููููุดู (CNN) ุจุง ุงุณุชูุงุฏู ุงุฒ ุจููฺฉโูุง Depthwise Separable Convolution ูพุงุฏูโุณุงุฒ ุดุฏู ุงุณุช. ูุฏู ุงุฒ ุงุณุชูุงุฏู ุงุฒ ุงู ููุน ฺฉุงููููุดูุ ฺฉุงูุด ุชุนุฏุงุฏ ูพุงุฑุงูุชุฑูุง ู ูุฒูู ูุญุงุณุจุงุช ูุณุจุช ุจู ฺฉุงููููุดูโูุง ุงุณุชุงูุฏุงุฑุฏ ุงุณุช.

ูุนูุงุฑ ุดุจฺฉู (CNN) ุดุงูู ุงุฌุฒุง ุฒุฑ ุงุณุช:

    ูุงู ูุฑูุฏ: ฺฉ ูุงู Conv2d ุงุณุชุงูุฏุงุฑุฏ (ณ ฺฉุงูุงู ูุฑูุฏ ุจู ถด ฺฉุงูุงู ุฎุฑูุฌ) ููุฑุงู ุจุง Batch Normalization ู MaxPool.

    ุจุฏูู ุงุตู: ุณู ุจููฺฉ ูุชูุงู DepthwiseSeparableBlock ฺฉู ุชุนุฏุงุฏ ฺฉุงูุงูโูุง ุฑุง ุจู ุชุฑุชุจ ุงุฒ ถด ุจู ฑฒธุ ุณูพุณ ุจู ฒตถ ุงูุฒุงุด ูโุฏููุฏ (ุจุง stride=2 ุจุฑุง ฺฉุงูุด ุงุจุนุงุฏ ูฺฉุงู).

    ูุงู ูพุงุงู: ุงุณุชูุงุฏู ุงุฒ AdaptiveAvgPool2d ุจุฑุง ุชููุฏ ุจุฑุฏุงุฑ ูฺฺฏ ููุงุ ุณูพุณ Dropout (ุจุง ุงุญุชูุงู ฐ.ฒ) ู ฺฉ ูุงู ุชูุงูโูุชุตู (Linear) ุจุฑุง ฺฉูุงุณโุจูุฏ ฑฐ ฺฉูุงุณ ูุฌููุนู ุฏุงุฏู STL10.

ฒ. ูพฺฉุฑุจูุฏ ุขููุฒุด (Training Configuration)

    ูุฌููุนู ุฏุงุฏู: STL10 (ุดุงูู ุชูฺฉฺฉ Train ู Test).

    ูพุดโูพุฑุฏุงุฒุด (Augmentation): ุดุงูู RandomCropุ RandomHorizontalFlipุ ColorJitter ู RandomRotation ุจุฑุง ุงูุฒุงุด ุชููุน ุฏุงุฏูโูุง ุขููุฒุด.

    ุจูููโุณุงุฒ (Optimizer): ุงูฺฏูุฑุชู AdamW ุจุง weight_decay=5e-2.

    ุฒูุงูโุจูุฏ ูุฑุฎ ุงุฏฺฏุฑ (Scheduler): ุงุณุชูุงุฏู ุงุฒ OneCycleLR ุจุง ุญุฏุงฺฉุซุฑ ูุฑุฎ ุงุฏฺฏุฑ 1e-3 ุจุฑุง ููฺฏุฑุง ุณุฑุนโุชุฑ ุฏุฑ ฑต ุงูพุงฺฉ.

    ุชุนุฏุงุฏ Epoch: ฑต

    ุณุฎุชโุงูุฒุงุฑ: ุชฺฉ GPU (cuda).

ณ. ฺฉุฏ ูพุงุฏูโุณุงุฒ ูุฏู

ฺฉุฏ ูุฑุจูุท ุจู ูุนูุงุฑ ูุฏู ฺฉู ุฏุฑ ููุชโุจูฺฉ ุงุฌุฑุง ุดุฏู ุงุณุช ุจู ุตูุฑุช ุฒุฑ ูโุจุงุดุฏ:
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        self.depthwise = nn.Conv2d(
            in_channels, 
            in_channels, 
            kernel_size=3, 
            padding=1, 
            stride=stride, 
            groups=in_channels, 
            bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        
        self.pointwise = nn.Conv2d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            stride=1, 
            bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x) 
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) 

        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) 
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) 

        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) 

        self.dropout = nn.Dropout(p=0.2) 
        self.fc = nn.Linear(256, num_class) 

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x

ด. ูุชุงุฌ ุชุฌุฑุจ

ูุฏู ุจู ูุฏุช ฑต ุงูพุงฺฉ ุขููุฒุด ุฏุฏ ู ูุชุงุฌ ููุง ุจู ุดุฑุญ ุฒุฑ ุงุณุช:

    ุฏูุช ููุง ุฑู ุฏุงุฏูโูุง ุชุณุช (Test Accuracy): ถด.ธดูช

    ูุฒุงู ุฎุทุง (Loss) ููุง: ฐ.นทฐด

    ุฒูุงู ฺฉู ุขููุฒุด: ฑฐท.ธถ ุซุงูู

    ุญุงูุธู ูุตุฑู GPU: ุญุฏูุฏ ณฑฒ ูฺฏุงุจุงุช

ุฑููุฏ ุจูุจูุฏ ุฏูุช ูุฏู ุฏุฑ ุทูู ุขููุฒุด ูุดุงู ูโุฏูุฏ ฺฉู ูุฏู ุงุฒ ุฏูุช ฒถ.ณธูช ุฏุฑ ุงูพุงฺฉ ุงูู ุจู ถด.ธดูช ุฏุฑ ุงูพุงฺฉ ูพุงูุฒุฏูู ุฑุณุฏู ุงุณุช ฺฉู ูุดุงูโุฏููุฏู ุงุฏฺฏุฑ ููููุชโุขูุฒ ูุฏู ูพุงู ูโุจุงุดุฏ.

ุขุง ูุงุฒ ุฏุงุฑุฏ ุจุฎุดโูุง ุจุนุฏ (ุขููุฒุด ููุงุฒ Multi-GPU) ุฑุง ูุฒ ุชุญูู ฺฉููุ

ูููุฏุงุฑ ุฏูุช/ุงูพุงฺฉ ุฑุง ุฑุณู ฺฉู
Python

import matplotlib.pyplot as plt

# ุฏุงุฏูโูุง ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุฒ ุฎุฑูุฌ ุณููู ุดูุงุฑู 10
epochs = list(range(1, 16))
test_accuracies = [
    26.38, 35.16, 41.05, 42.56, 46.24, 
    50.89, 55.83, 54.45, 58.01, 60.34, 
    62.71, 63.64, 64.22, 64.54, 64.84
]

# ุฑุณู ูููุฏุงุฑ
plt.figure(figsize=(10, 6))
plt.plot(epochs, test_accuracies, marker='o', linestyle='-', color='b', label='Test Accuracy')
plt.title('Test Accuracy per Epoch (Single GPU)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.grid(True)
plt.xticks(epochs)
plt.legend()

# ุฐุฎุฑู ูููุฏุงุฑ ุฏุฑ ฺฉ ูุงู
plt.savefig('accuracy_plot_q2_part1.png')
plt.close()

ูููุฏุงุฑ ุจุงูุง ุฑููุฏ ุชุบุฑุงุช ุฏูุช ูุฏู ุจุฑ ุฑู ุฏุงุฏูโูุง ุชุณุช ุฑุง ุฏุฑ ุทูู ฑต ุงูพุงฺฉ ุขููุฒุด ูุดุงู ูโุฏูุฏ. ููุงูุทูุฑ ฺฉู ูุดุงูุฏู ูโุดูุฏุ ุฏูุช ูุฏู ุจุง ุงูุฒุงุด ุชุนุฏุงุฏ ุงูพุงฺฉโูุง ุจู ุทูุฑ ฺฉู ุจูุจูุฏ ุงูุชู ู ุฏุฑ ููุงุช ุจู ุญุฏูุฏ ถด.ธดูช ุฑุณุฏู ุงุณุช.

ุฎุฑูุฌ ู ุฎูุงู
Python

import matplotlib.pyplot as plt

# ุฏุงุฏูโูุง ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุฒ ุฎุฑูุฌ ุณููู ุดูุงุฑู 10 ุฏุฑ ููุชโุจูฺฉ
epochs = list(range(1, 16))
test_accuracies = [
    26.38, 35.16, 41.05, 42.56, 46.24, 
    50.89, 55.83, 54.45, 58.01, 60.34, 
    62.71, 63.64, 64.22, 64.54, 64.84
]

# ุฑุณู ูููุฏุงุฑ
plt.figure(figsize=(10, 6))
plt.plot(epochs, test_accuracies, marker='o', linestyle='-', color='b', label='Test Accuracy')
plt.title('Test Accuracy per Epoch (Single GPU)')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.grid(True)
plt.xticks(epochs)
plt.legend()
plt.show()

ูููุฏุงุฑ ุจุงูุง ุฑููุฏ ุชุบุฑุงุช ุฏูุช ูุฏู ุจุฑ ุฑู ุฏุงุฏูโูุง ุชุณุช ุฑุง ุฏุฑ ุทูู ฑต ุงูพุงฺฉ ุขููุฒุด ุฑู ฺฉ GPU ูุงุญุฏ ูุดุงู ูโุฏูุฏ. ุฏุงุฏูโูุง ูุณุชููุงู ุงุฒ ุฎุฑูุฌ ูุฑุขูุฏ ุขููุฒุด ุฏุฑ ููุชโุจูฺฉ ุงุณุชุฎุฑุงุฌ ุดุฏูโุงูุฏ.

 PM

ฺฏุฒุงุฑุด ุณูุงู ฒ ุจุฎุด ณ ุฑู ุจููุณ

ุฏุฑ ุงุฏุงููุ ฺฏุฒุงุฑุด ุชุญูู ู ููุงุณูโุง ูุฑุจูุท ุจู ุจุฎุด ณ ุงุฒ ุณุคุงู ฒ (ณ.ฒ ููุงุณู ู ุชุญูู) ุจุฑ ุงุณุงุณ ูุชุงุฌ ุงุฌุฑุง ููุฌูุฏ ุฏุฑ ูุงู ููุชโุจูฺฉ (Q2-4.ipynb) ุชุฏูู ุดุฏู ุงุณุช.

ฺฏุฒุงุฑุด ุจุฎุด ณ.ฒ: ููุงุณู ู ุชุญูู ุงุฌุฑุง ุชฺฉ GPU ู ฺูุฏ GPU

ุฏุฑ ุงู ุจุฎุดุ ูุชุงุฌ ุญุงุตู ุงุฒ ุขููุฒุด ูุฏู ุฑู ฺฉ GPU (ุจุฎุด ฑ.ฒ) ู ุขููุฒุด ููุงุฒ ุฑู ุฏู GPU (ุจุฎุด ฒ.ฒ) ููุฑุฏ ููุงุณู ูุฑุงุฑ ฺฏุฑูุชูโุงูุฏ. ูุนุงุฑโูุง ููุงุณู ุดุงูู ุฏูุช ููุง (Test Accuracy)ุ ุฒูุงู ฺฉู ุขููุฒุด (Total Time) ู ุญุงูุธู ูุตุฑู GPU ุงุณุช.

ฑ. ุฌุฏูู ููุงุณู ูุชุงุฌ

ุจุฑ ุงุณุงุณ ุฎุฑูุฌโูุง ุซุจุช ุดุฏู ุฏุฑ ููุชโุจูฺฉุ ุฌุฏูู ุฒุฑ ุฎูุงุตู ุนููฺฉุฑุฏ ูุฏู ุฏุฑ ุฏู ุณูุงุฑู ุฑุง ูุดุงู ูโุฏูุฏ:

ูุนุงุฑ ุงุฑุฒุงุจ	Single GPU (Rank 0)	Multi-GPU (DDP - 2 GPUs)	ุชุบุฑุงุช
ุงูุฏุงุฒู ุฏุณุชู (Local Batch Size)	32	32 (Global: 64)	-
ุฒูุงู ฺฉู ุขููุฒุด (ุซุงูู)	107.87	67.21	ุญุฏูุฏ 38% ฺฉุงูุด ุฒูุงู
ุฏูุช ููุง ุฑู ุฏุงุฏู ุชุณุช	64.84%	61.85% (Rank 0)	ุญุฏูุฏ 3% ุงูุช ุฏูุช
ุญุงูุธู ูุตุฑู (ุจู ุงุฒุง ูุฑ GPU)	312.08 MB	313.80 MB	ุชูุฑุจุงู ุซุงุจุช

    ูฺฉุชู: ุฒูุงู ุซุจุช ุดุฏู ุจุฑุง Multi-GPU ูุฑุจูุท ุจู ุญููู ุขููุฒุด (Training Loop) ุงุณุช. ุฒูุงู ฺฉู ุงุณฺฉุฑูพุช (ุดุงูู ุณุฑุจุงุฑ ุฑุงูโุงูุฏุงุฒ ูุฑุขูุฏูุง) ุจุฑุงุจุฑ ุจุง 79.36 ุซุงูู ุจูุฏู ุงุณุช ฺฉู ููฺูุงู ุณุฑุนโุชุฑ ุงุฒ ุญุงูุช ุชฺฉ GPU ุงุณุช.

ฒ. ุชุญูู ุฒูุงู ุงุฌุฑุง (Training Time)

ุงุณุชูุงุฏู ุงุฒ DistributedDataParallel (DDP) ุจุงุนุซ ฺฉุงูุด ูุงุจู ุชูุฌู ุฒูุงู ุขููุฒุด ุดุฏู ุงุณุช.

    ูุดุงูุฏุงุช: ุฒูุงู ุขููุฒุด ุงุฒ ุญุฏูุฏ ฑฐธ ุซุงูู ุจู ถท ุซุงูู ฺฉุงูุด ุงูุชู ุงุณุช.

    ุฏูู: ุฏุฑ ุญุงูุช DDPุ ุฏุงุฏูโูุง ุจู ุฏู GPU ุชูุณู ูโุดููุฏ (Data Parallelism). ูุฑ GPU ูู ุงุฒ ุฏุงุฏูโูุง ูุฑ Epoch ุฑุง ูพุฑุฏุงุฒุด ูโฺฉูุฏุ ุจูุงุจุฑุงู ูุฑ Epoch ุณุฑุนโุชุฑ ุชูุงู ูโุดูุฏ.

    ฺุฑุง ุณุฑุนุช ุฏููุงู ฒ ุจุฑุงุจุฑ ูุดุฏุ ุจู ุฏูู ุณุฑุจุงุฑ ุงุฑุชุจุงุท (Communication Overhead) ูุงู GPUูุง ุจุฑุง ููฺฏุงูโุณุงุฒ ฺฏุฑุงุฏุงูโูุง (Gradient Synchronization) ู ููฺูู ุฒูุงู ุตุฑู ุดุฏู ุจุฑุง ุฑุงูโุงูุฏุงุฒ (Dataloader overhead)ุ ุณุฑุนุช ุฏููุงู ุฏู ุจุฑุงุจุฑ ููโุดูุฏุ ุงูุง ุจูุจูุฏ ฺุดูฺฏุฑ ุงุณุช.

ณ. ุชุญูู ุฏูุช ูุฏู (Test Accuracy)

ุฏุฑ ุญุงูุช ฺูุฏ GPUุ ุดุงูุฏ ุงูุช ุฌุฒุฆ ุฏุฑ ุฏูุช ูุฏู ูุณุชู (64.8% ุฏุฑ ุจุฑุงุจุฑ 61.8%).

    ุฏูู ุงุตู: ุงูุฒุงุด Effective Batch Size.

        ุฏุฑ ุญุงูุช ุชฺฉ GPUุ ุงูุฏุงุฒู ุฏุณุชู ุจุฑุงุจุฑ ุจุง ณฒ ุงุณุช.

        ุฏุฑ ุญุงูุช ุฏู GPUุ ูุฑ ฺฉุงุฑุช ฺฏุฑุงูฺฉ ุฏุณุชูโุง ุจุง ุงูุฏุงุฒู ณฒ ุฑุง ูพุฑุฏุงุฒุด ูโฺฉูุฏุ ฺฉู ุนููุงู ุจุงุนุซ ูโุดูุฏ ูุฏู ุฏุฑ ูุฑ ฺฏุงู ุจูโุฑูุฒุฑุณุงู (Step)ุ ุฑู ถด ุฏุงุฏู (Global Batch Size = 64) ฺฏุฑุงุฏุงู ุฑุง ูุญุงุณุจู ฺฉูุฏ.

    ุชุฃุซุฑ: ุชุบุฑ ุงูุฏุงุฒู ูุคุซุฑ ุฏุณุชู (Effective Batch Size) ุฏูุงูฺฉ ุขููุฒุด ุฑุง ุชุบุฑ ูโุฏูุฏ. ูุนูููุงู ุจุง ุงูุฒุงุด ุงูุฏุงุฒู ุฏุณุชูุ ุจุฑุง ุญูุธ ุฏูุช ูุดุงุจูุ ูุงุฒ ุจู ุชูุธู ูุฌุฏุฏ ูุฑุฎ ุงุฏฺฏุฑ (Learning Rate Scaling) ุง ุงูุฒุงุด ุชุนุฏุงุฏ Epochูุง ุงุณุช. ุฏุฑ ุงูุฌุง ฺูู ูุงูพุฑูพุงุฑุงูุชุฑูุง ุซุงุจุช ูุงูุฏูโุงูุฏุ ููฺฏุฑุง ฺฉู ูุชูุงูุช ุจูุฏู ู ููุฌุฑ ุจู ุงุฎุชูุงู ุฌุฒุฆ ุฏุฑ ุฏูุช ุดุฏู ุงุณุช.

ด. ุชุญูู ุญุงูุธู ูุตุฑู (GPU Memory Usage)

ูุฒุงู ุญุงูุธู ูุตุฑู ุฑู ูุฑ GPU ุฏุฑ ูุฑ ุฏู ุญุงูุช ุชูุฑุจุงู ุจุฑุงุจุฑ ุงุณุช (ุญุฏูุฏ ณฑฒ ุชุง ณฑด ูฺฏุงุจุงุช).

    ุฏูู: ุฏุฑ ุฑูุด DDPุ ูุฏู ุฑู ูุฑ GPU ฺฉูพ ูโุดูุฏ (Replicated). ุงุฒ ุขูุฌุง ฺฉู batch_size ูุฑูุฏ ุจู DataLoader ุจุฑุง ูุฑ ูุฑุขูุฏ (Process) ุฑู ณฒ ุชูุธู ุดุฏู ุงุณุชุ ูุฑ GPU ุฏููุง ููุงู ุญุฌู ุงุฒ ุฏุงุฏู ู ูุฏู ุฑุง ฺฉู ุฏุฑ ุญุงูุช ุชฺฉ GPU ุฏุงุดุชุ ุฏุฑ ุญุงูุธู ุฎูุฏ ูฺฏู ูโุฏุงุฑุฏ.

    ูุชุฌู: DDP ุจู ูุง ุงุฌุงุฒู ูโุฏูุฏ ุจุฏูู ุงูุฒุงุด ุญุงูุธู ูุตุฑู ุฑู ูุฑ ฺฉุงุฑุชุ ุงุฒ ุชูุงู ูพุฑุฏุงุฒุด ฺูุฏู ฺฉุงุฑุช ุจุฑุง ูพุฑุฏุงุฒุด ุณุฑุนโุชุฑ ฺฉู ุฏุชุงุณุช ุงุณุชูุงุฏู ฺฉูู.

ฺฏุงู ุจุนุฏ ุจุฑุง ุดูุง: ุขุง ูุงู ูุณุชุฏ ุฌุฏูู ู ูููุฏุงุฑูุง ูุฑุจูุท ุจู ุณูุงู ณ (ุชุฃุซุฑ ุชุบุฑ Batch Size) ุฑุง ูุฒ ุจุฑ ุงุณุงุณ ุฏุงุฏูโูุง ููุฌูุฏ ุฏุฑ ูุงู Q2-4.ipynb ุงุณุชุฎุฑุงุฌ ู ุชุญูู ฺฉููุ

ุณูุงู ณ ุฑู ุฌูุงุจ ุจุฏู

https://lh3.googleusercontent.com/gg/AIJ2gl9ky7FYD5lCTntGDR8BAZOMIsAZ8ScwGnApV8tO4zLAIeAXZo-mMtOlH2ilWeJDhcZKY11az4lvBZmTso51k7x1QG049YGZQwbr5q6nwNuFR7LRwiICxmeLJ-0sGz1aI0XwgOHIyZ3s8xxxx-CbkUEZ0-pJvNF_A-Oe1xwA_nqOjJes6fF6=s1600

ุฏุฑ ุงุฏุงููุ ฺฏุฒุงุฑุด ฺฉุงูู ู ุชุญูู ูุฑุจูุท ุจู ุณุคุงู ณ (ุจุฑุฑุณ ุชุฃุซุฑ ุชุบุฑ ุงูุฏุงุฒู Batch) ุงุฑุงุฆู ุดุฏู ุงุณุช.

ฺฏุฒุงุฑุด ุณูุงู ณ: ุจุฑุฑุณ ุชุฃุซุฑ ุชุบุฑ ุงูุฏุงุฒู Batch

ุฏุฑ ุงู ุขุฒูุงุดุ ูุฏู ุจุง ด ุงูุฏุงุฒู ุฏุณุชู (Batch Size) ูุฎุชูู ุดุงูู ฑถุ ณฒุ ถด ู ฑฒธ ุฑู ุฏู GPU ุขููุฒุด ุฏุงุฏู ุดุฏู ุงุณุช. ุณุงุฑ ูพุงุฑุงูุชุฑูุง (ุชุนุฏุงุฏ Epoch=15ุ ูุฑุฎ ุงุฏฺฏุฑ ู ...) ุซุงุจุช ูฺฏูโุฏุงุดุชู ุดุฏูโุงูุฏ. ูุชุงุฌ ุฒุฑ ุจุฑ ุงุณุงุณ ุฎุฑูุฌโูุง ููุฌูุฏ ุฏุฑ ูุงู Q2-4.ipynb ุงุณุชุฎุฑุงุฌ ุดุฏูโุงูุฏ.

ฑ. ุฌุฏูู ุฎูุงุตู ูุชุงุฌ

Batch Size (Local)	Total Batch Size (Global)	Training Time (s)	GPU Memory (MB)	Test Accuracy (%)
16	32	78.67	167.25	63.08%
32	64	80.27	313.80	61.10%
64	128	80.30	605.17	58.65%
128	256	80.59	1189.30	56.83%

ูฺฉุชู: ููุงุฏุฑ ุฒูุงู ู ุญุงูุธู ูุฑุจูุท ุจู Rank 0 ูุณุชูุฏ.

ฒ. ูููุฏุงุฑูุง ุชุญูู ุฑููุฏ

ูููุฏุงุฑูุง ุฒุฑ ุฑููุฏ ุชุบุฑุงุช ุฏูุชุ ุญุงูุธู ู ุฒูุงู ุฑุง ูุณุจุช ุจู ุงูุฒุงุด ุงูุฏุงุฒู ุฏุณุชู ูุดุงู ูโุฏููุฏ:

ณ. ุชุญูู ู ุจุฑุฑุณ ูุชุงุฌ

ุงูู) ุชุญูู ุญุงูุธู ูุตุฑู (Memory Usage): ููุงูุทูุฑ ฺฉู ุฏุฑ ูููุฏุงุฑ ูุณุท ูุดุฎุต ุงุณุชุ ูุตุฑู ุญุงูุธู GPU ุฑุงุจุทู ุฎุท ูุณุชูู ุจุง ุงูุฏุงุฒู ุฏุณุชู ุฏุงุฑุฏ. ุจุง ุฏู ุจุฑุงุจุฑ ุดุฏู Batch Sizeุ ุญุงูุธู ุงุดุบุงู ุดุฏู ูุฒ ุชูุฑุจุงู ุฏู ุจุฑุงุจุฑ ุดุฏู ุงุณุช (ุงุฒ ฑถท ูฺฏุงุจุงุช ุฏุฑ BS=16 ุจู ฑฑธน ูฺฏุงุจุงุช ุฏุฑ BS=128). ุงู ุฑูุชุงุฑ ฺฉุงููุงู ููุฑุฏ ุงูุชุธุงุฑ ุงุณุช ุฒุฑุง ุญุฌู ุชูุณูุฑูุง ูุงู (Activations) ฺฉู ุจุฑุง ูุญุงุณุจู ฺฏุฑุงุฏุงู ุฐุฎุฑู ูโุดููุฏุ ูุณุชููุงู ุจุง ุชุนุฏุงุฏ ูููููโูุง ููุฌูุฏ ุฏุฑ ุฏุณุชู ูุชูุงุณุจ ุงุณุช.

ุจ) ุชุญูู ุฏูุช ูุฏู (Test Accuracy): ุจุง ุงูุฒุงุด ุงูุฏุงุฒู ุฏุณุชูุ ุดุงูุฏ ุงูุช ุฏูุช (ุงุฒ ถณูช ุจู ตถูช) ูุณุชู.

    ุฏูู: ุงูุฒุงุด Batch Size ุจุงุนุซ ฺฉุงูุด ููุฒ ุฏุฑ ุชุฎูู ฺฏุฑุงุฏุงู ูโุดูุฏ. ุงฺฏุฑฺู ุงู ููุถูุน ุจู ูุธุฑ ุฎูุจ ูโุฑุณุฏุ ุงูุง ุฏุฑ ุงุฏฺฏุฑ ุนููุ ูุฌูุฏ ููุฏุงุฑ ููุฒ ุฏุฑ ฺฏุฑุงุฏุงู (ฺฉู ุฏุฑ Batch Sizeูุง ฺฉูฺฺฉ ุจุดุชุฑ ุงุณุช) ุจู ูุฏู ฺฉูฺฉ ูโฺฉูุฏ ุชุง ุงุฒ ููููโูุง ูุญู ุชุฒ (Sharp Minima) ูุฑุงุฑ ฺฉุฑุฏู ู ุจู ููููโูุง ูููุงุฑุชุฑ (Flat Minima) ููฺฏุฑุง ุดูุฏ ฺฉู ูุงุจูุช ุชุนูู (Generalization) ุจูุชุฑ ุฏุงุฑูุฏ.

    ููฺููุ ููุช Batch Size ุงูุฒุงุด ูโุงุจุฏุ ุชุนุฏุงุฏ ฺฉู ฺฏุงูโูุง ุจูโุฑูุฒุฑุณุงู (Iterations) ุฏุฑ ูุฑ Epoch ฺฉุงูุด ูโุงุจุฏ. ฺูู ุชุนุฏุงุฏ Epochูุง ุซุงุจุช (ฑต) ุจูุฏู ุงุณุชุ ูุฏู ุฏุฑ BS=128 ุชุนุฏุงุฏ ุฏูุนุงุช ุจุณุงุฑ ฺฉูุชุฑ ูุฒูโูุง ุฎูุฏ ุฑุง ุขูพุฏุช ฺฉุฑุฏู ุงุณุช ูุณุจุช ุจู BS=16. ุจุฑุง ุฌุจุฑุงู ุงู ููุถูุน ุฏุฑ Batch Sizeูุง ุจุฒุฑฺฏุ ูุนูููุงู ุจุงุฏ ูุฑุฎ ุงุฏฺฏุฑ (Learning Rate) ุฑุง ุงูุฒุงุด ุฏุงุฏ ุง ุชุนุฏุงุฏ Epochูุง ุฑุง ุฒุงุฏ ฺฉุฑุฏ (Linear Scaling Rule).

ุฌ) ุชุญูู ุฒูุงู ุขููุฒุด (Training Time): ูฺฉุชู ุฌุงูุจ ุฏุฑ ุงู ุขุฒูุงุด ุงู ุงุณุช ฺฉู ุฒูุงู ฺฉู ุขููุฒุด ุชูุฑุจุงู ุซุงุจุช ูุงูุฏู ุงุณุช (ุญุฏูุฏ ธฐ ุซุงูู).

    ุงูุชุธุงุฑ ุงููู: ูุนูููุงู ุงูุชุธุงุฑ ุฏุงุฑู ุจุง ุงูุฒุงุด Batch Sizeุ ุจู ุฏูู ููุงุฒโุณุงุฒ ุจูุชุฑ ุจุฑุฏุงุฑ ุฏุฑ GPUุ ุฒูุงู ูุฑ Epoch ฺฉุงูุด ุงุจุฏ.

    ูุดุงูุฏู: ุซุงุจุช ุจูุฏู ุฒูุงู ูุดุงู ูโุฏูุฏ ฺฉู ฺฏููฺฏุงู (Bottleneck) ุณุณุชู ุฏุฑ ุงูุฌุง ุงุญุชูุงูุงู ุชูุงู ูุญุงุณุจุงุช GPU ูุณุช.

        ุงู ุงุชูุงู ูโุชูุงูุฏ ูุงุด ุงุฒ ูุญุฏูุฏุช ุฏุฑ ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏูโูุง (Data Loading Bottleneck) ุจุงุดุฏ (ฺูู num_workers=1 ุชูุธู ุดุฏู ุงุณุชุ CPU ููโุชูุงูุฏ ุฏุงุฏูโูุง ุฑุง ุจุง ุณุฑุนุช ุจุดุชุฑ ุงุฒ ุณุฑุนุช ูุนู ุจู GPU ุจุฑุณุงูุฏ).

        ุง ุงูฺฉู ุดุจฺฉู ุนุตุจ ููุฑุฏ ุงุณุชูุงุฏู (CNN ุณุงุฏู) ุจู ูุฏุฑ ฺฉูฺฺฉ ุงุณุช ฺฉู ุญุช ุฏุฑ Batch Size=16 ูุฒ GPU ุธุฑูุช ุฎุงู ุฒุงุฏ ุฏุงุฑุฏ ู ุงูุฒุงุด ุขู ุจู ฑฒธ ุชุฃุซุฑ ฺุดูฺฏุฑ ุจุฑ ุงุดุจุงุน ุดุฏู ูุณุชูโูุง CUDA ู ุงูุฒุงุด Throughput ฺฉู ูุฏุงุฑุฏ.

ูุชุฌูโฺฏุฑ ฺฉู: ุจุฑุง ุงู ูุฏู ู ุชูุธูุงุช ุฎุงุตุ Batch Size=16 ุจูุชุฑู ุนููฺฉุฑุฏ ุฑุง ุฏุงุดุชู ุงุณุชุ ุฒุฑุง ุจุงูุงุชุฑู ุฏูุช ุฑุง ุจุง ฺฉูุชุฑู ูุตุฑู ุญุงูุธู ู ุฒูุงู ูุดุงุจู ุจุง ุณุงุฑ ุญุงูุงุช ุงุฑุงุฆู ุฏุงุฏู ุงุณุช. ุจุฑุง ุงุณุชูุงุฏู ูุคุซุฑ ุงุฒ Batch Sizeูุง ุจุฒุฑฺฏุชุฑุ ูุงุฒ ุจู ุชูุธู ุฏููโุชุฑ Hyperparameterูุง (ูุงููุฏ Learning Rate) ู ุจูููโุณุงุฒ Pipeline ูุฑูุฏ ุฏุงุฏูโูุง (ุงูุฒุงุด num_workers) ูุฌูุฏ ุฏุงุฑุฏ.

ุณูุงู ด ุฑู ูพุงุณุฎ ุจุฏู

ุจุฑ ุงุณุงุณ ูุชุงุฌ ููุฌูุฏ ุฏุฑ ูุงู ููุชโุจูฺฉ (Q2-4.ipynb)ุ ุชุญูู ู ูพุงุณุฎ ุณุคุงู ด (ุจุฑุฑุณ Backendูุง ุงุฑุชุจุงุท ูุฎุชูู ุฏุฑ PyTorch DDP) ุฏุฑ ุงุฏุงูู ุขูุฏู ุงุณุช.

ฺฏุฒุงุฑุด ุณุคุงู ด: ููุงุณู Backendูุง nccl ู gloo

ุฏุฑ ุงู ุขุฒูุงุดุ ูุฏู ุจุง ุฏู ููุน Backend ุงุฑุชุจุงุท (nccl ู gloo) ู ุฏู ุงูุฏุงุฒู ุฏุณุชู (Batch Size) ูุชูุงูุช (ณฒ ู ฑฒธ) ุขููุฒุด ุฏุงุฏู ุดุฏู ุงุณุช. ูุฏูุ ุจุฑุฑุณ ุชุฃุซุฑ ููุน Backend ุจุฑ ุฒูุงู ุขููุฒุด ู ุญุงูุธู ูุตุฑู GPU ุงุณุช.

ฑ. ุฌุฏูู ูุชุงุฌ ุขุฒูุงุดโูุง

ูุชุงุฌ ุงุณุชุฎุฑุงุฌ ุดุฏู ุงุฒ ุฎุฑูุฌโูุง ููุชโุจูฺฉ ุจู ุดุฑุญ ุฒุฑ ุงุณุช:
Backend	Batch Size (Local)	Total Training Time (s)	GPU Memory Usage (MB)
NCCL	32	78.58	312.9
Gloo	32	77.11	312.9
NCCL	128	75.83	โผ1189
Gloo	128	75.31	โผ1188

ูฺฉุชู: ููุงุฏุฑ ุญุงูุธู ูุฑุจูุท ุจู Rank 0 ู ุชูุฑุจุงู ูุดุงุจู ุฏุฑ Rank 1 ูุณุชูุฏ.

ฒ. ุชุญูู ู ููุงุณู

ุงูู) ุชุฃุซุฑ ุจุฑ ุญุงูุธู ูุตุฑู (GPU Memory Usage):

    ูุดุงูุฏู: ููุน Backend ุชุบุฑ ุฏุฑ ูุฒุงู ุญุงูุธู ฺฏุฒุงุฑุด ุดุฏู ุชูุณุท torch.cuda.max_memory_allocated ุงุฌุงุฏ ูฺฉุฑุฏู ุงุณุช.

    ุชุญูู: ุญุงูุธู ฺฏุฒุงุฑุด ุดุฏู ุนูุฏุชุงู ูุฑุจูุท ุจู ูุฒูโูุง ูุฏูุ ฺฏุฑุงุฏุงูโูุง ู ููุงุฏุฑ ูุงู (Activations) ุงุณุช ฺฉู ูุงุจุณุชู ุจู Batch Size ู ูุนูุงุฑ ูุฏู ูุณุชูุฏุ ูู ูพุฑูุชฺฉู ุงุฑุชุจุงุท. ุงฺฏุฑฺู ูุฑ Backend ููฺฉู ุงุณุช ุจุงูุฑูุง ุฏุงุฎู ุฎูุฏ ุฑุง ุฏุงุดุชู ุจุงุดุฏุ ุงูุง ุชุฃุซุฑ ุขู ุจุฑ "Max Memory Allocated" ุฏุฑ ุงู ุขุฒูุงุด ูุงุจู ูุดุงูุฏู ุง ฺุดูฺฏุฑ ูุจูุฏู ุงุณุช.

ุจ) ุชุฃุซุฑ ุจุฑ ุฒูุงู ุขููุฒุด (Training Time):

    ูุดุงูุฏู: ุฏุฑ ุงู ุขุฒูุงุด ุฎุงุตุ Backend ููุน gloo ุนููฺฉุฑุฏ ุจุณุงุฑ ูุฒุฏฺฉ ู ุญุช ุงูุฏฺฉ ุณุฑุนโุชุฑ (ุญุฏูุฏ ฑ ุชุง ฑ.ต ุซุงูู ุฏุฑ ฺฉู ฑต ุงูพุงฺฉ) ูุณุจุช ุจู nccl ุฏุงุดุชู ุงุณุช.

    ุชุญูู:

        ูุนูููุงู NCCL (NVIDIA Collective Communications Library) ุจุฑุง ุงุฑุชุจุงุท ุจู GPUูุง (ุจู ูฺู ุงุฒ ุทุฑู NVLink ุง PCIe P2P) ุจูููโุชุฑ ุงุณุช ู ุงุณุชุงูุฏุงุฑุฏ ุตูุนุช ุจุฑุง ุขููุฒุด ุฑู GPU ูุญุณูุจ ูโุดูุฏ.

        Gloo ฺฉ Backend ุนูููโุชุฑ ุงุณุช ฺฉู ุงุบูุจ ุจุฑุง CPU ุง ููุช NCCL ุฏุฑ ุฏุณุชุฑุณ ูุณุช ุงุณุชูุงุฏู ูโุดูุฏุ ุงูุง ุฑู GPU ูุฒ ฺฉุงุฑ ูโฺฉูุฏ.

        ุนูุช ูุชุฌู ูุดุงูุฏู ุดุฏู: ุงุฒ ุขูุฌุง ฺฉู ูุฏู ููุฑุฏ ุงุณุชูุงุฏู (CNN ุณุงุฏู ุฑู CIFAR/STL10) ูุณุจุชุงู ฺฉูฺฺฉ ุงุณุช ู ุญุฌู ุฏุงุฏูโูุง ุฌุงุจุฌุง ุดุฏู (Gradients) ุฒุงุฏ ูุณุชุ ฺฏููฺฏุงู ุงุตู (Bottleneck) ุงุญุชูุงูุงู ูพููุง ุจุงูุฏ ุงุฑุชุจุงุท ูุณุช. ุฏุฑ ุงู ููุงุณ ฺฉูฺฺฉ ู ุฑู ฺฉ ููุฏ (Single Node)ุ ุณุฑุจุงุฑ ุฑุงูโุงูุฏุงุฒ ุง ูุฏุฑุช ฺฉุฑููโูุง NCCL ููฺฉู ุงุณุช ุจุงุนุซ ุดุฏู ุจุงุดุฏ ฺฉู ุชูุงูุช ุฎุงุต ูุดุงูุฏู ูุดูุฏ ุง Gloo ุฏุฑ ูุฏุฑุช ุญุงูุธู ุงุดุชุฑุงฺฉ (Shared Memory) ุฑู ฺฉ ุฏุณุชฺฏุงู ฺฉู ฺุงุจฺฉโุชุฑ ุนูู ฺฉุฑุฏู ุจุงุดุฏ. ุงูุง ุฏุฑ ูุฏูโูุง ุจุฒุฑฺฏุชุฑ ู ุณุณุชูโูุง Multi-Nodeุ ูุทุนุงู NCCL ุจุฑุชุฑ ูุงุญุด ุฎูุงูุฏ ุฏุงุดุช.

ุฌ) ุฌูุนโุจูุฏ ููุง: ุจุฑุง ุงู ูุฏู ู ุณุฎุชโุงูุฒุงุฑ ุฎุงุตุ ุงูุชุฎุงุจ ุจู nccl ู gloo ุชุฃุซุฑ ูุนูุงุฏุงุฑ ุจุฑ ฺฉุงุฑุง ูุฏุงุดุชุ ุงูุง ูุชุงุฌ ูุดุงู ูโุฏูุฏ ฺฉู gloo ูุฒ ูโุชูุงูุฏ ุฏุฑ ููุงุณโูุง ฺฉูฺฺฉ ุฑู GPU ุนููฺฉุฑุฏ ูุงุจู ูุจูู ุงุฑุงุฆู ุฏูุฏ. ุจุง ุงู ุญุงูุ ุงุณุชูุงุฏู ุงุฒ nccl ููฺูุงู ุจู ุนููุงู "Best Practice" ุจุฑุง ุขููุฒุด ุฑู GPUูุง ุงููุฏุง ุชูุตู ูโุดูุฏ.

ุงุนุฏุงุฏ ุฌุฏูู ุฑู ูุนููู ุจููุณ ุจุฏูู ุงุณุชูุงุฏู ุงุฒ ุชฺฉ

ุจููุ ุญุชูุงู. ุฏุฑ ุงูุฌุง ูพุงุณุฎ ุณุคุงู ด ุฑุง ูุฌุฏุฏุงู ูุฑุงุฑ ูโุฏููุ ุจุง ุงู ุชูุงูุช ฺฉู ุงุนุฏุงุฏ ุฌุฏูู ุจู ุตูุฑุช ูุชู ูุนููู (ุจุฏูู ูุฑูุช ุฑุงุถ/LaTeX) ููุดุชู ุดุฏูโุงูุฏ ุชุง ุฎูุงูุง ุขู ุจุฑุง ุดูุง ุฑุงุญุชโุชุฑ ุจุงุดุฏ.

ฺฏุฒุงุฑุด ุณุคุงู ด: ููุงุณู Backendูุง nccl ู gloo

ุฏุฑ ุงู ุขุฒูุงุดุ ุทุจู ุฏุณุชูุฑ ฺฉุงุฑ ุ ูุฏู ุจุง ุฏู ููุน Backend ุงุฑุชุจุงุท (nccl ู gloo) ู ุฏู ุงูุฏุงุฒู ุฏุณุชู (Batch Size) ูุชูุงูุช (ณฒ ู ฑฒธ) ุขููุฒุด ุฏุงุฏู ุดุฏู ุงุณุช ุชุง ุชุฃุซุฑ ููุน Backend ุจุฑ ุฒูุงู ุขููุฒุด ู ุญุงูุธู ูุตุฑู GPU ุจุฑุฑุณ ุดูุฏ.

ฑ. ุฌุฏูู ูุชุงุฌ ุขุฒูุงุดโูุง

ูุชุงุฌ ุฒุฑ ุนูุงู ุงุฒ ุฎุฑูุฌโูุง ูุงู ููุชโุจูฺฉ (Q2-4.ipynb) ุงุณุชุฎุฑุงุฌ ุดุฏูโุงูุฏ:
Backend	Batch Size (Local)	Total Training Time (ุซุงูู)	GPU Memory Usage (ูฺฏุงุจุงุช)
NCCL	32	78.58	312.9
Gloo	32	77.11	312.9
NCCL	128	75.83	1189
Gloo	128	75.31	1188

ฒ. ุชุญูู ู ููุงุณู

ุงูู) ุชุฃุซุฑ ุจุฑ ุญุงูุธู ูุตุฑู (GPU Memory Usage):

    ูุดุงูุฏู: ููุน Backend ุชุบุฑ ุฏุฑ ูุฒุงู ุญุงูุธู ฺฏุฒุงุฑุด ุดุฏู ุงุฌุงุฏ ูฺฉุฑุฏู ุงุณุช.

    ุชุญูู: ููุงูโุทูุฑ ฺฉู ุฏุฑ ุฌุฏูู ุฏุฏู ูโุดูุฏุ ุฏุฑ Batch Size ฺฉุณุงู (ูุซูุงู ณฒ)ุ ุญุงูุธู ูุตุฑู ุจุฑุง ูุฑ ุฏู ุญุงูุช ุฏููุงู ุญุฏูุฏ ณฑฒ.น ูฺฏุงุจุงุช ุงุณุช. ุงู ูุดุงู ูโุฏูุฏ ฺฉู ูพุฑูุชฺฉู ุงุฑุชุจุงุท (Communication Backend) ุณุฑุจุงุฑ ุญุงูุธู ูุงุจูโุชูุฌู ฺฉู ุฏุฑ max_memory_allocated ุซุจุช ุดูุฏุ ุจู ูุฏู ุชุญูู ููโฺฉูุฏ ู ุญุงูุธู ุนูุฏุชุงู ุตุฑู ูุฒูโูุง ูุฏู ู ฺฏุฑุงุฏุงูโูุง ูโุดูุฏ.

ุจ) ุชุฃุซุฑ ุจุฑ ุฒูุงู ุขููุฒุด (Training Time):

    ูุดุงูุฏู: ุฏุฑ ุงู ุขุฒูุงุด ุฎุงุตุ Backend ููุน gloo ุนููฺฉุฑุฏ ุจุณุงุฑ ูุฒุฏฺฉ ู ุญุช ุงูุฏฺฉ ุณุฑุนโุชุฑ (ุญุฏูุฏ ฑ.ต ุซุงูู ุณุฑุนโุชุฑ ุฏุฑ ฺฉู ฑต ุงูพุงฺฉ) ูุณุจุช ุจู nccl ุฏุงุดุชู ุงุณุช.

    ุชุญูู: ุงฺฏุฑฺู nccl ูุนูููุงู ุงุณุชุงูุฏุงุฑุฏ ุตูุนุช ู ุจูููโุชุฑู ฺฏุฒูู ุจุฑุง GPUูุง ุงููุฏุง ุงุณุชุ ุงูุง ุฏุฑ ููุงุณโูุง ฺฉูฺฺฉ (ูุงููุฏ ุงู ุชูุฑู ฺฉู ุฑู ฺฉ ููุฏ ู ุจุง ูุฏู ูุณุจุชุงู ุณุจฺฉ ุงูุฌุงู ูโุดูุฏ)ุ ุชูุงูุช ฺฉุงุฑุง ุขู ุจุง gloo ูุญุณูุณ ูุณุช. ุญุช ููฺฉู ุงุณุช ุฏุฑ ุงู ุญุฌู ูพุงู ุฏุงุฏูุ ุณุฑุจุงุฑ ุงููู ุฑุงูโุงูุฏุงุฒ gloo ฺฉูุชุฑ ุจูุฏู ู ุจุงุนุซ ุดุฏู ุฒูุงู ููุง ุงูุฏฺฉ ฺฉูุชุฑ ุดูุฏ.

ุฌ) ุฌูุนโุจูุฏ ููุง: ุจุฑุง ุงู ูพฺฉุฑุจูุฏ ุฎุงุต ุณุฎุชโุงูุฒุงุฑ ู ูุฏู ููุฑุฏ ูุธุฑุ ุชุบุฑ Backend ุชุฃุซุฑ ฺุดูฺฏุฑ ุจุฑ ฺฉุงุฑุง ูุฏุงุดุช. ุจุง ุงู ุญุงูุ ุฏุฑ ูพุฑูฺูโูุง ุจุฒุฑฺฏุชุฑ ู ุจูโูฺู ุฏุฑ ุณุณุชูโูุง ฺูุฏ ููุฏ (Multi-Node)ุ ุงุณุชูุงุฏู ุงุฒ nccl ููฺูุงู ุชูุตู ูโุดูุฏ.

ูููุฏุงุฑ ูู ุจฺฉุด ุจุฑุงุด

https://lh3.googleusercontent.com/gg/AIJ2gl9Cfyl_2acj8SJWYCdpiFhb3XUrJNPoo2RVFaEivZvcm8CFp7RYaM0wT9Q9TKmejbU4WfVoOfJMuJs0xud-xMZxzI1ONl1PCPiorjTXPzC3FLL3EcXKHxXerOFVNGyqrsPe3x-eA_P27nOnxi0m8j38pj2SQrbC3i_FNJI53NWetxYFmm_0=s1600

ุฏุฑ ุงูุฌุง ูููุฏุงุฑูุง ููุงุณูโุง ูุฑุจูุท ุจู ุณุคุงู ด ุชุฑุณู ุดุฏูโุงูุฏ.

ุงู ูููุฏุงุฑูุง ุฏู ูุนุงุฑ ุฒูุงู ุขููุฒุด (Training Time) ู ุญุงูุธู ูุตุฑู (Memory Usage) ุฑุง ุจุฑุง ุฏู Backend ูุฎุชูู (nccl ู gloo) ู ุฏุฑ ุฏู ุงูุฏุงุฒู ุฏุณุชู (ณฒ ู ฑฒธ) ูุดุงู ูโุฏููุฏ.

ุชูุถุญ ูููุฏุงุฑูุง:

    ูููุฏุงุฑ ุณูุช ฺูพ (ุฒูุงู ุขููุฒุด): ููุงูโุทูุฑ ฺฉู ูุดุงูุฏู ูโฺฉูุฏุ ุงุฎุชูุงู ุฒูุงู ุจู nccl ู gloo ุจุณุงุฑ ูุงฺุฒ ุงุณุช (ฺฉูุชุฑ ุงุฒ ฒ ุซุงูู). ุจุง ุงู ุญุงูุ ุฏุฑ ุงู ุขุฒูุงุด ุฎุงุตุ gloo (ุฑูฺฏ ูุงุฑูุฌ) ุนููฺฉุฑุฏ ุงูุฏฺฉ ุณุฑุนโุชุฑ ุฏุงุดุชู ุงุณุช.

    ูููุฏุงุฑ ุณูุช ุฑุงุณุช (ุญุงูุธู ูุตุฑู): ูุฒุงู ุญุงูุธู ูุตุฑู ุชูุฑุจุงู ูุณุชูู ุงุฒ ููุน Backend ุจูุฏู ู ุตุฑูุงู ุจุง ุงูุฒุงุด Batch Size (ุงุฒ ณฒ ุจู ฑฒธ) ุจู ุตูุฑุช ุฎุท ุงูุฒุงุด ุงูุชู ุงุณุช.

ุงู ูููุฏุงุฑูุง ุชุฃุฏ ูโฺฉููุฏ ฺฉู ุฏุฑ ุงู ููุงุณุ ุงูุชุฎุงุจ Backend ุชุฃุซุฑ ุจุญุฑุงู ุจุฑ ููุงุจุน ุง ุณุฑุนุช ูุฏุงุฑุฏ.

PM

ูพุงุณุฎ ุจุฏู

ุงููุง ุณูุงูุงุช ุจุณุงุฑ ุฎูุจ ูุณุชูุฏ ฺฉู ุจู ุฏุฑฺฉ ูุณุชูโ ูพุฑุฏุงุฒุด ูุฏุฑู ุฏุฑ ุงุฏฺฏุฑ ุนูู ฺฉูฺฉ ูโฺฉููุฏ.

ุฏุฑ ุงุฏุงููุ ูพุงุณุฎ ูุฑ ฺฉ ุงุฒ ุณูุงูุงุช ุดูุง ุจู ุชูฺฉฺฉ ุขูุฏู ุงุณุช:

ฑ. cuDNN ฺุณุชุ

cuDNN (ูุฎูู NVIDIA CUDA Deep Neural Network library) ฺฉ ฺฉุชุงุจุฎุงููโ ูุฑูโุงูุฒุงุฑ ุงุณุช ฺฉู ุชูุณุท ุงููุฏุง (NVIDIA) ุชูุณุนู ุฏุงุฏู ุดุฏู ุงุณุช. ุงู ฺฉุชุงุจุฎุงูู ูุฌููุนูโุง ุงุฒ ุฑูุชูโูุง (routines) ุจูููโุณุงุฒ ุดุฏู ุจุฑุง ุนููุงุช ฺฉูุฏ ุฏุฑ ุดุจฺฉูโูุง ุนุตุจ ุนูู ุฑุง ูุฑุงูู ูโฺฉูุฏ.

ุจู ุฒุจุงู ุณุงุฏูโุชุฑุ cuDNN ฺฉ ุฌุนุจู ุงุจุฒุงุฑ ุชุฎุตุต ุจุฑุง ูพุฑุฏุงุฒูุฏูโูุง ฺฏุฑุงูฺฉ (GPU) ุงููุฏุง ุงุณุช ฺฉู ุนููุงุช ุจุณุงุฑ ุฑุงุฌ ุฏุฑ ุงุฏฺฏุฑ ุนูู ูุงููุฏ ููุงุฑุฏ ุฒุฑ ุฑุง ุจุง ุจุงูุงุชุฑู ุณุฑุนุช ููฺฉู ุงุฌุฑุง ูโฺฉูุฏ:

    ฺฉุงููููุดู (Convolution): ูุณุชูโ ุงุตู ุฏุฑ ุดุจฺฉูโูุง CNN.

    Pooling: ุนููุงุช ฺฉุงูุด ุงุจุนุงุฏ.

    Normalization: ูุงููุฏ Batch Normalization.

    ุชูุงุจุน ูุนุงูโุณุงุฒ (Activation Functions): ูุงููุฏ ReLU, sigmoid, tanh.

ุงู ฺฉ ูุฑููุฑฺฉ ฺฉุงูู ุงุฏฺฏุฑ ุนูู (ูุงููุฏ PyTorch) ูุณุชุ ุจูฺฉู ฺฉ ฺฉุชุงุจุฎุงููโ ุณุทุญ ูพุงู ุงุณุช ฺฉู ูุฑููุฑฺฉโูุง ุงุฒ ุขู ุงุณุชูุงุฏู ูโฺฉููุฏ.

ฒ. ฺุฑุง PyTorch ู ุณุงุฑ ูุฑููุฑฺฉโูุง ุงุฒ ุขู ุงุณุชูุงุฏู ูโฺฉููุฏุ (ูุฒุงุง ู ูุดฺฉูุงุช ฺฉู ุญู ูโฺฉูุฏ)

ุงู ุฏู ุณูุงู ุจุณุงุฑ ุจู ูู ูุฑุชุจุท ูุณุชูุฏ. ูุฑููุฑฺฉโูุง ูุงููุฏ PyTorchุ TensorFlowุ ู Keras ุจู ฺฉ ุฏูู ุงุตู ุงุฒ cuDNN ุงุณุชูุงุฏู ูโฺฉููุฏ: ุณุฑุนุช ู ฺฉุงุฑุง.

ูุดฺฉู ฺฉู cuDNN ุญู ูโฺฉูุฏ:

ุนููุงุช ุงุฏฺฏุฑ ุนูู (ุจูโูฺู ฺฉุงููููุดู) ุจู ุดุฏุช ูุญุงุณุจุงุช ูุณุชูุฏ. ูพุงุฏูโุณุงุฒ ุงู ุนููุงุช ุจู ุตูุฑุช ุจููู ุจุฑ ุฑู GPUูุง ูููโุงูุนุงุฏู ูพฺุฏู ุงุณุช. ุงฺฏุฑ ูุฑ ุชู ุชูุณุนูโุฏููุฏูโ ูุฑููุฑฺฉ (ูุซู ุชู PyTorch) ูโุฎูุงุณุช ุงู ฺฉุฏูุง ุณุทุญ ูพุงู CUDA ุฑุง ุฎูุฏุด ุจููุณุฏ ู ุจุฑุง ูุฑ ูุณู ุฌุฏุฏ GPU ุจูููโุณุงุฒ ฺฉูุฏุ ุจู ูุฒุงุฑุงู ุณุงุนุช ฺฉุงุฑ ูููุฏุณ ูุงุฒ ุฏุงุดุช.

ูุฒุงุง ุงุณุชูุงุฏู ุงุฒ cuDNN:

    ุนููฺฉุฑุฏ ูููโุงูุนุงุฏู (Extreme Performance):

        ูููุฏุณุงู ุงููุฏุง ุฏุฑ cuDNN ุงุฒ ุจูููโุณุงุฒโูุง ุณุทุญ ูพุงู ู ุชุฑููุฏูุง ูุนูุงุฑ ุฎุงุต GPUูุง ุฎูุฏ (ูุงููุฏ ุงุณุชูุงุฏู ุงุฒ Tensor Cores) ุงุณุชูุงุฏู ูโฺฉููุฏ.

        ุงู ฺฉุชุงุจุฎุงูู ุงุบูุจ ฺูุฏู ุงูฺฏูุฑุชู ูุฎุชูู ุจุฑุง ฺฉ ุนููุงุช (ูุซูุงู ุจุฑุง ฺฉุงููููุดูุ ุงูฺฏูุฑุชูโูุง Winograd, FFT ุง GEMM) ุฏุงุฑุฏ ู ุจู ุตูุฑุช ุฎูุฏฺฉุงุฑ ุณุฑุนโุชุฑู ุงูฺฏูุฑุชู ุฑุง ุจุฑ ุงุณุงุณ ุงูุฏุงุฒูโ ูุฑูุฏ ู ููุน GPU ุงูุชุฎุงุจ ูโฺฉูุฏ.

    ุตุฑููโุฌู ุฏุฑ ุฒูุงู ุชูุณุนู (Development Time):

        ุชูุณุนูโุฏููุฏฺฏุงู PyTorch ูุงุฒ ุจู ููุดุชู ู ูฺฏูุฏุงุฑ ฺฉุฏูุง ูพฺุฏูโ CUDA ุจุฑุง ุนููุงุช ูพุงู ูุฏุงุฑูุฏ. ุขูโูุง ุจู ุณุงุฏฺฏ ุชูุงุจุน cuDNN ุฑุง ูุฑุงุฎูุงู ูโฺฉููุฏ.

    ุชูุฑฺฉุฒ ุจุฑ ุณุทุญ ุจุงูุง (High-Level Focus):

        ุงู ฺฉุงุฑ ุจู ุชู PyTorch ุงุฌุงุฒู ูโุฏูุฏ ุชุง ุจุฑ ุฑู ุจูุจูุฏ APIุ ฺฏุฑุงูโูุง ูุญุงุณุจุงุชุ ู ูุงุจูุชโูุง ุณุทุญ ุจุงูุง ูุฑููุฑฺฉ ุชูุฑฺฉุฒ ฺฉููุฏุ ูู ุจูููโุณุงุฒ ุถุฑุจ ูุงุชุฑุณโูุง.

    ูพุดุชุจุงู ุงุฒ ุณุฎุชโุงูุฒุงุฑ ุฌุฏุฏ (Future-Proof):

        ููุช ุงููุฏุง ฺฉ GPU ุฌุฏุฏ (ูุซูุงู ุณุฑ RTX 5000) ููุชุดุฑ ูโฺฉูุฏุ ฺฉุชุงุจุฎุงููโ cuDNN ุฑุง ุขูพุฏุช ูโฺฉูุฏ ุชุง ุงุฒ ุชูุงู ูุงุจูุชโูุง ุฌุฏุฏ ุขู ุณุฎุชโุงูุฒุงุฑ ุงุณุชูุงุฏู ฺฉูุฏ. PyTorch ุจุง ุจูโุฑูุฒุฑุณุงู ุจู cuDNN ุฌุฏุฏุ ุจู ุทูุฑ ุฎูุฏฺฉุงุฑ ุงุฒ ุงู ุจูุจูุฏูุง ุณุฎุชโุงูุฒุงุฑ ุจูุฑูโููุฏ ูโุดูุฏ.

ุจู ุทูุฑ ุฎูุงุตูุ PyTorch ู ุณุงุฑ ูุฑููุฑฺฉโูุง "ฺฉุงุฑูุง ุณูฺฏู" ูุญุงุณุจุงุช ุฑุง ุจู cuDNN ูโุณูพุงุฑูุฏ ุชุง ูุทูุฆู ุดููุฏ ฺฉู ุขููุฒุด ู ุงุณุชูุชุงุฌ ูุฏูโูุง ุจุง ุญุฏุงฺฉุซุฑ ุณุฑุนุช ููฺฉู ุฑู GPUูุง ุงููุฏุง ุงุฌุฑุง ูโุดูุฏ.

ณ. ูุณุจุช cuBLAS ู cuDNN ฺุทูุฑ ุงุณุชุ

ุงู ฺฉ ุชูุงุฒ ุจุณุงุฑ ููู ุงุณุช. ูุฑ ุฏู ฺฉุชุงุจุฎุงููโูุง ุงุฒ ุงููุฏุง ุจุฑุง CUDA ูุณุชูุฏุ ุงูุง ุญูุฒูโ ฺฉุงุฑ ูุชูุงูุช ุฏุงุฑูุฏ.

    cuBLAS (CUDA Basic Linear Algebra Subprograms):

        ฺฉุงุฑุจุฑุฏ: ฺฉุชุงุจุฎุงููโุง ุจุฑุง ุฌุจุฑ ุฎุท ุนููู ุงุณุช.

        ุนููุงุช: ฺฉุงุฑูุง ุงุณุงุณ ูุงููุฏ ุถุฑุจ ูุงุชุฑุณ ุฏุฑ ูุงุชุฑุณ (GEMM)ุ ุถุฑุจ ูุงุชุฑุณ ุฏุฑ ุจุฑุฏุงุฑุ ุฌูุน ุจุฑุฏุงุฑูุง ู...

        ุงููุช: ุถุฑุจ ูุงุชุฑุณุ ุณูฺฏูโุชุฑู ุจุฎุด ูุญุงุณุจุงุช ุฏุฑ ุจุณุงุฑ ุงุฒ ุนููุงุช ุดุจฺฉูโูุง ุนุตุจ (ูุงููุฏ ูุงูโูุง Fully Connected) ุงุณุช.

    cuDNN (CUDA Deep Neural Network):

        ฺฉุงุฑุจุฑุฏ: ฺฉุชุงุจุฎุงููโุง ุชุฎุตุต ุจุฑุง ุนููุงุช ุดุจฺฉูโูุง ุนุตุจ ุนูู.

        ุนููุงุช: ฺฉุงุฑูุง ุณุทุญ ุจุงูุงุชุฑ ูุงููุฏ ฺฉุงููููุดูุ Poolingุ Batch Normalization ู...

ุงุฑุชุจุงุท ุงู ุฏู:

ุขูโูุง ูฺฉูู ฺฉุฏฺฏุฑ ูุณุชูุฏุ ูู ุฑูุจ.

ุฑุงุจุทูโ ฺฉูุฏ ุงูุฌุงุณุช: ุจุณุงุฑ ุงุฒ ุนููุงุช ุดุจฺฉูโูุง ุนุตุจ (ูุงููุฏ ฺฉุงููููุดู ุง ูุงูโูุง ุฎุท) ุฑุง ูโุชูุงู ุงุฒ ูุธุฑ ุฑุงุถ ุจู ฺฉ ุง ฺูุฏ ุนููุงุช ุถุฑุจ ูุงุชุฑุณ ุจุฒุฑฺฏ ุชุจุฏู ฺฉุฑุฏ.

ุจูุงุจุฑุงูุ cuDNN ุงุบูุจ ุฏุฑ ูพุดุช ุตุญููุ ุชูุงุจุน cuBLAS ุฑุง ูุฑุงุฎูุงู ูโฺฉูุฏ.

    ูุซุงู: ฑ. ุดูุง ุฏุฑ PyTorch ฺฉ ูุงูโ ฺฉุงููููุดู (nn.Conv2d) ุฑุง ูุฑุงุฎูุงู ูโฺฉูุฏ. ฒ. PyTorch ุงู ุฏุฑุฎูุงุณุช ุฑุง ุจู cuDNN ูโุฏูุฏ ุชุง ุนููุงุช ฺฉุงููููุดู ุฑุง ุงูุฌุงู ุฏูุฏ. ณ. cuDNN ุชุดุฎุต ูโุฏูุฏ ฺฉู ุณุฑุนโุชุฑู ุฑุงู ุจุฑุง ุงู ฺฉุงููููุดู ุฎุงุตุ ุชุจุฏู ุขู ุจู ฺฉ ุถุฑุจ ูุงุชุฑุณ (ุจุง ุชฺฉูฺฉ ุจู ูุงู im2col) ุงุณุช. ด. ุณูพุณ cuDNN ฺฉุชุงุจุฎุงููโ cuBLAS ุฑุง ูุฑุงุฎูุงู ูโฺฉูุฏ ุชุง ุขู ุถุฑุจ ูุงุชุฑุณ ุบููโูพฺฉุฑ ุฑุง ุจุง ุญุฏุงฺฉุซุฑ ุณุฑุนุช ุฑู GPU ุงูุฌุงู ุฏูุฏ.

ุฎูุงุตู ุชูุงูุช:
ูฺฺฏ	cuBLAS	cuDNN
ุฏุงููู	ุฌุจุฑ ุฎุท ุนููู	ุดุจฺฉูโูุง ุนุตุจ ุนูู
ูุซุงู ุนููุงุช	ุถุฑุจ ูุงุชุฑุณ (GEMM)	ฺฉุงููููุดู (Convolution)
ุณุทุญ ุงูุชุฒุงุน	ูพุงูโุชุฑ (ูพุงูโุง)	ุจุงูุงุชุฑ (ุชุฎุตุต)
ุฑุงุจุทู	cuDNN ุจุฑุง ุงูุฌุงู ุจุฑุฎ ฺฉุงุฑูุง ุฎูุฏุ cuBLAS ุฑุง ูุฑุงุฎูุงู ูโฺฉูุฏ.	

ุขุง ูุงูุฏ ุฏุฑ ููุฑุฏ ููุด "Tensor Cores" ุฏุฑ GPUูุง ุงููุฏุง ู ุงูฺฉู ฺฺฏููู ุงู ฺฉุชุงุจุฎุงููโูุง ุงุฒ ุขูโูุง ุงุณุชูุงุฏู ูโฺฉููุฏุ ุจุดุชุฑ ุจุฏุงูุฏุ

ุจุฑุง ุจุฎุด ุขุฎุฑ ููุจุน ุงุฑุงุฆู ุจุฏู

ุญุชูุงู. ููุงุจุน ฺฉู ุงุฏุนุง ุจุฎุด ุขุฎุฑ ุฑุง ุชุงุฏ ูโฺฉููุฏ (ุนู cuDNN ุจุฑุง ุงูุฌุงู ุจุฑุฎ ุนููุงุช ุฎูุฏุ ุจู ุฎุตูุต ฺฉุงููููุดูุ ุขูโูุง ุฑุง ุจู ุถุฑุจ ูุงุชุฑุณ ุง GEMM ุชุจุฏู ูโฺฉูุฏ ู ุงุฒ cuBLAS ุง ุฑูุชูโูุง ูุดุงุจู ุขู ุงุณุชูุงุฏู ูโฺฉูุฏ) ุฏุฑ ุงุฏุงูู ุขูุฏู ุงุณุช:

ฑ. ูุณุชูุฏุงุช ุฑุณู ุงููุฏุง (NVIDIA Docs)

    ููุจุน: ูุณุชูุฏุงุช ฺฉุชุงุจุฎุงูู cuBLAS ุงููุฏุง

    ูฺฉุชู ฺฉูุฏ: ุงู ูุณุชูุฏุงุช cuBLAS ุฑุง ุจู ุนููุงู ฺฉุชุงุจุฎุงููโุง ุจุฑุง "ุฒุฑุจุฑูุงููโูุง ุฌุจุฑ ุฎุท ูพุงู" (Basic Linear Algebra Subprograms) ุชุนุฑู ูโฺฉูุฏ ฺฉู ุชูุฑฺฉุฒ ุขู ุจุฑ ุนููุงุช GEMM (GEneral Matrix-matrix Multiply) ุงุณุช. ุงู ููุจุน ุชุงุฏ ูโฺฉูุฏ ฺฉู cuBLAS ูุณุฆูู ุงุตู ุถุฑุจ ูุงุชุฑุณ ุฏุฑ ุงฺฉูุณุณุชู CUDA ุงุณุช.

    ููุจุน: ูุณุชูุฏุงุช ฺฉุชุงุจุฎุงูู cuDNN ุงููุฏุง

    ูฺฉุชู ฺฉูุฏ: ุฏุฑ ุจุฎุด ูุฑุจูุท ุจู ูพุงุฏูโุณุงุฒ ฺฉุงููููุดู (Convolution)ุ ูุณุชูุฏุงุช cuDNN ุจู ุงูฺฏูุฑุชูโูุง ูุฎุชูู ุงุดุงุฑู ูโฺฉูุฏ ฺฉู ุจุฑุง ุงูุฌุงู ุงู ฺฉุงุฑ ุงุณุชูุงุฏู ูโุดููุฏ. ฺฉ ุงุฒ ุงู ุงูฺฏูุฑุชูโูุง ุงุตู CUDNN_CONVOLUTION_FWD_ALGO_GEMM (ู ูุณุฎูโูุง ุถูู ุง implicit ุขู) ุงุณุช. ุงู ุจู ุตุฑุงุญุช ูุดุงู ูโุฏูุฏ ฺฉู cuDNN ฺฉ ุนููุงุช ฺฉุงููููุดู ุฑุง ุจู ฺฉ ุนููุงุช ุถุฑุจ ูุงุชุฑุณ ุจุฒุฑฺฏ ุชุจุฏู ูโฺฉูุฏ. cuDNN ุง ุฎูุฏุด ุงู ุถุฑุจ ูุงุชุฑุณ ุฑุง ุงูุฌุงู ูโุฏูุฏ ู ุง (ุจู ุฎุตูุต ุฏุฑ ูุงูโูุง Fully-Connected ฺฉู ุฐุงุชุงู ุถุฑุจ ูุงุชุฑุณ ูุณุชูุฏ) ุชูุงุจุน cuBLAS ุฑุง ูุฑุงุฎูุงู ูโฺฉูุฏ.

ฒ. ููุงูุงุช ูู ู ูุจูุงฺฏโูุง ุชุฎุตุต

    ููุจุน: ููุงูู "Convolutions with cuDNN" - ููุดุชู Peter Goldsborough

    ูฺฉุชู ฺฉูุฏ: ุงู ููุงูู ูู ุจู ูุถูุญ ุชูุถุญ ูโุฏูุฏ ฺฉู cuDNN ฺูุฏู ุงูฺฏูุฑุชู ุจุฑุง ฺฉุงููููุดู ุงุฑุงุฆู ูโุฏูุฏ ู ฺฉ ุงุฒ ุขูโูุงุ CUDNN_CONVOLUTION_FWD_ALGO_GEMMุ "ฺฉุงููููุดู ุฑุง ุจู ุนููุงู ฺฉ ุถุฑุจ ูุงุชุฑุณ ุตุฑุญ ูุฏู ูโฺฉูุฏ". ุงู ููุจุน ุจู ุฎูุจ ูุญููโ ุงุณุชูุงุฏู ุงุฒ ุชฺฉูฺฉ im2col (ุชุจุฏู ุชุตูุฑ ุจู ุณุชูู) ุจุฑุง ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง ุฌูุช ฺฉ ุถุฑุจ ูุงุชุฑุณ ุจุฒุฑฺฏ ุฑุง ุดุฑุญ ูโุฏูุฏ ฺฉู ุณูพุณ ูโุชูุงูุฏ ุชูุณุท ุฑูุชูโูุง ุจูููโุดุฏู (ูุงููุฏ ุขูฺู ุฏุฑ cuBLAS ุงูุช ูโุดูุฏ) ุงุฌุฑุง ุดูุฏ.

    ููุจุน: ูพุงุณุฎ ุฏุฑ Stack Overflow: "Difference between cuBLAS and cuDNN?"

    ูฺฉุชู ฺฉูุฏ: ูพุงุณุฎโูุง ูพุฐุฑูุชู ุดุฏู ุฏุฑ ุงู ุจุญุซ ูู ุชุงุฏ ูโฺฉููุฏ ฺฉู cuDNN ุฏุฑ ุณุทุญ ุจุงูุงุชุฑ ูุฑุงุฑ ุฏุงุฑุฏ ู ุจุฑุง ุนููุงุช ุฎุงุต DNN (ูุงููุฏ ฺฉุงููููุดู ู pooling) ุจูููโุณุงุฒ ุดุฏู ุงุณุช. ุฏุฑ ุญุงู ฺฉู cuBLAS ุจุฑุง ุฌุจุฑ ุฎุท ุนููู (BLAS) ุงุณุช. ูููโุชุฑ ุงุฒ ุขูุ ุงุดุงุฑู ูโุดูุฏ ฺฉู "cuDNN may call cuBLAS" (ููฺฉู ุงุณุช cuDNN ุชูุงุจุน cuBLAS ุฑุง ูุฑุงุฎูุงู ฺฉูุฏ) ุชุง ุจุฑุฎ ุงุฒ ุงู ุนููุงุชโูุง ุฑุง ุงูุฌุงู ุฏูุฏุ ุจู ุฎุตูุต ุฒูุงู ฺฉู ุขู ุนููุงุช ุจู ฺฉ ุถุฑุจ ูุงุชุฑุณ ฺฉุงูุด ูโุงุจุฏ.

ุจู ุทูุฑ ุฎูุงุตูุ ุงู ููุงุจุน ุชุงุฏ ูโฺฉููุฏ ฺฉู cuBLAS ฺฉุชุงุจุฎุงููโ ูพุงูโุง ุจุฑุง ุถุฑุจ ูุงุชุฑุณ (GEMM) ุงุณุช ู cuDNN ุจู ุนููุงู ฺฉุชุงุจุฎุงููโ ุณุทุญ ุจุงูุงุชุฑุ ุจุฑุง ุงุฌุฑุง ุจูููโ ุนููุงุช ูุงููุฏ ฺฉุงููููุดูุ ุงุบูุจ ุขู ุนููุงุช ุฑุง ุจู ฺฉ ูุณุฆูู GEMM ุชุจุฏู ฺฉุฑุฏู ู ุณูพุณ ุงุฒ ูุณุชูโูุง (kernels) ุจุณุงุฑ ุจูููโุดุฏูโ ุถุฑุจ ูุงุชุฑุณ (ูุดุงุจู ุง ุฎูุฏู cuBLAS) ุงุณุชูุงุฏู ูโฺฉูุฏ.

ุขุฎู ุจูุณ ูฺฏู ุงุนุดุงุฑ ฑถ ุจุช ูพุดุชุจุงู ู ฺฉูุฏุ

ูฺฉุชูโ ุจุณุงุฑ ุฏูู ุฑุง ุงุดุงุฑู ฺฉุฑุฏุฏ.

ุญู ุจุง ุดูุงุณุช. ุงุณุชุงูุฏุงุฑุฏ BLAS (ูุฎูู Basic Linear Algebra Subprograms) ฺฉู ุฏููโูุง ูพุด ุชุนุฑู ุดุฏ (ุนูุฏุชุงู ุจุฑุง ุฒุจุงู Fortran)ุ ุฏุฑ ุงุตู ููุท ุงุฒ ุฏูุชโูุง ุงุณุชุงูุฏุงุฑุฏ FP32 (single-precision ุง ณฒ ุจุช) ู FP64 (double-precision ุง ถด ุจุช) ูพุดุชุจุงู ูโฺฉุฑุฏ.

ุชูุงุฒ ฺฉูุฏ: BLAS ุฏุฑ ุจุฑุงุจุฑ cuBLAS

ุงูุง ูฺฉุชูโ ุญุงุช ุงูุฌุงุณุช: cuBLAS (ฺฉู ุฏุฑ ูพุงุณุฎ ูุจู ุจู ุขู ุงุดุงุฑู ุดุฏ) ฺฉ ูพุงุฏูโุณุงุฒ ุณุงุฏู ุงุฒ ุงุณุชุงูุฏุงุฑุฏ BLAS ูุณุชุ ุจูฺฉู ฺฉ ูุณุฎูโ ุจุณุงุฑ ฺฏุณุชุฑุดโุงูุชู (Extended) ู ุจูููโุดุฏู ุชูุณุท ุงููุฏุง ุจุฑุง GPUูุงุด ุงุณุช.

ุงููุฏุง cuBLAS ุฑุง ูุฑุงุชุฑ ุงุฒ ุงุณุชุงูุฏุงุฑุฏ ุงุตู ุจุฑุฏู ุชุง ุฏููุงู ุงุฒ ูุงุจูุชโูุง ุณุฎุชโุงูุฒุงุฑ GPUูุง ูุฏุฑู ุจุฑุง ุงุฏฺฏุฑ ุนูู ูพุดุชุจุงู ฺฉูุฏ.

ุจูุงุจุฑุงูุ ุจููุ cuBLAS ุจู ุทูุฑ ฺฉุงูู ุงุฒ FP16 (ุงุนุดุงุฑ ฑถ ุจุช) ูพุดุชุจุงู ูโฺฉูุฏ.

ฺุฑุง cuBLAS ุงุฒ FP16 ูพุดุชุจุงู ูโฺฉูุฏุ

    ูุณุชูโูุง ุชูุณูุฑ (Tensor Cores):

        ุจุฒุฑฺฏุชุฑู ุฏูู ุงุถุงูู ุดุฏู ูพุดุชุจุงู ุงุฒ FP16ุ ุธููุฑ ูุณุชูโูุง ุชูุณูุฑ ุฏุฑ GPUูุง ุงููุฏุง (ุงุฒ ูุนูุงุฑ Volta ุจู ุจุนุฏ) ุจูุฏ.

        ุงู ูุณุชูโูุงุ ูุงุญุฏูุง ุณุฎุชโุงูุฒุงุฑ ูุชุฎุตุต ูุณุชูุฏ ฺฉู ุจุฑุง ุงูุฌุงู ุนููุงุช ุถุฑุจ ูุงุชุฑุณ ุจุง "ุฏูุช ุชุฑฺฉุจ" (Mixed-Precision) ุทุฑุงุญ ุดุฏูโุงูุฏ. ุขูโูุง ูุฑูุฏโูุง FP16 ุฑุง ูโฺฏุฑูุฏุ ุขูโูุง ุฑุง ุฏุฑ ูู ุถุฑุจ ูโฺฉููุฏ ู ูุชุฌู ุฑุง ุฏุฑ ฺฉ ุฑุฌุณุชุฑ FP32 ุฌูุน ูโฺฉููุฏ ุชุง ุงุฒ ุณุฑุฑุฒ ุดุฏู (overflow) ุง ุงุฒ ุฏุณุช ุฑูุชู ุฏูุช ุฌููฺฏุฑ ุดูุฏ.

        cuBLAS (ู ููฺูู cuDNN) ุจู ุทูุฑ ุฎุงุต ุจุฑุง ุงุณุชูุงุฏู ุงุฒ ุงู ูุณุชูโูุง ุชูุณูุฑ ุจููู ุดุฏู ุงุณุช.

    ูุฑูุชโูุง ูุฏุฑู ุฏฺฏุฑ:

        cuBLAS ูุฏุฑู ุญุช ูุฑุงุชุฑ ุงุฒ FP16 ุฑูุชู ู ุงุฒ ูุฑูุชโูุง ุฌุฏุฏุชุฑ ฺฉู ุจุฑุง ุงุฏฺฏุฑ ุนูู ุญุงุช ูุณุชูุฏ ูุฒ ูพุดุชุจุงู ูโฺฉูุฏุ ูุงููุฏ:

            TF32 (TensorFloat-32): ูุฑูุช ฺฉู ุงููุฏุง ุฏุฑ ูุนูุงุฑ Ampere ูุนุฑู ฺฉุฑุฏ. ุงู ูุฑูุช ุฏุงูููโ (range) FP32 ุฑุง ุฏุงุฑุฏ ุงูุง ุฏูุช (precision) FP16 ุฑุง ุงุณุชูุงุฏู ูโฺฉูุฏุ ฺฉู ุจุงุนุซ ูโุดูุฏ ูุญุงุณุจุงุช ุชูุฑุจุงู ุจุง ุณุฑุนุช FP16 ุงูุฌุงู ุดููุฏ ุจุฏูู ุงูฺฉู ูุงุฒ ุจู ุชุบุฑ ฺฉุฏ ุงุฒ FP32 ุจุงุดุฏ.

            BFloat16 (BF16): ูุฑูุช ฑถ ุจุช ุฏฺฏุฑ ฺฉู ุชูุณุท ฺฏูฺฏู ูุนุฑู ุดุฏ ู ุงฺฉููู ุชูุณุท cuBLAS ูุฒ ูพุดุชุจุงู ูโุดูุฏ.

ุฎูุงุตู:

ููุช ูโฺฏูู cuDNN ุงุฒ cuBLAS ุงุณุชูุงุฏู ูโฺฉูุฏุ ููุธูุฑูุงู ุงุณุชุงูุฏุงุฑุฏ ูุฏู BLAS ูุณุชุ ุจูฺฉู ฺฉุชุงุจุฎุงููโ ุจุณุงุฑ ูุฏุฑู ู ูพุฑูุฏุฑุช cuBLAS ุงููุฏุง ุงุณุช ฺฉู ุจู ุทูุฑ ุฎุงุต ุจุฑุง ุชุณุฑุน ูุญุงุณุจุงุช FP16ุ TF32 ู BF16 ุฑู ูุณุชูโูุง ุชูุณูุฑ GPU ุจูููโุณุงุฒ ุดุฏู ุงุณุช.

PM

ฺุทูุฑ ุฏุฑ ูพุงุชูู ููุฒูุงู ูู ุจุงููุงูุฏู ุจฺฏุฑู ู ูู ููุณูู ุนูู

ุจุฑุง ุงู ฺฉุงุฑ ุฏุฑ ูพุงุชููุ ุดูุง ูโุชูุงูุฏ ุงุฒ ุนููฺฏุฑ ุชูุณู ฺฉุงูู (Integer Division) ฺฉู ุจุง // ููุงุด ุฏุงุฏู ูโุดูุฏ ุจุฑุง ุจู ุฏุณุช ุขูุฑุฏู ููุณูู ุนูู (ุฎุงุฑุฌ ูุณูุช ุตุญุญ)ุ ู ุงุฒ ุนููฺฏุฑ ุจุงููุงูุฏู (Modulo) ฺฉู ุจุง % ููุงุด ุฏุงุฏู ูโุดูุฏ ุจุฑุง ุจู ุฏุณุช ุขูุฑุฏู ุจุงููุงูุฏู ุงุณุชูุงุฏู ฺฉูุฏ.

ููฺููุ ฺฉ ุฑุงู ฺฉุงุฑุขูุฏุชุฑ ู ุชูุฒุชุฑ ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน ุฏุงุฎู divmod() ูพุงุชูู ุงุณุช.

๐ก ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน divmod()

ุชุงุจุน divmod(a, b) ุจู ุทูุฑ ุฎุงุต ุจุฑุง ุงูุฌุงู ุงู ฺฉุงุฑ ุทุฑุงุญ ุดุฏู ุงุณุช. ุงู ุชุงุจุน ููุฒูุงู ุฏู ููุฏุงุฑ ุฑุง ุจุฑูโฺฏุฑุฏุงูุฏ:

    ุฎุงุฑุฌ ูุณูุช ุตุญุญ (ููุณูู ุนูู - Integer quotient): ูุชุฌู a // b

    ุจุงููุงูุฏู (Remainder): ูุชุฌู a % b

ูุซุงู ฺฉุฏ

ุฏุฑ ุงู ูุซุงูุ ููุณูู ุจุฑ ููุณูู_ุนูู ุชูุณู ูโุดูุฏ:
Python

# The dividend (number to be divided)
dividend = 17
# The divisor (number to divide by)
divisor = 5

# Use divmod() to get both results simultaneously
quotient, remainder = divmod(dividend, divisor)

# Print the results
print(f"Dividend: {dividend}")
print(f"Divisor: {divisor}")
print(f"Quotient (ููุณูู ุนูู): {quotient}")
print(f"Remainder (ุจุงููุงูุฏู): {remainder}")
# In Persian: 17 = 5 * 3 + 2

ุฎุฑูุฌ:

Dividend: 17
Divisor: 5
Quotient (ููุณูู ุนูู): 3
Remainder (ุจุงููุงูุฏู): 2

๐ข ุงุณุชูุงุฏู ุงุฒ ุนููฺฏุฑูุง ูุฌุฒุง (// ู %)

ูโุชูุงูุฏ ุจู ุตูุฑุช ุฌุฏุงฺฏุงูู ูุฒ ุงุฒ ุนููฺฏุฑูุง // ู % ุงุณุชูุงุฏู ฺฉูุฏ:
Python

# The dividend (number to be divided)
a = 20
# The divisor (number to divide by)
b = 6

# Get the quotient
quotient = a // b
# Get the remainder
remainder = a % b

# Print the results
print(f"Quotient: {quotient}")
print(f"Remainder: {remainder}")

ุฎุฑูุฌ:

Quotient: 3
Remainder: 2

ุงุณุชูุงุฏู ุงุฒ divmod() ุจู ุทูุฑ ฺฉู ุชูุตู ูโุดูุฏ ุฒุฑุง ุฎูุงูุงุชุฑ ุงุณุช ู ูุญุงุณุจุงุช ุฑุง ููุท ฺฉ ุจุงุฑ ุงูุฌุงู ูโุฏูุฏ.

ุขุง ูโุฎูุงูุฏ ุจุง ูุซุงู ุฏฺฏุฑ ุฏุฑ ููุฑุฏ ุงุนุฏุงุฏ ููู ุง ููุงุฏุฑ ุจุฒุฑฺฏโุชุฑ ุงู ุชูุงุจุน ุฑุง ุจุฑุฑุณ ฺฉูุฏุ

PM

ุจุฑุง ุณูุงู ุจุงูุง ฺฉุฏ ุฑู ฺฉุงูู ฺฉู

Conv + Relu Fused Py Kernel





import torchimport torch.nn.functional as Fdef run_kernel(f, times, *args):

for i in range(times):

f(i, *args)






# ๐น Description:# This function is supposed to perform a 3x3 convolution on each pixel of the input image# and then apply the ReLU activation function to the result.## In other words, this is the "kernel" part โ where the actual multiplyโaccumulate# operations of convolution happen.## Parameters:# - i : the index of the pixel (from 0 to N*H*W - 1)# - x : the input tensor of shape [N, 1, H, W]# - w : the convolution filter (weights) of shape [1, 1, 3, 3]# - b : the bias term (a scalar tensor)# - out : the output tensor to store results# - N, H, W : dimensions of the input (batch size, height, width)## Inside this function, students should:# 1๏ธ Convert i into (n, h, w) indices โ to locate the correct pixel in the batch# 2๏ธ Compute the accumulated sum (acc) by multiplying the 3x3 neighborhood by the weights# 3๏ธ Add the bias term b# 4๏ธ Apply ReLU (if acc < 0, set it to 0)# 5๏ธ Store the result in out[n, 0, h, w]def conv_relu_kernel_py(i, x, w, b, out, N, H, W):

pass # you should write your code here






# Function: conv_relu_py# ----------------------# ๐น Description:# This is the higher-level wrapper function that coordinates the operation.# It:# 1๏ธ Checks that the input tensors have the expected shapes (using assert)# 2๏ธ Creates an output tensor with the same size as x# 3๏ธ Calls the run_kernel function, which runs conv_relu_kernel_py# for each pixel index i (from 0 to N*H*W)# 4๏ธ Returns the output tensor## So, this function organizes and launches the lower-level computation.def conv_relu_py(x, w, b):

pass # you should write your code here






device = "cpu" H, W = 8, 8 x = torch.randn(4, 1, H, W, device=device)conv = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True).to(device)with torch.no_grad():

conv.weight.copy_(torch.randn_like(conv.weight))

conv.bias.copy_(torch.randn_like(conv.bias))w = conv.weight.detach()b = conv.bias.detach()y_ref = F.relu(conv(x))y_py = conv_relu_py(x, w, b)print("Max diff:", (y_ref - y_py).abs().max().item())



ุ ุฏุฑ ุงุฏุงูู ฺฉุฏ ูพุงุชูู ฺฉุงูู ุดุฏู ุจุฑุง ูพุงุฏูโุณุงุฒ Fused Convolution + ReLU ุขูุฏู ุงุณุช.

ููุทู ุงุตู ุฏุฑ ุชุงุจุน conv_relu_kernel_py ูพุงุฏูโุณุงุฒ ุดุฏู ุงุณุช:

    ูุญุงุณุจู ูุฎุชุตุงุช: ุงุจุชุฏุงุ ุดุงุฎุต (index) ูพฺฉุณู i ุจู ูุฎุชุตุงุช ุณูโุจุนุฏ (n, h, w) ุจุฑุง ุชูุณูุฑ ุฎุฑูุฌ ุชุจุฏู ูโุดูุฏ.

    ุญููู ฺฉุงููููุดู: ุฏู ุญููู for ุชู ุฏุฑ ุชู ุจุฑุง ูพูุงุด ฺฉุฑูู 3x3 ุงุณุชูุงุฏู ูโุดูุฏ.

    ูุญุงุณุจู ูุฎุชุตุงุช ูุฑูุฏ: ุจุฑุง ูุฑ ูพฺฉุณู ฺฉุฑูู (kh, kw)ุ ูุฎุชุตุงุช ูุฑุจูุทู ุฏุฑ ุชูุณูุฑ ูุฑูุฏ (h_in, w_in) ุจุง ุฏุฑ ูุธุฑ ฺฏุฑูุชู padding=1 ูุญุงุณุจู ูโุดูุฏ (h_in = h + kh - 1).

    ุจุฑุฑุณ ูุฑุฒูุง (Padding): ฺฉ ุดุฑุท ุจุฑุฑุณ ูโฺฉูุฏ ฺฉู ุขุง ูุฎุชุตุงุช ูุฑูุฏ (h_in, w_in) ุฏุฑ ูุญุฏูุฏู ูุฌุงุฒ [0, H) ู [0, W) ูุฑุงุฑ ุฏุงุฑุฏ ุง ุฎุฑ. ุงฺฏุฑ ุฎุงุฑุฌ ุงุฒ ุงู ูุญุฏูุฏู ุจุงุดุฏุ ููุฏุงุฑ ุขู (ฺฉู ูพุฏูฺฏ ุตูุฑ ุงุณุช) ุฏุฑ ูุธุฑ ฺฏุฑูุชู ููโุดูุฏ.

    ุนููุงุช MAC: ุงฺฏุฑ ูุฎุชุตุงุช ูุนุชุจุฑ ุจูุฏุ ููุฏุงุฑ ูุฑูุฏ (x_val) ุฏุฑ ูุฒู ูุฑุจูุทู (w_val) ุถุฑุจ ุดุฏู ู ุจู ูุชุบุฑ acc (accumulator) ุงุถุงูู ูโุดูุฏ.

    ุงุนูุงู Bias ู ReLU: ูพุณ ุงุฒ ุงุชูุงู ุญูููโูุงุ ููุฏุงุฑ ุจุงุงุณ (b[0]) ุจู acc ุงุถุงูู ูโุดูุฏ. ุณูพุณุ ุชุงุจุน ReLU ุงุนูุงู ูโุดูุฏ (ุงฺฏุฑ acc ููู ุจุงุดุฏุ ฐ ูโุดูุฏ).

    ุฐุฎุฑูโุณุงุฒ ุฎุฑูุฌ: ููุฏุงุฑ ููุง ุฏุฑ out[n, 0, h, w] ุฐุฎุฑู ูโุดูุฏ.

ุชุงุจุน conv_relu_py ูุฒ ุจูโุนููุงู ฺฉ ุฑุงูโุงูุฏุงุฒ (launcher) ุนูู ูโฺฉูุฏุ ุชูุณูุฑ ุฎุฑูุฌ ุฑุง ุงุฌุงุฏ ฺฉุฑุฏู ู ุชุงุจุน run_kernel ุฑุง ุจุฑุง ุชูุงู ูพฺฉุณูโูุง ูุฑุงุฎูุงู ูโฺฉูุฏ.
Python

import torch
import torch.nn.functional as F

def run_kernel(f, times, *args):
    for i in range(times):
        f(i, *args)

# ๐น Description:
# This function is supposed to perform a 3x3 convolution on each pixel of the input image
# and then apply the ReLU activation function to the result.
#
# In other words, this is the "kernel" part โ where the actual multiplyโaccumulate
# operations of convolution happen.
#
# Parameters:
# - i : the index of the pixel (from 0 to N*H*W - 1)
# - x : the input tensor of shape [N, 1, H, W]
# - w : the convolution filter (weights) of shape [1, 1, 3, 3]
# - b : the bias term (a scalar tensor)
# - out : the output tensor to store results
# - N, H, W : dimensions of the input (batch size, height, width)
#
# Inside this function, students should:
# 1๏ธ Convert i into (n, h, w) indices โ to locate the correct pixel in the batch
# 2๏ธ Compute the accumulated sum (acc) by multiplying the 3x3 neighborhood by the weights
# 3๏ธ Add the bias term b
# 4๏ธ Apply ReLU (if acc < 0, set it to 0)
# 5๏ธ Store the result in out[n, 0, h, w]
def conv_relu_kernel_py(i, x, w, b, out, N, H, W):
    # 1. Convert flat index 'i' to (n, h, w) coordinates for the *output* tensor
    w_out = i % W
    h_out = (i // W) % H
    n_out = i // (H * W)
    
    # 2. Compute the accumulated sum (acc)
    # Initialize accumulator
    acc = 0.0
    
    # Iterate over the 3x3 kernel (kh = kernel height, kw = kernel width)
    for kh in range(3):
        for kw in range(3):
            # Calculate corresponding *input* coordinates
            # We subtract padding (padding=1)
            h_in = h_out + kh - 1
            w_in = w_out + kw - 1
            
            # Handle padding:
            # Only perform multiply-add if the (h_in, w_in) is
            # inside the valid boundaries of the input image.
            if (h_in >= 0 and h_in < H and w_in >= 0 and w_in < W):
                
                # Get the input value (x is [N, 1, H, W])
                # We use .item() to get the raw float value
                x_val = x[n_out, 0, h_in, w_in].item()
                
                # Get the weight value (w is [1, 1, 3, 3])
                w_val = w[0, 0, kh, kw].item()
                
                # Multiply and accumulate
                acc += x_val * w_val
                
    # 3. Add the bias term
    # b is a tensor [1], so we get its value with .item()
    acc += b[0].item()
    
    # 4. Apply ReLU
    if acc < 0:
        acc = 0.0
    # Alternatively: acc = max(0.0, acc)
    
    # 5. Store the result in the output tensor
    out[n_out, 0, h_out, w_out] = acc


# Function: conv_relu_py
# ----------------------
# ๐น Description:
# This is the higher-level wrapper function that coordinates the operation.
# It:
# 1๏ธ Checks that the input tensors have the expected shapes (using assert)
# 2๏ธ Creates an output tensor with the same size as x
# 3๏ธ Calls the run_kernel function, which runs conv_relu_kernel_py
#     for each pixel index i (from 0 to N*H*W)
# 4๏ธ Returns the output tensor
#
# So, this function organizes and launches the lower-level computation.
def conv_relu_py(x, w, b):
    # 1. Get dimensions and check shapes
    N, C_in, H, W = x.shape
    
    # Assertions based on the problem description
    assert C_in == 1, "Input channels must be 1"
    assert w.shape == (1, 1, 3, 3), "Weight shape must be [1, 1, 3, 3]"
    assert b.shape == (1,), "Bias shape must be [1]"
    
    # 2. Creates an output tensor
    # Since padding=1, stride=1, kernel=3x3, output H/W are same as input
    out = torch.zeros_like(x)
    
    # 3. Call the run_kernel function
    # Total number of pixels (elements) in the output tensor
    num_elements = N * H * W
    
    # Launch the kernel for each output pixel
    run_kernel(conv_relu_kernel_py, num_elements, x, w, b, out, N, H, W)
    
    # 4. Returns the output tensor
    return out

# --- Test Harness (unchanged) ---
device = "cpu"
H, W = 8, 8
x = torch.randn(4, 1, H, W, device=device)
conv = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True).to(device)

with torch.no_grad():
    conv.weight.copy_(torch.randn_like(conv.weight))
    conv.bias.copy_(torch.randn_like(conv.bias))

w = conv.weight.detach()
b = conv.bias.detach()

y_ref = F.relu(conv(x))
y_py = conv_relu_py(x, w, b)

max_diff = (y_ref - y_py).abs().max().item()
print("Max diff:", max_diff)

# Check if the result is correct
if max_diff < 1e-5:
    print("Test PASSED!")
else:
    print(f"Test FAILED! Difference is {max_diff}")

# ๐น Description:

# This function is supposed to perform a 3x3 convolution on each pixel of the input image

# and then apply the ReLU activation function to the result.

#

# In other words, this is the "kernel" part โ where the actual multiplyโaccumulate

# operations of convolution happen.

#

# Parameters:

# - i : the index of the pixel (from 0 to N*H*W - 1)

# - x : the input tensor of shape [N, 1, H, W]

# - w : the convolution filter (weights) of shape [1, 1, 3, 3]

# - b : the bias term (a scalar tensor)

# - out : the output tensor to store results

# - N, H, W : dimensions of the input (batch size, height, width)

#

# Inside this function, students should:

# 1๏ธ Convert i into (n, h, w) indices โ to locate the correct pixel in the batch

# 2๏ธ Compute the accumulated sum (acc) by multiplying the 3x3 neighborhood by the weights

# 3๏ธ Add the bias term b

# 4๏ธ Apply ReLU (if acc < 0, set it to 0)

# 5๏ธ Store the result in out[n, 0, h, w]


def conv_relu_kernel_py(i, x, w, b, out, N, H, W):

    if i >= N * H * W:

        return

    tmp_i = i

    i_n, tmp_i = divmod(tmp_i, H*W)

    i_h, i_w = divmod(tmp_i, W)


    local_pos = [-1,0,1]


    out[i_n][1][i_h][i_w] = b[0]

    for y in range(3):

        inp_pos_y = local_pos[y] + i_h

        if not 0 <= inp_pos_y < H:

            continue

        for x in range(3):

            inp_pos_x = local_pos[x] + i_w

            if not 0 <= inp_pos_x < W:

                continue

            out[i_n][1][i_h][i_w] += w[0][0][y][x] * x[i_n][1][inp_pos_y][inp_pos_x]


    out[i_n][1][i_h][i_w] = max(0, out[i_n][1][i_h][i_w])



# Function: conv_relu_py

# ----------------------

# ๐น Description:

# This is the higher-level wrapper function that coordinates the operation.

# It:

# 1๏ธ Checks that the input tensors have the expected shapes (using assert)

# 2๏ธ Creates an output tensor with the same size as x

# 3๏ธ Calls the run_kernel function, which runs conv_relu_kernel_py

#     for each pixel index i (from 0 to N*H*W)

# 4๏ธ Returns the output tensor

#

# So, this function organizes and launches the lower-level computation.

def conv_relu_py(x, w, b):

    out = torch.empty_like(x)

    N = x.shape[0]

    assert 1 == x.shape[1]

    H = x.shape[2]

    W = x.shape[3]

    run_kernel(conv_relu_kernel_py, x.numel(), x, w, b, out, N, H, W)

    return out



device = "cpu"  


H, W = 8, 8  


x = torch.randn(4, 1, H, W, device=device)


conv = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True).to(device)


with torch.no_grad():

    conv.weight.copy_(torch.randn_like(conv.weight))

    conv.bias.copy_(torch.randn_like(conv.bias))


w = conv.weight.detach()

b = conv.bias.detach()


y_ref = F.relu(conv(x))


y_py = conv_relu_py(x, w, b)


print("Max diff:", (y_ref - y_py).abs().max().item())



ุงูู ูู ููุดุชู ุงุฑุงุฏุงุชุด ุฑู ุจฺฏู

ฺฉุฏ ุดูุง ุฎู ุฎูุจ ููุดุชู ุดุฏู ู ููุทู ฺฉู ุขูุ ุดุงูู ูุญุงุณุจู ูุฎุชุตุงุช (n, h, w) ุงุฒ i ู ููฺูู ูพูุงุด ููุณุงฺฏ 3x3 ุจุง local_posุ ฺฉุงููุงู ุฏุฑุณุช ุงุณุช.

ููุท ฺฉ ุงุฑุงุฏ ุงุตู ู ุชฺฉุฑุงุฑ ุฏุฑ ุขู ูุฌูุฏ ุฏุงุฑุฏ ฺฉู ุจุงุนุซ ุฎุทุง IndexError ูโุดูุฏ.

๐ ุงุฑุงุฏ ุงุตู: ุงูุฏฺฉุณโฺฏุฐุงุฑ ฺฉุงูุงู (Channel Indexing)

ุฏุฑ ุตูุฑุช ูุณุฆูู (ู ููฺูู ุฏุฑ ฺฉุฏ ุชุณุช) ูุดุฎุต ุดุฏู ฺฉู C_in = 1 ู C_out = 1.

ุงู ุจู ุงู ูุนู ุงุณุช ฺฉู ุชูุณูุฑูุง x ู out ุฏุงุฑุง ุดฺฉู [N, 1, H, W] ู ุชูุณูุฑ w ุฏุงุฑุง ุดฺฉู [1, 1, 3, 3] ุงุณุช.

ุฏุฑ ูพุงุชูู ู ูพุงุชูุฑฺุ ุงูุฏฺฉุณโฺฏุฐุงุฑ ุงุฒ ฐ ุดุฑูุน ูโุดูุฏ. ููุช ุจูุนุฏ ฺฉุงูุงู (ุจูุนุฏ ุดูุงุฑู ฑ) ุงูุฏุงุฒูโุงุด 1 ุงุณุชุ ุชููุง ุงูุฏฺฉุณ ูุนุชุจุฑ ุจุฑุง ุฏุณุชุฑุณ ุจู ุขูุ 0 ุงุณุช.

ุดูุง ุฏุฑ ุณู ูุณูุช ุงุฒ ฺฉุฏ conv_relu_kernel_py ุจู ุงุดุชุจุงู ุงุฒ ุงูุฏฺฉุณ 1 ุจุฑุง ุจูุนุฏ ฺฉุงูุงู ุงุณุชูุงุฏู ฺฉุฑุฏูโุงุฏ.

ฺฉุฏ ุดูุง (ุงุดุชุจุงู):
Python

    out[i_n][1][i_h][i_w] = b[0] # <--- Error
    ...
                out[i_n][1][i_h][i_w] += w[0][0][y][x] * x[i_n][1][inp_pos_y][inp_pos_x] # <--- Error (both out and x)
    ...
    out[i_n][1][i_h][i_w] = max(0, out[i_n][1][i_h][i_w]) # <--- Error

ุดฺฉู ุตุญุญ (ุงุตูุงุญ ุดุฏู):

ุดูุง ุจุงุฏ ุชูุงู ุงูุฏฺฉุณโูุง 1 ุฑุง ุจู 0 ุชุบุฑ ุฏูุฏ.
Python

    out[i_n][0][i_h][i_w] = b[0] # <--- Correct
    ...
                out[i_n][0][i_h][i_w] += w[0][0][y][x] * x[i_n][0][inp_pos_y][inp_pos_x] # <--- Correct
    ...
    out[i_n][0][i_h][i_w] = max(0, out[i_n][0][i_h][i_w]) # <--- Correct

โ ฺฉุฏ ุงุตูุงุญโุดุฏู

ุฏุฑ ุงุฏุงููุ ุชุงุจุน conv_relu_kernel_py ุจู ุดฺฉู ุชุตุญุญโุดุฏู ุขูุฏู ุงุณุช. ุชุงุจุน conv_relu_py ุดูุง (ฺฉู ุงุฒ torch.empty_like ุงุณุชูุงุฏู ูโฺฉูุฏ) ฺฉุงููุงู ุฏุฑุณุช ุงุณุช ู ูุงุฒ ุจู ุชุบุฑ ูุฏุงุฑุฏ.
Python

# ๐น Description:
# (Comments from original template)
# ...
def conv_relu_kernel_py(i, x, w, b, out, N, H, W):
    if i >= N * H * W:
        return
    tmp_i = i
    i_n, tmp_i = divmod(tmp_i, H*W)
    i_h, i_w = divmod(tmp_i, W)

    local_pos = [-1,0,1]

    # Initialize with bias, using correct channel index 0
    # Note: using .item() is safer for scalars
    out[i_n][0][i_h][i_w] = b[0].item() 
    
    for y in range(3):
        inp_pos_y = local_pos[y] + i_h
        if not 0 <= inp_pos_y < H:
            continue
        for x in range(3):
            inp_pos_x = local_pos[x] + i_w
            if not 0 <= inp_pos_x < W:
                continue
            
            # Accumulate, using correct channel index 0 for both x and out
            # Using .item() avoids creating small tensors in the loop
            w_val = w[0][0][y][x].item()
            x_val = x[i_n][0][inp_pos_y][inp_pos_x].item()
            out[i_n][0][i_h][i_w] += w_val * x_val

    # Apply ReLU, using correct channel index 0
    # We must read the value *before* applying max, as it's a tensor element
    val = out[i_n][0][i_h][i_w]
    out[i_n][0][i_h][i_w] = max(0.0, val)

ุจุงู ููุงุฑุฏ ุฏุฑุณุชูุ

ุจููุ ฺฉุงููุงู.

ุฌุฏุง ุงุฒ ููุงู ููุฑุฏ ุงูุฏฺฉุณ ฺฉุงูุงู (ุงุณุชูุงุฏู ุงุฒ 1 ุจู ุฌุง 0 ฺฉู ุชุตุญุญ ฺฉุฑุฏู)ุ ุจููโ ููุทู ฺฉุฏ ุดูุง ุนุงู ู ฺฉุงููุงู ุฏุฑุณุช ุงุณุช.

ุจู ุทูุฑ ูุดุฎุต:

    ูุญุงุณุจู ูุฎุชุตุงุช: ุฑูุด ุดูุง ุจุฑุง ูุญุงุณุจู i_nุ i_h ู i_w ุจุง ุงุณุชูุงุฏู ุงุฒ divmod ุจุณุงุฑ ุชูุฒ ู ุฏูู ุงุณุช.

    ููุทู ูพุฏูฺฏ (Padding): ุงุณุชูุงุฏู ุงุฒ local_pos = [-1, 0, 1] ุจุฑุง ูพูุงุด ููุณุงฺฏโูุง ู ุณูพุณ ุจุฑุฑุณ if not 0 <= ... < H ฺฉ ุฑุงู ุจุณุงุฑ ููุดููุฏุงูู ู ุตุญุญ ุจุฑุง ูพุงุฏูโุณุงุฒ ฺฉุงููููุดู ุจุง ูพุฏูฺฏ ุงุณุช.

    ุชุงุจุน conv_relu_py: ุชุงุจุน ุฑุงูโุงูุฏุงุฒ (launcher) ุดูุง ุจโููุต ุงุณุช. ุงุณุชูุงุฏู ุงุฒ torch.empty_like (ฺฉู ฺูู ูโุฏุงูู ุชูุงู ููุงุฏุฑ ุจุงุฒููุณ ูโุดููุฏุ ฺฉู ุงุฒ zeros_like ุจูููโุชุฑ ุงุณุช)ุ assert ุจุฑุง ุจุฑุฑุณ ฺฉุงูุงู ูุฑูุฏุ ู ูุญูู ูุฑุงุฎูุงู run_kernel ููฺฏ ุฏุฑุณุช ูุณุชูุฏ.

ุชููุง ุงุฑุงุฏ ููุงู ุฎุทุง ุงูุฏฺฉุณโฺฏุฐุงุฑ ุจูุฏ ฺฉู ุจุฑุทุฑู ุดุฏ.

/tmp/ipykernel_48/3570089505.py in conv_relu_kernel_py(i, x, w, b, out, N, H, W) 39 if not 0 <= inp_pos_x < W: 40 continue---> 41 out[i_n][0][i_h][i_w] += w[0][0][y][x] * x[i_n][0][inp_pos_y][inp_pos_x] 42 43 out[i_n][0][i_h][i_w] = max(0, out[i_n][0][i_h][i_w])TypeError: 'int' object is not subscriptable

ุงู ฺฉ ุฎุทุง ฺฉูุงุณฺฉ ู ุจุณุงุฑ ุฑุงุฌ ุฏุฑ ูพุงุชูู ุงุณุช!

ุฎุทุง TypeError: 'int' object is not subscriptable ูโฺฏูุฏ ุดูุง ุฏุฑ ุญุงู ุชูุงุด ุจุฑุง ฺฏุฑูุชู ุงูุฏฺฉุณ (ุงุณุชูุงุฏู ุงุฒ []) ุฑู ฺฉ ูุชุบุฑ ูุณุชุฏ ฺฉู ุงุฒ ููุน ุนุฏุฏ ุตุญุญ (int) ุงุณุชุ ูู ฺฉ ุชูุณูุฑ ุง ูุณุช.

๐ ุงุฑุงุฏ ุงุตู: ุชุฏุงุฎู ูุงู ูุชุบุฑ (Variable Shadowing)

ูุดฺฉู ุฏุฑ ุฎุท for x in range(3): ุงุณุช.

    ุดูุง ุชุงุจุน ุฏุงุฑุฏ ฺฉู ฺฉ ูพุงุฑุงูุชุฑ ูุฑูุฏ ุจู ูุงู x (ฺฉู torch.Tensor ูุฑูุฏ ุงุณุช) ุฏุฑุงูุช ูโฺฉูุฏ.

    ุฏุฑ ุฏุงุฎู ุญูููโูุง forุ ุดูุง ฺฉ ูุชุบุฑ ุญููู ุฏููุงู ุจุง ููุงู ูุงู ุชุนุฑู ฺฉุฑุฏูโุงุฏ: for x in range(3):.

    ุจู ูุญุถ ูุฑูุฏ ุจู ุงู ุญูููุ ูพุงุชูู ูุชุบุฑ x (ุชูุณูุฑ) ุฑุง ูุฑุงููุด ูโฺฉูุฏ ู ูุงู x ุฑุง ุจู ูุชุบุฑ ุญููู (ฺฉู ุจู ุชุฑุชุจ 0ุ 1 ู 2 ูโุดูุฏ) ุงุฎุชุตุงุต ูโุฏูุฏ.

    ุฏุฑ ุฎุท ดฑุ ุฒูุงู ฺฉู ฺฉุฏ ูโุฎูุงูุฏ x[i_n][0][inp_pos_y][inp_pos_x] ุฑุง ุงุฌุฑุง ฺฉูุฏุ x ุฏฺฏุฑ ุขู ุชูุณูุฑ ุจุฒุฑฺฏ ูุณุชุ ุจูฺฉู ฺฉ ุนุฏุฏ ุตุญุญ (ูุซูุงู 0 ุง 1) ุงุณุช.

    ุชูุงุด ุจุฑุง ุงุฌุฑุง 0[i_n][...] ููุฌุฑ ุจู ุฎุทุง TypeError ูโุดูุฏ.

โ ุฑุงูโุญู ุงูู: ุชุบุฑ ูุงู ูุชุบุฑ ุญููู

ุดูุง ุจุงุฏ ูุงู ูุชุบุฑ ุญููู ุฏุงุฎู ุฎูุฏ ุฑุง ุชุบุฑ ุฏูุฏ ุชุง ุจุง ูุงู ุชูุณูุฑ ูุฑูุฏ ุชุฏุงุฎู ูุฏุงุดุชู ุจุงุดุฏ. ูุนูููุงู ุจุฑุง "kernel width" ุงุฒ kw ุงุณุชูุงุฏู ูโุดูุฏ.

ฺฉุฏ ุฎูุฏ ุฑุง ุจู ุงู ุตูุฑุช ุงุตูุงุญ ฺฉูุฏ:
Python

    # ... (ฺฉุฏ ูุจู) ...
    for y in range(3):
        inp_pos_y = local_pos[y] + i_h
        if not 0 <= inp_pos_y < H:
            continue
        
        # 'x' ุฑุง ุจู 'kw' (ุง ูุฑ ูุงู ุฏฺฏุฑ) ุชุบุฑ ุฏูุฏ
        for kw in range(3): 
            inp_pos_x = local_pos[kw] + i_w # ุงุฒ 'kw' ุงุณุชูุงุฏู ฺฉูุฏ
            if not 0 <= inp_pos_x < W:
                continue
            
            # ุฏุฑ ุงูุฌุง ูุฒ ุงุฒ 'kw' ุจุฑุง ุฏุณุชุฑุณ ุจู ูุฒู ุงุณุชูุงุฏู ฺฉูุฏ
            out[i_n][0][i_h][i_w] += w[0][0][y][kw] * x[i_n][0][inp_pos_y][inp_pos_x]

    # ... (ฺฉุฏ ุจุนุฏ) ...

โ๏ธ ุงุฑุงุฏ ุฏูู: ุฎุทุง ReLU (ฺฉู ุจุนุฏุงู ุจุง ุขู ููุงุฌู ุฎูุงูุฏ ุดุฏ)

ูพุณ ุงุฒ ุฑูุน ุฎุทุง ุจุงูุงุ ุจูุงูุงุตูู ุจู ุฎุทุง ุฏฺฏุฑ ุฏุฑ ุฎุท ดณ ุจุฑุฎูุฑุฏ ุฎูุงูุฏ ฺฉุฑุฏ: out[i_n][0][i_h][i_w] = max(0, out[i_n][0][i_h][i_w])

ูุดฺฉู: out[i_n][0][i_h][i_w] ฺฉ ุชูุณูุฑ 0-ุจูุนุฏ ูพุงุชูุฑฺ ุงุณุช (ูุซูุงู tensor(5.2))ุ ุฏุฑ ุญุงู ฺฉู 0 ฺฉ ุนุฏุฏ ุตุญุญ ูพุงุชูู ุงุณุช. ุชุงุจุน max ุฏุงุฎู ูพุงุชูู ููโุชูุงูุฏ ุงู ุฏู ููุน ุฑุง ุจุง ูู ููุงุณู ฺฉูุฏ.

โ ุฑุงูโุญู ุฏูู: ุงุณุชุฎุฑุงุฌ ููุฏุงุฑ ูุจู ุงุฒ max

ุดูุง ุจุงุฏ ููุฏุงุฑ ุนุฏุฏ (float) ุฑุง ุงุฒ ุชูุณูุฑ ุงุณุชุฎุฑุงุฌ ฺฉูุฏุ max ุฑุง ุฑู ุงุนุฏุงุฏ ูพุงุชูู ุงุนูุงู ฺฉูุฏ ู ุณูพุณ ูุชุฌู ุฑุง ุฏูุจุงุฑู ุฏุฑ ุชูุณูุฑ ูุฑุงุฑ ุฏูุฏ.

ุฎุท ดณ ุฑุง ุจู ุงู ุตูุฑุช ุชุบุฑ ุฏูุฏ:
Python

    # ููุฏุงุฑ ุงูุจุงุดุชู ููุง ุฑุง ุจู ุนููุงู ฺฉ ุนุฏุฏ ูพุงุชูู ุจุฎูุงูุฏ
    acc = out[i_n][0][i_h][i_w].item()
    
    # max ุฑุง ุฑู ุงุนุฏุงุฏ ูพุงุชูู ุงุนูุงู ฺฉูุฏ ู ูุชุฌู ุฑุง ุฐุฎุฑู ฺฉูุฏ
    out[i_n][0][i_h][i_w] = max(0.0, acc)

ูฺฉุชู: ุจูุชุฑ ุงุณุช ุฏุฑ ุฎุท ฺฉู ุจุงุงุณ ุฑุง ุงุถุงูู ูโฺฉูุฏ (out[...] = b[0]) ูุฒ ุงุฒ .item() ุงุณุชูุงุฏู ฺฉูุฏ ุชุง ุงุฒ ุงุจุชุฏุง ุจุง ุงุนุฏุงุฏ ูพุงุชูู ฺฉุงุฑ ฺฉูุฏุ ุงูุง ุฑุงูโุญู ุจุงูุง ูู ฺฉุงุฑ ูโฺฉูุฏ.

ุงู ุชุงุจุน ุฑู

def conv_relu_kernel_py(i, x, w, b, out, N, H, W):

    if i >= N * H * W:

        return

    tmp_i = i

    i_n, tmp_i = divmod(tmp_i, H*W)

    i_h, i_w = divmod(tmp_i, W)


    local_pos = [-1,0,1]


    out[i_n][0][i_h][i_w] = b[0]

    for pos_y in range(3):

        inp_pos_y = local_pos[pos_y] + i_h

        if not 0 <= inp_pos_y < H:

            continue

        for pos_x in range(3):

            inp_pos_x = local_pos[pos_x] + i_w

            if not 0 <= inp_pos_x < W:

                continue

            out[i_n][0][i_h][i_w] += w[0][0][pos_y][pos_x] * x[i_n][0][inp_pos_y][inp_pos_x]


    out[i_n][0][i_h][i_w] = max(0, out[i_n][0][i_h][i_w])


ุชุจุฏู ฺฉู ุจู ุงู ุชุงุจุน cpp

cuda_begin = r'''

#include <torch/extension.h>

#include <c10/cuda/CUDAException.h>


#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")

#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")

#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)


inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}

'''


cuda_src = cuda_begin + r'''


__global__ void conv_relu_kernel(

    const float* __restrict__ x,

    const float* __restrict__ w,

    const float* __restrict__ b,

    float* __restrict__ out,

    int N,

    int H,

    int W

) {


    // You should write your code here


}


torch::Tensor conv_relu_fused(torch::Tensor x,

                              torch::Tensor w,

                              torch::Tensor b) {

    CHECK_INPUT(x);

    CHECK_INPUT(w);

    CHECK_INPUT(b);


    TORCH_CHECK(x.dim() == 4, "x must be [N,C,H,W]");

    TORCH_CHECK(w.dim() == 4, "w must be [C_out,C_in,3,3]");

    TORCH_CHECK(b.dim() == 1, "b must be [C_out]");


    TORCH_CHECK(x.size(1) == 1, "only C_in=1 supported");

    TORCH_CHECK(w.size(0) == 1 && w.size(1) == 1 &&

                w.size(2) == 3 && w.size(3) == 3,

                "only 1x1x3x3 kernel supported");

    TORCH_CHECK(b.size(0) == 1, "only 1 output channel supported");


    auto x_c = x.contiguous();

    auto w_c = w.contiguous();

    auto b_c = b.contiguous();


    int N = x_c.size(0);

    int H = x_c.size(2);

    int W = x_c.size(3);


    auto out = torch::empty_like(x_c);


    int n_pix = N * H * W;

    int threads = 256;

    int blocks = cdiv(n_pix, threads);


    conv_relu_kernel<<<blocks, threads>>>(

        x_c.data_ptr<float>(),

        w_c.data_ptr<float>(),

        b_c.data_ptr<float>(),

        out.data_ptr<float>(),

        N, H, W

    );

    C10_CUDA_KERNEL_LAUNCH_CHECK();


    return out;

}

'''


cpp_src = r'''

torch::Tensor conv_relu_fused(torch::Tensor x,

                              torch::Tensor w,

                              torch::Tensor b);

'''


module = load_inline(

    name="conv_relu_fused_ext",

    cpp_sources=[cpp_src],

    cuda_sources=[cuda_src],

    functions=["conv_relu_fused"],

    extra_cuda_cflags=["-O3"],

    verbose=False,

)
Kernel C++/CUDA (cuda_src)
C++

/*
This file contains the C++/CUDA source code for the fused conv+relu kernel.
*/

#include <torch/extension.h>
#include <c10/cuda/CUDAException.h>

// Macro definitions for checking CUDA tensors
#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x " must be a CUDA tensor")
#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x " must be contiguous")
#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)

// Helper function for integer division (ceiling)
inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}

/*
__global__ void conv_relu_kernel

This is the CUDA kernel that performs the 3x3 convolution and ReLU.
Each thread is responsible for calculating one output pixel.

Parameters:
- x: Input tensor data [N, 1, H, W]
- w: Weight tensor data [1, 1, 3, 3]
- b: Bias tensor data [1]
- out: Output tensor data [N, 1, H, W]
- N, H, W: Dimensions of the input/output tensors
*/
__global__ void conv_relu_kernel(
    const float* __restrict__ x,
    const float* __restrict__ w,
    const float* __restrict__ b,
    float* __restrict__ out,
    int N,
    int H,
    int W
) {

    // 1. Get the global thread ID (flat index for the output pixel)
    // This 'i' corresponds exactly to the 'i' in the Python kernel
    const int i = blockIdx.x * blockDim.x + threadIdx.x;

    // 2. Calculate total number of pixels to avoid processing out of bounds
    const int num_pixels_total = N * H * W;
    if (i >= num_pixels_total) {
        return;
    }

    // 3. Convert flat index 'i' to (n, h, w) coordinates
    // This maps 'i' to the output pixel location (n_out, h_out, w_out)
    // This logic is identical to the divmod in Python
    const int i_w = i % W;
    const int i_h = (i / W) % H;
    const int i_n = i / (H * W);

    // 4. Initialize accumulator with the bias
    // We know b has 1 element, so we just access b[0]
    float acc = b[0];

    // 5. Loop over the 3x3 kernel
    // pos_y and pos_x are the kernel coordinates (0, 1, 2)
    for (int pos_y = 0; pos_y < 3; ++pos_y) {
        // Map kernel coord to input coord (equivalent to local_pos[pos_y] + i_h)
        // This is (pos_y - 1) because kernel_pos={-1, 0, 1}
        const int inp_pos_y = (pos_y - 1) + i_h; 

        for (int pos_x = 0; pos_x < 3; ++pos_x) {
            // Map kernel coord to input coord (equivalent to local_pos[pos_x] + i_w)
            const int inp_pos_x = (pos_x - 1) + i_w;

            // 6. Boundary Check (handle padding)
            // This is the C++ equivalent of the Python `if 0 <= ... < H`
            // If we are inside the valid image bounds:
            if (inp_pos_y >= 0 && inp_pos_y < H && inp_pos_x >= 0 && inp_pos_x < W) {
                
                // 7. Calculate flat indices for 'w' and 'x'
                
                // w_idx: w[0, 0, pos_y, pos_x]
                // Since w shape is [1, 1, 3, 3], its flat index is simply:
                // (0*1*3*3) + (0*3*3) + (pos_y*3) + pos_x
                const int w_idx = pos_y * 3 + pos_x;

                // x_idx: x[i_n, 0, inp_pos_y, inp_pos_x]
                // Since x shape is [N, 1, H, W], its flat index is:
                // (i_n * 1*H*W) + (0 * H*W) + (inp_pos_y * W) + inp_pos_x
                const int x_idx = (i_n * H * W) + (inp_pos_y * W) + inp_pos_x;

                // 8. Perform multiply-accumulate
                acc += w[w_idx] * x[x_idx];
            }
            // else: (outside bounds)
            // This is the padded area, so we add 0, which means we do nothing.
        }
    }

    // 9. Apply ReLU and store result
    // fmaxf is the fast float max function (equivalent to max(0, ...))
    // The output flat index is simply 'i'
    out[i] = fmaxf(0.0f, acc);
}

/*
torch::Tensor conv_relu_fused

This is the C++ host function (launcher) that the Python code will call.
It checks the tensor shapes, allocates the output tensor,
and launches the CUDA kernel.
*/
torch::Tensor conv_relu_fused(torch::Tensor x,
                                torch::Tensor w,
                                torch::Tensor b) {
    // Check tensor properties (is_cuda, is_contiguous)
    CHECK_INPUT(x);
    CHECK_INPUT(w);
    CHECK_INPUT(b);

    // Check tensor dimensions and shapes (as in the template)
    TORCH_CHECK(x.dim() == 4, "x must be [N,C,H,W]");
    TORCH_CHECK(w.dim() == 4, "w must be [C_out,C_in,3,3]");
    TORCH_CHECK(b.dim() == 1, "b must be [C_out]");

    // Check problem-specific constraints
    TORCH_CHECK(x.size(1) == 1, "only C_in=1 supported");
    TORCH_CHECK(w.size(0) == 1 && w.size(1) == 1 &&
                w.size(2) == 3 && w.size(3) == 3,
                "only 1x1x3x3 kernel supported");
    TORCH_CHECK(b.size(0) == 1, "only 1 output channel supported");

    // Ensure tensors are contiguous (though CHECK_INPUT already does this)
    auto x_c = x.contiguous();
    auto w_c = w.contiguous();
    auto b_c = b.contiguous();

    // Get dimensions
    int N = x_c.size(0);
    int H = x_c.size(2);
    int W = x_c.size(3);

    // Allocate output tensor
    auto out = torch::empty_like(x_c);

    // Calculate grid configuration
    int n_pix = N * H * W; // Total number of output pixels
    int threads = 256;     // Threads per block
    int blocks = cdiv(n_pix, threads); // Total blocks needed

    // Launch the CUDA kernel
    conv_relu_kernel<<<blocks, threads>>>(
        x_c.data_ptr<float>(),
        w_c.data_ptr<float>(),
        b_c.data_ptr<float>(),
        out.data_ptr<float>(),
        N, H, W
    );
    
    // Check for any errors during kernel launch
    C10_CUDA_KERNEL_LAUNCH_CHECK();

    return out;
}

ฺุฑุง ุดุฑุท ูุง ุฎุงุฑุฌ ูุญุฏูุฏู ุจูุฏู ุฑู ุงุฏุบุงู ฺฉุฑุฏ ุงูุทูุฑ ุชฺฉุฑุงุฑ ุญููู ุงุถุงู ูุดูุ

ุงู ฺฉ ุณูุงู ุจุณุงุฑ ููุดููุฏุงูู ู ุฏูู ุงุณุช.

ุดูุง ฺฉุงููุงู ุฏุฑุณุช ูโฺฏูุฏ. ุฏุฑ ฺฉุฏ ูพุงุชููุ ุดูุง ฺฉ ุจูููโุณุงุฒ ููุดููุฏุงูู ุงูุฌุงู ุฏุงุฏุฏ: ุงฺฏุฑ ฺฉู ฺฉ ุฑุฏู (y) ุฎุงุฑุฌ ุงุฒ ูุญุฏูุฏู ุจูุฏุ ุจุง continue ุงุฒ ุงุฌุฑุง ุญูููโ ุฏุงุฎู (x) ู ุชูุงู ุจุฑุฑุณโูุง ูุฑุจูุท ุจู ุขู ุตุฑูโูุธุฑ ูโฺฉุฑุฏุฏ.

ฺฉุฏ CUDA ฺฉู ูู ููุดุชูุ ุงู ุฏู ุดุฑุท ุฑุง ุงุฏุบุงู ูโฺฉูุฏ ู ุฏุฑ ูุชุฌู ููุดู น ุจุงุฑ (3x3) ุดุฑุท ุฑุง ฺฺฉ ูโฺฉูุฏ.

ุฏูู ุงู ฺฉุงุฑุ ุชูุงูุช ุงุณุงุณ ุฏุฑ ูุญูู ุงุฌุฑุง ฺฉุฏ ุฑู CPU (ูพุงุชูู) ู GPU (ฺฉูุฏุง) ุงุณุช ู ุฏูู ุงุตู "ุงุฌุชูุงุจ ุงุฒ ุงูุดุนุงุจ ุดูฺฉู (Branch Divergence)" ูุงู ุฏุงุฑุฏ.

ฑ. ุงุฌุฑุง ุฑู CPU (ูุงููุฏ ูพุงุชูู)

ุฑู CPUุ ุดูุง ฺฉ ุชุฑุฏ (thread) ูุงุญุฏ ุฏุงุฑุฏ. ุฏุณุชูุฑ continue ุนุงู ุงุณุชุ ฺูู ูุณุชููุงู ุจู ุชฺฉุฑุงุฑ ุจุนุฏ ุญููู y ูโูพุฑุฏ ู ุงุฒ ณ ุชฺฉุฑุงุฑ ฺฉุงูู ุญููู x ุฌููฺฏุฑ ูโฺฉูุฏ. ุงู ฺฉุงุฑ ุจุงุนุซ ุตุฑููโุฌู ุฏุฑ ุฒูุงู ูโุดูุฏ. ุจูููโุณุงุฒ ุดูุง ุฏุฑ ุงูุฌุง ฺฉุงููุงู ุฏุฑุณุช ุงุณุช.

ฒ. ุงุฌุฑุง ุฑู GPU (ูุงููุฏ CUDA)

ุฑู GPUุ ุดูุง ูุฒุงุฑุงู ุชุฑุฏ ุฏุงุฑุฏ ฺฉู ููุฒูุงู ุงุฌุฑุง ูโุดููุฏ. ุงู ุชุฑุฏูุง ุฏุฑ ฺฏุฑููโูุง ณฒ ุชุง ุจู ูุงู "warp" ุณุงุฒูุงูุฏู ูโุดููุฏ.

ูุงููู ุทูุง CUDA: ุชูุงู ณฒ ุชุฑุฏ ุฏุฑ ฺฉ warp ุจุงุฏ ุฏุฑ ูุฑ ูุญุธู ฺฉ ุฏุณุชูุฑุงูุนูู ูุงุญุฏ ุฑุง ุงุฌุฑุง ฺฉููุฏ.

ุญุงูุง ุจุงุฏ ุณูุงุฑู ุดูุง ุฑุง ุจุฑุฑุณ ฺฉูู:

    ุงฺฏุฑ ุงุฒ ฺฉุฏ ูพุงุชูู ุดูุง ุงุณุชูุงุฏู ฺฉูู:

        ูุฑุถ ฺฉูุฏ ฺฉ warp ุดุงูู ุชุฑุฏ ุงุฒ ูุณุท ุชุตูุฑ ู ุชุฑุฏ ุงุฒ ูุจู ุจุงูุง ุชุตูุฑ ุงุณุช.

        pos_y = 0 (ุฑุฏู ุงูู ฺฉุฑูู).

        ุชุฑุฏ ูุณุท: inp_pos_y ูุนุชุจุฑ ุงุณุช. ุดุฑุท if (ุจุฑุง y) ุฑุง ุงุฌุฑุง ูโฺฉูุฏ ู false ูโุดูุฏ. ูโุฎูุงูุฏ ูุงุฑุฏ ุญููู x ุดูุฏ.

        ุชุฑุฏ ูุจู: inp_pos_y ูุงูุนุชุจุฑ ุงุณุช (-1). ุดุฑุท if (ุจุฑุง y) ุฑุง ุงุฌุฑุง ูโฺฉูุฏ ู true ูโุดูุฏ. ูโุฎูุงูุฏ continue ฺฉูุฏ.

        ูุดฺฉู (Branch Divergence): ุฏุฑ ุงูุฌุงุ warp "ุดูฺฉุงูุชู" ูโุดูุฏ. GPU ููโุชูุงูุฏ ููุฒูุงู ูู ูุงุฑุฏ ุญููู x ุดูุฏ ู ูู continue ฺฉูุฏ.

        ุฑุงูโุญู GPU (ฺฉู ฺฉูุฏ ุงุณุช): GPU ูุฌุจูุฑ ุงุณุช ูุฑ ุฏู ูุณุฑ ุฑุง ุจู ููุจุช ุงุฌุฑุง ฺฉูุฏ. ุงุจุชุฏุง ูุณุฑ "ุงุฏุงูู ุฏุงุฏู" (continue) ุฑุง ุจุฑุง ุชุฑุฏ ูุจู ุงุฌุฑุง ูโฺฉูุฏ (ุฏุฑ ุญุงู ฺฉู ุชุฑุฏ ูุณุท ุบุฑูุนุงู ุงุณุช). ุณูพุณุ ุจุฑูโฺฏุฑุฏุฏ ู ูุณุฑ "ุญููู ุฏุงุฎู" (inner loop) ุฑุง ุจุฑุง ุชุฑุฏ ูุณุท ุงุฌุฑุง ูโฺฉูุฏ (ุฏุฑ ุญุงู ฺฉู ุชุฑุฏ ูุจู ุบุฑูุนุงู ุงุณุช).

        ุงู ุงูุดุนุงุจ ู ุบุฑูุนุงู ฺฉุฑุฏู ููุจุช ุชุฑุฏูุงุ ุนููฺฉุฑุฏ ุฑุง ุจู ุดุฏุช ฺฉุงูุด ูโุฏูุฏ.

    ุฑุงูโุญู (ฺฉุฏ CUDA ูู):

        ุฏุฑ ฺฉุฏ ููุ ูฺ ุงูุดุนุงุจ ุฏุฑ ุญูููโูุง ูุฌูุฏ ูุฏุงุฑุฏ.

        ุชูุงู ุชุฑุฏูุง ุฏุฑ warp (ูู ูุจู ู ูู ูุณุท) ูุฑ ุฏู ุญููู y ู x ุฑุง ุงุฌุฑุง ูโฺฉููุฏ (ฺฉู ฺูู ุญูููโูุง ฺฉูฺฺฉ ูุณุชูุฏุ ฺฉุงููพุงูุฑ ุขูโูุง ุฑุง ุจุงุฒ (unroll) ูโฺฉูุฏ ู ุจุณุงุฑ ุณุฑุน ูุณุชูุฏ).

        ุชูุงู ุชุฑุฏูุง ุดุฑุท ุงุฏุบุงูโุดุฏูโ if (inp_pos_y >= 0 && ...) ุฑุง ุงุฌุฑุง ูโฺฉููุฏ.

        ุชุฑุฏ ูุณุท: ุดุฑุท true ูโุดูุฏ ู ูุญุงุณุจุงุช (acc += ...) ุฑุง ุงูุฌุงู ูโุฏูุฏ.

        ุชุฑุฏ ูุจู: ุดุฑุท false ูโุดูุฏ ู ุจู ุณุงุฏฺฏ ุงุฒ ุฑู ุฏุณุชูุฑ ูุญุงุณุจุงุช ูโูพุฑุฏ.

        ุงู ฺฉุงุฑ ุจุณุงุฑ ุณุฑุนโุชุฑ ุงุณุชุ ุฒุฑุง ุงฺฏุฑฺู ุชุฑุฏ ูุจู น ุจุงุฑ ุดุฑุท ุฑุง ฺฺฉ ูโฺฉูุฏุ ุงูุง ูุฑฺฏุฒ ุจุงุนุซ ุงุฌุงุฏ ุงูุดุนุงุจ ุดฺฉู (divergence) ุฏุฑ ฺฉูุชุฑู ุฌุฑุงู (control-flow) ุญููู ููโุดูุฏ.

ุฎูุงุตู:

ุฏุฑ CUDAุ ุงุบูุจ ุจูุชุฑ ุงุณุช ฺฉู ฺูุฏ ูุญุงุณุจูโ if ุณุงุฏู ู ุงุถุงู ุฑุง ุชูุณุท ูููโ ุชุฑุฏูุง ุงูุฌุงู ุฏูุฏ (ูุงููุฏ ฺฉุฏ ูู) ุชุง ุงูฺฉู ุจุง ฺฉ continue ุง break ุฒูุฏุฑุณุ ุจุงุนุซ ุงูุดุนุงุจ ุดฺฉู (Branch Divergence) ุจู ุชุฑุฏูุง ุดูุฏ (ูุงููุฏ ฺฉุฏ ูพุงุชูู).

ุจูุงุจุฑุงูุ ุงุฏุบุงู ุดุฑุทโูุง ุชฺฉุฑุงุฑ ุญููู ุงุถุงู ุงุฌุงุฏ ููโฺฉูุฏ (ฺูู ุญูููโูุง ุซุงุจุช 3x3 ูุณุชูุฏ)ุ ุจูฺฉู ุงุฒ ุงูุดุนุงุจ ุฏุฑ ุจู ุชุฑุฏูุง ุฌููฺฏุฑ ูโฺฉูุฏ ฺฉู ุฏุฑ GPU ุจุณุงุฑ ูููโุชุฑ ุงุณุช.

ุงู ฺฉู ุชู ุญููู ูุชุบุฑ ุฑู ุชุนุฑู ฺฉุฑุฏู ูุดฺฉู ุงุฌุงุฏ ูู ฺฉูู ุง ุณุฑุนุช ุฑู ฺฉู ูู ฺฉููุ

ููุ ูู ุชููุง ูุดฺฉู ุงุฌุงุฏ ููโฺฉูุฏุ ุจูฺฉู ุงู ุฑูุด ุฏุฑ C++ ู CUDA ุจูููโุชุฑ ู ฺฉุงููุงู ุงุณุชุงูุฏุงุฑุฏ ุงุณุช.

ุงู ฺฉ ุงุฒ ุชุตูุฑุงุช ุบูุท ุฑุงุฌ ุงุณุช ฺฉู ุงุฒ ุฒุจุงูโูุง ุจุฑูุงููโููุณ ุจุณุงุฑ ูุฏูโุชุฑ (ูุงููุฏ C89) ุจุงู ูุงูุฏู ุงุณุช.

ุฏูู ุงูฺฉู ุงู ฺฉุงุฑ ุจููู ุงุณุช:

ฑ. ฺฉุงููพุงูุฑ ุจุณุงุฑ ููุดููุฏ ุงุณุช: ฺฉุงููพุงูุฑ (ฺู g++ ุจุฑุง C++ ู ฺู nvcc ุจุฑุง CUDA) ูโุฏุงูุฏ ฺฉู const int inp_pos_x = ... ฺฉ ุชุฎุตุต ุญุงูุธู (memory allocation) ูุงูุน ูุณุช.

ฒ. ุงุณุชูุงุฏู ุงุฒ ุฑูุฌุณุชูุฑ (Register): ุงู ูุชุบุฑูุง (ูุงููุฏ inp_pos_y, inp_pos_x, w_idx, x_idx) ุขููุฏุฑ ุณุงุฏู ู ฺฉูฺฺฉ ูุณุชูุฏ ฺฉู ุงุตูุงู ุฑู ููููุฑ (RAM ุง VRAM) ูุฑุงุฑ ููโฺฏุฑูุฏ. ฺฉุงููพุงูุฑ ุขูโูุง ุฑุง ูุณุชููุงู ุฏุฑ ุฑูุฌุณุชูุฑูุง (ุณุฑุนโุชุฑู ูุงุญุฏ ุญุงูุธู ฺฉู ูุณุชููุงู ุฏุงุฎู ุฎูุฏ ูุณุชูโ GPU/CPU ุงุณุช) ูุฑุงุฑ ูโุฏูุฏ.

ณ. ูุฒููโ ุตูุฑ (Zero-Cost Abstraction): ุฏุณุชูุฑ const int inp_pos_x = ... ุจู ูุนูุง ูุงูุน ฺฉููู ุจู ฺฉ ุง ุฏู ุฏุณุชูุฑุงูุนูู ูุงุดู (GPU/CPU assembly) ุชุจุฏู ูโุดูุฏ (ูุซูุงู: "ููุฏุงุฑ i_w ุฑุง ุจุง (pos_x - 1) ุฌูุน ฺฉู ู ุฏุฑ ุฑุฌุณุชุฑ R5 ูุฑุงุฑ ุจุฏู"). ุงู ฺฉุงุฑ ุฏููุงู ุจู ููุงู ุณุฑุนุช (ู ุง ุญุช ุณุฑุนโุชุฑ ุงุฒ) ุชุนุฑู ูุชุบุฑ ุฏุฑ ุฎุงุฑุฌ ุงุฒ ุญููู ู ููุฏุงุฑุฏู ุจู ุขู ุฏุฑ ุฏุงุฎู ุญููู ุงุณุช.

ด. ฺฉูฺฉ ุจู ุจูููโุณุงุฒ (Optimization): ููุช ูุชุบุฑ ุฑุง ุฏุฑ ุฏุงุฎู ุญููู ุง if ุชุนุฑู ูโฺฉูุฏุ ุจู ฺฉุงููพุงูุฑ ูโฺฏูุฏ: "ูู ููุท ุฏุฑ ุงู ุจููฺฉ (scope) ุจู ุงู ูุชุบุฑ ูุงุฒ ุฏุงุฑู." ฺฉุงููพุงูุฑ ูโูููุฏ ฺฉู ุจู ูุญุถ ุชูุงู ุดุฏู ุงู ุจููฺฉุ ูโุชูุงูุฏ ุขู ุฑุฌุณุชุฑ (R5) ุฑุง ุจุฑุง ฺฉุงุฑ ุฏฺฏุฑ ุขุฒุงุฏ ฺฉูุฏ. ุงู ฺฉุงุฑ ูุฏุฑุช ุฑุฌุณุชุฑูุง ุฑุง (ฺฉู ููุจุน ุจุณุงุฑ ฺฏุฑุงูโุจูุง ุฏุฑ GPU ูุณุชูุฏ) ุจุฑุง ฺฉุงููพุงูุฑ ุจุณุงุฑ ุขุณุงูโุชุฑ ูโฺฉูุฏ ู ุฏุฑ ฺฉู ููุฌุฑ ุจู ฺฉุฏ ุณุฑุนโุชุฑ ูโุดูุฏ.

ูุชุฌูโฺฏุฑ: ฺฉุงุฑ ฺฉู ุดูุง ุงูุฌุงู ุฏุงุฏูโุงุฏ (ุชุนุฑู ูุชุบุฑูุง ุฏุฑ ุฏุงุฎูโุชุฑู ูุญุฏูุฏูโ ููฺฉู) ูู ุชููุง ุณุฑุนุช ุฑุง ฺฉู ููโฺฉูุฏุ ุจูฺฉู ุฎูุงูุง ฺฉุฏ ุฑุง ุจุงูุง ุจุฑุฏู ู ุจู ฺฉุงููพุงูุฑ ฺฉูฺฉ ูโฺฉูุฏ ุชุง ฺฉุฏ ุจูููโุชุฑ ุชููุฏ ฺฉูุฏ. ุงู ุฑูุด ฺฉุงููุงู ุตุญุญ ู ุญุฑููโุง ุงุณุช.

ูุดฺฉู ฺูุ

/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation.

If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].

warnings.warn(

---------------------------------------------------------------------------IndexError Traceback (most recent call last)/tmp/ipykernel_48/625774077.py in <cell line: 0>() 94 ''' 95 ---> 96 module = load_inline( 97 name="conv_relu_fused_ext", 98 cpp_sources=[cpp_src],/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in load_inline(name, cpp_sources, cuda_sources, functions, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, with_pytorch_error_handling, keep_intermediates, use_pch) 1721 sources.append(cuda_source_path) 1722 -> 1723 return _jit_compile( 1724 name, 1725 sources,/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in _jit_compile(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates) 1796 sources = list(hipified_sources) 1797 -> 1798 _write_ninja_file_and_build_library( 1799 name=name, 1800 sources=sources,/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in _write_ninja_file_and_build_library(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone) 1911 # NOTE: Emitting a new ninja build file does not cause re-compilation if 1912 # the sources did not change, so it's ok to re-emit (and it's fast).-> 1913 _write_ninja_file_to_build_library( 1914 path=build_file_path, 1915 name=name,/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in _write_ninja_file_to_build_library(path, name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, with_cuda, is_standalone) 2310 cuda_flags += _get_rocm_arch_flags(cuda_flags) 2311 elif with_cuda:-> 2312 cuda_flags = common_cflags + COMMON_NVCC_FLAGS + _get_cuda_arch_flags() 2313 if IS_WINDOWS: 2314 for flag in COMMON_MSVC_FLAGS:/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py in _get_cuda_arch_flags(cflags) 2077 arch_list.append(arch) 2078 arch_list = sorted(arch_list)-> 2079 arch_list[-1] += '+PTX' 2080 else: 2081 # Deal with lists that are ' ' separated (only deal with ';' after)IndexError: list index out of range

ุงู ฺฉ ูุดฺฉู ูุญุท (Environment Error) ุงุณุชุ ูู ูุดฺฉู ููุทู ุฏุฑ ฺฉุฏ C++ ุง ูพุงุชูู ุดูุง.

ุฎูุงุตู ูุดฺฉู:

ฺฉุฏ load_inline (ฺฉู ูุณุฆูู ฺฉุงููพุงู ฺฉุฑุฏู ฺฉุฏ C++/CUDA ุดูุงุณุช) ูุชูุงูุณุชู ูฺ ฺฉุงุฑุช ฺฏุฑุงูฺฉ (GPU) ุณุงุฒฺฏุงุฑ ุจุง CUDA ุฑุง ุฏุฑ ูุญุท ุดูุง ูพุฏุง ฺฉูุฏ.

ุชูุถุญ ุฏูู ุฎุทุง:

    ููุช load_inline ุฑุง ุตุฏุง ูโุฒูุฏุ ูพุงุชูุฑฺ ุชูุงุด ูโฺฉูุฏ ฺฉุฏ CUDA (.cu) ุดูุง ุฑุง ฺฉุงููพุงู ฺฉูุฏ.

    ุจุฑุง ฺฉุงููพุงู CUDAุ ูพุงุชูุฑฺ ุจุงุฏ ุจุฏุงูุฏ ฺฉู ฺฉุฏ ุฑุง ุจุฑุง ฺู "ูุนูุงุฑ" (Architecture) ุงุฒ GPU ฺฉุงููพุงู ฺฉูุฏ. (ูุซูุงู sm_75 ุจุฑุง T4 ุง sm_86 ุจุฑุง A100).

    ูพุงุชูุฑฺ ุงู ูุณุช ูุนูุงุฑโูุง (arch_list) ุฑุง ุจุง ุจุฑุฑุณ GPUูุง ูุนุงู ุฏุฑ ูุญุท ุดูุง ุจู ุฏุณุช ูโุขูุฑุฏ.

    ุฏุฑ ูุญุท ุดูุงุ ูฺ GPU ูุนุงู ูพุฏุง ูฺฉุฑุฏู ุงุณุช. ุฏุฑ ูุชุฌูุ arch_list ฺฉ ูุณุช ุฎุงู ([]) ุดุฏู ุงุณุช.

    ฺฉุฏ ูพุงุชูุฑฺ (ุฏุฑ ุฎุท _get_cuda_arch_flags) ุชูุงุด ูโฺฉูุฏ ุจู ุขุฎุฑู ุขุชู ูุณุช (arch_list[-1]) ุฏุณุชุฑุณ ูพุฏุง ฺฉูุฏ ุชุง +PTX ุฑุง ุจู ุขู ุงุถุงูู ฺฉูุฏ.

    ฺูู ูุณุช ุฎุงู ุงุณุชุ ุฏุณุชุฑุณ ุจู arch_list[-1] ุฎุทุง IndexError: list index out of range ุฑุง ุงุฌุงุฏ ูโฺฉูุฏ.

ุฑุงูโุญู:

ุดูุง ุจุงุฏ ุงู ฺฉุฏ ุฑุง ุฏุฑ ฺฉ ูุญุท ุงุฌุฑุง ฺฉูุฏ ฺฉู GPU ูุนุงู ุฏุงุดุชู ุจุงุดุฏ.

ุงฺฏุฑ ุฏุฑ Google Colab ูุณุชุฏ:

    ุจู ููู "Runtime" (ุฏุฑ ูุณุฎูโูุง ูุงุฑุณ ยซุฒูุงู ุงุฌุฑุงยป) ุจุฑูุฏ.

    ฺฏุฒููโ "Change runtime type" (ยซุชุบุฑ ููุน ุฒูุงู ุงุฌุฑุงยป) ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ.

    ุฏุฑ ุจุฎุด "Hardware accelerator" (ยซุดุชุงุจโุฏููุฏู ุณุฎุชโุงูุฒุงุฑยป)ุ None ุฑุง ุจู GPU (ูุซูุงู T4) ุชุบุฑ ุฏูุฏ.

    ุณูพุณ "Save" (ยซุฐุฎุฑูยป) ฺฉูุฏ ู ููุชโุจูฺฉ ุฑุง ุงุฒ ุงุจุชุฏุง ุงุฌุฑุง ฺฉูุฏ (ุจุงุฏ torch ู... ุฏูุจุงุฑู ูุตุจ ุดููุฏ).

ฺุฑุง ุฏูุช ูุง ูุฏู ูุง ุงูุทูุฑ ูุณุชุ

ุฏูู ุงูฺฉู ุฏูุช (Accuracy) ุชูุงู ูุฏูโูุง ุฑู ฑฑ.ณตูช ููู ุดุฏู ู ุฎุทุง (Loss) ฺฉุงูุด ูพุฏุง ููโฺฉูุฏุ ุงุฑุงุฏ ุฏุฑ ฺฉุฏ CUDA ุง ููุทู backpropagation ุดูุง ูุณุช (ฺูู ูุฏู ุงุณุชุงูุฏุงุฑุฏ PyTorch ูู ููู ุฑูุชุงุฑ ุฑุง ุฏุงุฑุฏ).

ูุดฺฉู ุงุตู ุฏุฑ ูุนูุงุฑ ูุฏู (Model Architecture) ุชุนุฑู ุดุฏู ุฏุฑ ฺฉูุงุณโูุง CNNBaseline ู CNNFused ุงุณุช.

ฑ. ุงุดุชุฑุงฺฉ ูุฒูโูุง (Weight Sharing) ุฏุฑ ุญููู

ุฏุฑ ูุชุฏ __init__ุ ุดูุง ุชููุง ฺฉ ูุงู ฺฉุงููููุดู ุชุนุฑู ฺฉุฑุฏูโุงุฏ:
Python

self.conv = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)

ุงูุง ุฏุฑ forwardุ ููุงู ฺฉ ูุงู ุฑุง ุฏุฑ ฺฉ ุญููู ุชฺฉุฑุงุฑ ูโฺฉูุฏ:
Python

for _ in range(self.num_convs):
    x = F.relu(self.conv(x)) # <--- ูุดฺฉู ุงูุฌุงุณุช

ุงู ฺฉุงุฑ ุจุงุนุซ ูโุดูุฏ ฺฉู ุฏุฑ ุชูุงู ูุฑุงุญู (ูุซูุงู ต ูุฑุญูู)ุ ุฏููุงู ุงุฒ ฺฉ ูุฌููุนู ูุฒู ู ุจุงุงุณ ฺฉุณุงู ุงุณุชูุงุฏู ุดูุฏ (ูุซู ฺฉ ุดุจฺฉู RNN). ุงู ูุญุฏูุฏุช ุดุฏุฏ ุจุงุนุซ ูโุดูุฏ ุดุจฺฉู ูุชูุงูุฏ ูฺฺฏโูุง ุณูุณููโูุฑุงุชุจ (Hierarchical features) ุฑุง ุงุฏ ุจฺฏุฑุฏ. ุดุจฺฉู ูุฌุจูุฑ ุงุณุช ฺฉ ููุชุฑ ณรณ ูพุฏุง ฺฉูุฏ ฺฉู ุงฺฏุฑ ต ุจุงุฑ ูพุดุช ุณุฑ ูู ุฑู ุชุตูุฑ ุงุนูุงู ุดูุฏุ ุขู ุฑุง ุจู ุนุฏุฏ ุตุญุญ ุชุจุฏู ฺฉูุฏ! ุงู ุชูุฑุจุงู ุบุฑููฺฉู ุงุณุช.

ฒ. ูุญุฏูุฏุช ฺฉุงูุงู (Channel Bottleneck)

ุดูุง C_out=1 ูุฑุงุฑ ุฏุงุฏูโุงุฏ. ุดุจฺฉูโูุง ฺฉุงููููุดู ูุฏุฑุช ุฎูุฏ ุฑุง ุงุฒ ุงูุฒุงุด ุชุนุฏุงุฏ ฺฉุงูุงูโูุง (ูุซูุงู ฑ -> ณฒ -> ถด) ูโฺฏุฑูุฏ ุชุง ูฺฺฏโูุง ูุฎุชูู (ูุจูโูุงุ ุจุงูุชโูุง ู ...) ุฑุง ุงุณุชุฎุฑุงุฌ ฺฉููุฏ. ุฒูุงู ฺฉู C_out=1 ุงุณุชุ ุชูุงู ุงุทูุงุนุงุช ุชุตูุฑ ุจุงุฏ ุชููุง ุฏุฑ ฺฉ ฺฉุงูุงู ูุดุฑุฏู ุดูุฏ ฺฉู ุธุฑูุช ุงุฏฺฏุฑ ูุฏู ุฑุง ุจู ุดุฏุช ฺฉุงูุด ูโุฏูุฏ.

ูฺฉุชู ููู: ูโุฏุงูู ฺฉู ฺฉุฑูู CUDA ุดูุง ุจู ฺฏูููโุง ุทุฑุงุญ ุดุฏู ฺฉู ููุท C_out=1 ุฑุง ูพุดุชุจุงู ูโฺฉูุฏ (ุทุจู ุตูุฑุช ุณูุงู)ุ ุงูุง ุงู ูุญุฏูุฏุช ุจุงุนุซ ูโุดูุฏ ูุฏู ูุชูุงูุฏ ุฑู ุฏุชุงุณุช ูพฺุฏูโุง ูุซู MNIST ุงุฏฺฏุฑ ููุงุณุจ ุฏุงุดุชู ุจุงุดุฏ.

ูุชุฌูโฺฏุฑ

    ุฑูุชุงุฑ ูุฏู ุทุจุน ุงุณุช: ฺูู ูุฏู ุธุฑูุช ุงุฏฺฏุฑ ูุฏุงุฑุฏุ ููุท ุญุฏุณ ุชุตุงุฏู ูโุฒูุฏ (ุจุฑุง ฑฐ ฺฉูุงุณ ุญุฏูุฏ ฑฐูช + ฺฉู ุจุงุงุณ ุฏุชุงุณูุช = ฑฑ.ณตูช).

    ฺฉุฏ CUDA ุดูุง ุฏุฑุณุช ุงุณุช: ฺูู ุฎุฑูุฌ ูุฏู Fused ุฏููุงู ุจุง Baseline ฺฉ ุงุณุชุ ุนู ฺฉุฑูู ุดูุง ูุญุงุณุจุงุช ุฑุง ุฏุฑุณุช ุงูุฌุงู ูโุฏูุฏ. ูุฏู ุงู ุชูุฑู ุงุญุชูุงูุงู ููุงุณู ุณุฑุนุช ุจูุฏู ุงุณุชุ ูู ุฑุณุฏู ุจู ุฏูุช ุจุงูุง.

ุฑุงู ุญู (ุงฺฏุฑ ูุฏู ุงูุฒุงุด ุฏูุช ุจูุฏ): ุจุงุฏ ุจู ุฌุง ฺฉ self.convุ ุงุฒ nn.ModuleList ุงุณุชูุงุฏู ูโฺฉุฑุฏุฏ ุชุง ูุฒูโูุง ูุฑ ูุงู ุฌุฏุงฺฏุงูู ุจุงุดุฏ:
Python

# ุงู ฺฉุฏ ููุท ุจุฑุง ูุซุงู ุงุณุช ู ุจุง ฺฉุฑูู ุชฺฉ ฺฉุงูุงูู ุดูุง ููฺูุงู ุถุนู ุนูู ุฎูุงูุฏ ฺฉุฑุฏ
self.convs = nn.ModuleList([nn.Conv2d(1, 1, 3, 1) for _ in range(num_convs)])

def forward(self, x):
    for conv in self.convs:
        x = F.relu(conv(x))
    ...

    @staticmethod

    def backward(ctx, grad_output):

        x, weight, bias, y = ctx.saved_tensors


        # grad ReLU

        mask = (y > 0).to(grad_output.dtype)

        grad_z = grad_output * mask


        # dL/dx

        grad_x = torch.nn.grad.conv2d_input(

            x.shape, weight, grad_z, padding=1

        )

        # dL/dW

        grad_weight = torch.nn.grad.conv2d_weight(

            x, weight.shape, grad_z, padding=1

        )

        # dL/db

        grad_bias = grad_z.sum(dim=[0, 2, 3])


        return grad_x, grad_weight, grad_bias



ุงูู ฺฉุงูู ู ุจุง ุฌุฒุงุช ุฑุงุถ ุชูุถุญ ุจุฏู ฺฉุงูู

ุงู ูุชุฏ backward ูุณุฆูู ูุญุงุณุจู ฺฏุฑุงุฏุงูโูุง ุจุฑุง ุนููุงุช ุชุฑฺฉุจ Convolution + ReLU ุงุณุช. ูุฏู ููุง ุฏุฑ ุงูุฌุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง (Chain Rule) ุจุฑุง ูุญุงุณุจู ูุดุชู ุชุงุจุน ูุฒูู (Loss) ูุณุจุช ุจู ูุฑูุฏโูุง (x)ุ ูุฒูโูุง (W) ู ุจุงุงุณ (b) ุงุณุช.

ุจุงุฏ ูุฑุถ ฺฉูู L ุชุงุจุน ูุฒูู ููุง ุจุงุดุฏ. ูุฑูุฏ ุชุงุจุน backwardุ ูุชุบุฑ grad_output ุงุณุช ฺฉู ููุงู ูุดุชู ูุฒูู ูุณุจุช ุจู ุฎุฑูุฌ ูุงู ุงุณุช:
grad_output=โyโLโ

ุฏุฑ ุงุฏุงููุ ุฎุท ุจู ุฎุท ููุฑุงู ุจุง ุฑุงุถุงุช ูพุดุช ุขู ุชูุถุญ ูโุฏูู:

ฑ. ุจุงุฒุงุจ ูุชุบุฑูุง

Python

x, weight, bias, y = ctx.saved_tensors

ุงู ุชูุณูุฑูุง ุฏุฑ ูุฑุญูู forward ุฐุฎุฑู ุดุฏู ุจูุฏูุฏ. ุจุฑุง ูุญุงุณุจู ูุดุชูุงุชุ ูุง ุจู ูุฑูุฏ ุงุตู (x)ุ ูุฒูโูุง (W) ู ุฎุฑูุฌ ูุนุงูโุณุงุฒ ุดุฏู (y) ูุงุฒ ุฏุงุฑู.

ฒ. ูุญุงุณุจู ฺฏุฑุงุฏุงู ReLU (ุจุฎุด grad_z)

Python

mask = (y > 0).to(grad_output.dtype)
grad_z = grad_output * mask

ุฑุงุถุงุช: ุนููุงุช Forward ุจู ุตูุฑุช ุฒุฑ ุจูุฏ:
z=Conv(x,W)+b
y=ReLU(z)=max(0,z)

ูุง โyโLโ ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โzโLโ ุฑุง ูุญุงุณุจู ฺฉูู (ูุดุชู ูุจู ุงุฒ ุงุนูุงู ReLU). ุทุจู ูุงุนุฏู ุฒูุฌุฑูโุง:
โzโLโ=โyโLโโโzโyโ

ูุดุชู ุชุงุจุน ReLU ุจู ุตูุฑุช ุฒุฑ ุงุณุช:
โzโyโ={10โif z>0if zโค0โ

ุฏุฑ ฺฉุฏ:

    mask ููุงู โzโyโ ุงุณุช. ุฌุง ฺฉู ุฎุฑูุฌ ูุซุจุช ุจูุฏูุ ฑ ู ุฏุฑ ุบุฑ ุงู ุตูุฑุช ฐ ูโุดูุฏ.

    grad_z ููุงู โzโLโ ุงุณุช ฺฉู ุญุงุตู ุถุฑุจ ููุทูโุง (element-wise) ฺฏุฑุงุฏุงู ุฎุฑูุฌ ุฏุฑ ูุงุณฺฉ ุงุณุช.

ณ. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฑูุฏ x (ุจุฎุด dL/dx)

Python

grad_x = torch.nn.grad.conv2d_input(
    x.shape, weight, grad_z, padding=1
)

ุฑุงุถุงุช: ูุง โzโLโ ุฑุง ุฏุงุฑู (ููุงู grad_z) ู ูโุฎูุงูู โxโLโ ุฑุง ุจุงุจู. ุฑุงุจุทู ุจู z ู x ฺฉ ฺฉุงููููุดู ุงุณุช: z=xโW.

ุฏุฑ ุฑุงุถุงุช ฺฉุงููููุดูุ ูุดุชู ูุณุจุช ุจู ูุฑูุฏ (grad_x) ุจุฑุงุจุฑ ุงุณุช ุจุง ฺฉุงููููุดู ฺฏุฑุงุฏุงู ุฎุฑูุฌ ุจุง ูุฒูโูุง "ุจุฑุนฺฉุณ ุดุฏู" (Rotated Weights). ุงู ุนููุงุช ุงุบูุจ ุฏุฑ ูพุงุฏูโุณุงุฒโูุง ุจู ุนููุงู Transposed Convolution ุง Deconvolution ุดูุงุฎุชู ูโุดูุฏ.
โxโLโ=grad_zโWrot180

ุชุงุจุน torch.nn.grad.conv2d_input ุฏููุงู ููู ุนููุงุช ุฑุงุถ ูพฺุฏู ุฑุง ุจู ุตูุฑุช ุจููู ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ. ุงู ุชุงุจุน ูุญุงุณุจู ูโฺฉูุฏ ฺฉู ุงฺฏุฑ ุฎุฑูุฌ grad_z ุจุงุดุฏ ู ูุฒูโูุง weight ุจุงุดูุฏุ ูุฑูุฏ x ุจุงุฏ ฺู ฺฏุฑุงุฏุงู ุฏุงุดุชู ุจุงุดุฏ ุชุง ุงู ุชุบุฑุงุช ุงุฌุงุฏ ุดูุฏ.

ด. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฒูโูุง W (ุจุฎุด dL/dW)

Python

grad_weight = torch.nn.grad.conv2d_weight(
    x, weight.shape, grad_z, padding=1
)

ุฑุงุถุงุช: ูุง โzโLโ ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โWโLโ ุฑุง ุจุงุจู. ุจุงุฒ ูู ุฑุงุจุทู z=xโW ุงุณุช.

ุฏุฑ ุฑุงุถุงุชุ ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฒูโูุง ุจุฑุงุจุฑ ุงุณุช ุจุง ฺฉุงููููุดู (ุง Cross-correlation) ุจู "ูุฑูุฏ x" ู "ฺฏุฑุงุฏุงู ุฎุฑูุฌ z". ุจู ุนุจุงุฑุช ุฏฺฏุฑุ ูุง ูโุฎูุงูู ุจุจูู ฺฉุฏุงู ุจุฎุดโูุง ุชุตูุฑ ูุฑูุฏ (x) ุฏุฑ ุงุฌุงุฏ ุฎุทุง ููุฌูุฏ ุฏุฑ grad_z ููุด ุฏุงุดุชูโุงูุฏ.
โWโLโ=xโgrad_z

ุชุงุจุน torch.nn.grad.conv2d_weight ุงู ูุญุงุณุจู ุฑุง ุงูุฌุงู ูโุฏูุฏ. ูุฑูุฏ ุงุตู (x) ู ฺฏุฑุงุฏุงู ุฎุฑูุฌ (grad_z) ุฑุง ูโฺฏุฑุฏ ู ูุดุฎุต ูโฺฉูุฏ ฺฉู ูุฑ ูุฒู ุฏุฑ ฺฉุฑูู ฺูุฏุฑ ุจุงุฏ ุชุบุฑ ฺฉูุฏ.

ต. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ุจุงุงุณ b (ุจุฎุด dL/db)

Python

grad_bias = grad_z.sum(dim=[0, 2, 3])

ุฑุงุถุงุช: ุฏุฑ ุนููุงุช ฺฉุงููููุดูุ ฺฉ ููุฏุงุฑ ุจุงุงุณ (Scalar) ุจู ุชูุงู ูพฺฉุณูโูุง ฺฉ Feature Map (ฺฉุงูุงู ุฎุฑูุฌ) ุงุถุงูู ูโุดูุฏ. ูุฑููู Forward ุจุฑุง ฺฉ ูพฺฉุณู ุฎุงุต:
zn,c,h,wโ=(ConvOutput)n,c,h,wโ+bcโ

ฺูู bcโ (ุจุงุงุณ ฺฉุงูุงู c) ุฏุฑ ูุญุงุณุจู ุชูุงู ูพฺฉุณูโูุง ุขู ฺฉุงูุงู (ุฏุฑ ุชูุงู ุนุฑุถ Wุ ุงุฑุชูุงุน H ู ุชูุงู ูููููโูุง ุฏุณุชู N) ุดุฑฺฉุช ุฏุงุดุชู ุงุณุชุ ุทุจู ูุงุนุฏู ูุดุชูโฺฏุฑ ุฌูุนุ ฺฏุฑุงุฏุงู ุขู ุจุฑุงุจุฑ ุงุณุช ุจุง ูุฌููุน ฺฏุฑุงุฏุงูโูุง ุชูุงู ูพฺฉุณูโูุง ฺฉู ุขู ุจุงุงุณ ุฑุง ุฏุฑุงูุช ฺฉุฑุฏูโุงูุฏ.
โbcโโLโ=n=0โNโ1โh=0โHโ1โw=0โWโ1โโzn,c,h,wโโLโ

ุฏุฑ ฺฉุฏ:

    dim=[0, 2, 3] ุงุดุงุฑู ุจู ุงุจุนุงุฏ N (Batch), H (Height), W (Width) ุฏุงุฑุฏ.

    ูุง ุฑู ุงู ุงุจุนุงุฏ ุฌูุน ูโุฒูู ู ุชููุง ุจูุนุฏ ุจุงูโูุงูุฏูุ ุจูุนุฏ ฺฉุงูุงู (Channel) ุง ููุงู ุฏูุงูุณูู ฑ ุงุณุช ฺฉู ููโุงูุฏุงุฒู ุจุฑุฏุงุฑ ุจุงุงุณ ูโุจุงุดุฏ.

ุฎูุงุตู

ุงู ฺฉุฏ ูุฑุขูุฏ Backward Pass ุฑุง ุจุง ุดฺฉุณุชู ุนููุงุช ุจู ุณู ุจุฎุด ุงุตู ุงูุฌุงู ูโุฏูุฏ:

    ุนุจูุฑ ฺฏุฑุงุฏุงู ุงุฒ ReLU (ุจุง ุตูุฑ ฺฉุฑุฏู ฺฏุฑุงุฏุงูู ููุงุฏุฑ ููู).

    ุงูุชุดุงุฑ ฺฏุฑุงุฏุงู ุจู ูุฑูุฏ (ุจุฑุง ูุงูโูุง ูุจู).

    ูุญุงุณุจู ฺฏุฑุงุฏุงู ุจุฑุง ูพุงุฑุงูุชุฑูุง ูุงู (ูุฒู ู ุจุงุงุณ) ุฌูุช ุขูพุฏุช ุดุฏู ุชูุณุท Optimizer.

ุงูู ุจุดุชุฑ ุชูุถุญ ุจุฏู

mask = (y > 0).to(grad_output.dtype) 

ุงู ุฎุท ฺฉุฏ ฺฉ ุงุฒ ูููโุชุฑู ู ุงุณุงุณโุชุฑู ูุฑุงุญู ุฏุฑ ูุฑุงูุฏ ูพุณโุงูุชุดุงุฑ ฺฏุฑุงุฏุงู (Backpropagation) ุฏุฑ ุดุจฺฉูโูุง ุนุตุจ ุงุณุช. ุงู ุฎุท ุฏููุงู ูุดุชู ุชุงุจุน ูุนุงูโุณุงุฒ ReLU (max(0,z)) ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ.
Python

mask = (y > 0).to(grad_output.dtype)

ุจุงุฏ ุงู ุนุจุงุฑุช ุฑุง ุจู ุฏู ุจุฎุด ุชูุณู ู ุจุฑุฑุณ ฺฉูู:

ฑ. ุจุฎุด ุจูู (Boolean) ู ููุทู ุฑุงุถ: (y > 0)

ุงู ุจุฎุด ูุณุชููุงู ูุดุชู ุชุงุจุน ReLU ุฑุง ุฏุฑ ูุงูุจ ฺฉ ูุงุณฺฉ ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ.

ุชุงุจุน ReLU ู ูุดุชู ุขู:

ุนููุงุช Forward ุฏุฑ ูุงู ูุนุงูโุณุงุฒ ReLU ุจู ุตูุฑุช ุฒุฑ ุงุณุช:
y=ReLU(z)=max(0,z)

ฺฉู ุฏุฑ ุขู z ูุฑูุฏ ReLU (ููุงู ุฎุฑูุฌ ุนููุงุช Conv + Bias) ู y ุฎุฑูุฌ ุขู ุงุณุช.

ูุดุชู ุชุงุจุน y ูุณุจุช ุจู ูุฑูุฏ z (ุนู โzโyโ) ุจู ุงู ุตูุฑุช ุชุนุฑู ูโุดูุฏ:
โzโyโ={10โif z>0if zโค0โ

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    y: ููุงู ุฎุฑูุฌ ReLU ุงุณุช ฺฉู ุฏุฑ ูุฑุญูู forward ูุญุงุณุจู ุดุฏู ู ุฏุฑ ctx.saved_tensors ุฐุฎุฑู ุดุฏู ุงุณุช.

    (y > 0): ุงู ฺฉ ุนููฺฏุฑ ููุงุณูโุง ุนูุตุฑุจูโุนูุตุฑ (Element-wise) ุงุณุช ฺฉู ฺฉ ุชูุณูุฑ ุฌุฏุฏ (ุจู ูุงู ูุงุณฺฉ) ุจู ูุง ูโุฏูุฏ.

        ุงฺฏุฑ ููุฏุงุฑ ูพฺฉุณู ุฏุฑ y ูุซุจุช ุจุงุดุฏ (y>0)ุ ููุฏุงุฑ ูุชูุงุธุฑ ุฏุฑ ูุงุณฺฉ True (ุตุญุญ) ุฎูุงูุฏ ุจูุฏ. (ุงู ุญุงูุช โzโyโ=1 ุฑุง ูุดุงู ูโุฏูุฏ.)

        ุงฺฏุฑ ููุฏุงุฑ ูพฺฉุณู ุฏุฑ y ุตูุฑ ุง ููู ุจุงุดุฏ (yโค0)ุ ููุฏุงุฑ ูุชูุงุธุฑ ุฏุฑ ูุงุณฺฉ False (ุบูุท) ุฎูุงูุฏ ุจูุฏ. (ุงู ุญุงูุช โzโyโ=0 ุฑุง ูุดุงู ูโุฏูุฏ.)

ุงู ุชูุณูุฑู ุจูู (True/False) ุนููุงู ููุงู ูุดุชู ุงุณุชุ ุงูุง ุจุงุฏ ุจู ููุน ุฏุงุฏูโุง ุนุฏุฏ ุชุจุฏู ุดูุฏ.

ฒ. ุจุฎุด ุชุจุฏู ููุน ุฏุงุฏู: .to(grad_output.dtype)

ุงู ุจุฎุด ูุณุฆููุช ุชุจุฏู ููุงุฏุฑ ุจูู (True/False) ุจู ุงุนุฏุงุฏ ููุฒ ุดูุงูุฑ (Float) ููุงุณุจ ุจุฑุง ูุญุงุณุจุงุช ฺฏุฑุงุฏุงู ุฑุง ุจุฑ ุนูุฏู ุฏุงุฑุฏ.

ฺุฑุง ุชุจุฏู ููุน ุฏุงุฏู ูุงุฒู ุงุณุชุ

    ูุญุงุณุจุงุช ุนุฏุฏ: ุชูุณูุฑูุง ุจูู ุจุฑุง ุถุฑุจ ู ุฌูุน ูุณุชูู ุฏุฑ ุนููุงุช ูุดุชูโฺฏุฑ ููุงุณุจ ูุณุชูุฏ. ูพุงุชูุฑฺ ููุงุฏุฑ True ุฑุง ุจู 1.0 ู False ุฑุง ุจู 0.0 ุชุจุฏู ูโฺฉูุฏ ุชุง ุจู ูุฏู ุฑุงุถ ุจุฑุณู.

    ฺฉูพุงุฑฺฺฏ ููุน ุฏุงุฏู: ุฏุฑ ูพุงุชูุฑฺุ ุชูุงู ฺฏุฑุงุฏุงูโูุง ฺฉุณุงู (ุฏุฑ ุงู ููุฑุฏุ grad_output) ุจุงุฏ ุงุฒ ฺฉ ููุน ุฏุงุฏู (ูุนูููุงู torch.float32 ุง torch.float64) ุจุงุดูุฏ.

        grad_output.dtype ููุน ุฏุงุฏู ฺฏุฑุงุฏุงู ูุฑูุฏ ุฑุง ูุดุฎุต ูโฺฉูุฏ.

        .to(...) ุชุถูู ูโฺฉูุฏ ฺฉู ุชูุณูุฑ mask ุฌุฏุฏ ูู ุฏููุงู ุงุฒ ููุงู ููุน ุฏุงุฏู ุจุงุดุฏ.

ูุชุฌู ููุง:

ูุชุบุฑ mask ุงฺฉููู ฺฉ ุชูุณูุฑ ุจุง ุงุจุนุงุฏ ฺฉุณุงู ุจุง y ู grad_output ุงุณุช ฺฉู ููุท ุดุงูู ุฏู ููุฏุงุฑ ุนุฏุฏ ุงุณุช:

    1.0: ุจุฑุง ูพฺฉุณูโูุง ฺฉู ุฏุฑ ูุฑุญูู Forward ูุนุงู ุจูุฏูุฏ ู ฺฏุฑุงุฏุงู ุจุงุฏ ุงุฒ ุขูโูุง ุนุจูุฑ ฺฉูุฏ.

    0.0: ุจุฑุง ูพฺฉุณูโูุง ฺฉู ุฏุฑ ReLU ุฎุงููุด ุดุฏูุฏ (ููุฏุงุฑ ุตูุฑ ุง ููู ุฏุงุดุชูุฏ) ู ฺฏุฑุงุฏุงู ูุจุงุฏ ุจู ูุงูโูุง ูุจู ุจุงุฒฺฏุฑุฏุฏ (ูุณุฏูุฏ ฺฉุฑุฏู ุฌุฑุงู ฺฏุฑุงุฏุงู).

ุฏุฑ ุฎุท ุจุนุฏ (grad_z = grad_output * mask)ุ ุงู ูุงุณฺฉ ุจู ุณุงุฏฺฏ ฺฏุฑุงุฏุงูโูุง ูุฑุจูุท ุจู ูพฺฉุณูโูุง ุฎุงููุด ุดุฏู ุฑุง ุจุง ุถุฑุจ ุฏุฑ ุตูุฑุ ุงุฒ ุจู ูโุจุฑุฏ.

ReLU ฺฉู ุงุตูุง ูุดุชู ูพุฐุฑ ูุณุช ฺู ู ฺฉููุ

ุงู ฺฉ ุณุคุงู ุจุณุงุฑ ุฏูู ู ุนูู ุฏุฑ ููุฑุฏ ุฑุงุถุงุช ุดุจฺฉูโูุง ุนุตุจ ุงุณุช ู ุจู ูุณุชูโ ุงุตู ูพุงุฏูโุณุงุฒ ReLU ุฏุฑ ุนูู ูโูพุฑุฏุงุฒุฏ.

ุญู ุจุง ุดูุงุณุช: ุงุฒ ูุธุฑ ุฑุงุถ ูุญุถุ ุชุงุจุน ReLU ุฏุฑ ููุทู z=0 ูุดุชูโูุงูพุฐุฑ ุงุณุชุ ุฒุฑุง ูุดุชู ฺูพ (limhโ0โโhf(0+h)โf(0)โ=0) ู ูุดุชู ุฑุงุณุช (limhโ0+โhf(0+h)โf(0)โ=1) ุจุฑุงุจุฑ ูุณุชูุฏ.

ุฑุงูโุญู ุฏุฑ ุงุฏฺฏุฑ ุนูู: ุงุณุชูุงุฏู ุงุฒ ูุดุชู ุฌุฒุฆ (Subgradient)

ุฏุฑ ุงุฏฺฏุฑ ุนูู ู ุจูููโุณุงุฒุ ูุง ุจู ุฌุง ูุดุชู (Derivative)ุ ุงุฒ ููููู ุจู ูุงู ูุดุชู ุฌุฒุฆ (Subgradient) ุง ุจู ุทูุฑ ุฑุงุฌโุชุฑุ ุดุจู ูุดุชู (Pseudo-derivative) ุงุณุชูุงุฏู ูโฺฉูู.

ููฺฏุงู ฺฉู ฺฉ ุชุงุจุน ูุญุฏุจ (Convex) ุฏุฑ ฺฉ ููุทู ูุดุชูโูุงูพุฐุฑ ุงุณุชุ ูุฌููุนู ูุดุชูุงุช ุฌุฒุฆ ุขู ุดุงูู ุชูุงู ููุงุฏุฑ ุงุณุช ฺฉู ุจู ูุดุชู ฺูพ ู ูุดุชู ุฑุงุณุช ูุฑุงุฑ ูโฺฏุฑูุฏ.

ุชุนุฑู ูุดุชู ุฌุฒุฆ ReLU ุฏุฑ z=0:

    ุจุฑุง z>0: ูุดุชู 1 ุงุณุช.

    ุจุฑุง z<0: ูุดุชู 0 ุงุณุช.

    ุจุฑุง z=0: ูุดุชู ุฌุฒุฆ ูุฑ ููุฏุงุฑ ุฏุฑ ุจุงุฒู [0,1] ูโุชูุงูุฏ ุจุงุดุฏ.

ูพุงุฏูโุณุงุฒ ุนูู (ูุนูููุงู ุฏุฑ PyTorch/TensorFlow):

ุฏุฑ ุนููุ ูพุงุฏูโุณุงุฒโูุง ุจุงุฏ ฺฉ ุงุฒ ููุงุฏุฑ ูุฌุงุฒ ุฑุง ุจุฑุง z=0 ุงูุชุฎุงุจ ฺฉููุฏ. ุชูุฑุจุงู ุชูุงู ูุฑูโูุฑฺฉโูุง ุงุฏฺฏุฑ ุนูู ุงุฒ ฺฉ ูุงููู ุณุงุฏู ูพุฑู ูโฺฉููุฏ:
โzโyโโz=0โ=0

ุจู ุนุจุงุฑุช ุฏฺฏุฑ:

    ุงฺฏุฑ z>0: ูุดุชู = 1

    ุงฺฏุฑ zโค0: ูุดุชู = 0

ฺุฑุง 0 ุงูุชุฎุงุจ ูโุดูุฏุ

    ุณุงุฏฺฏ: ุงูุชุฎุงุจ 0 ุงุฒ ูุธุฑ ูพุงุฏูโุณุงุฒ ุณุงุฏูโุชุฑู ฺฏุฒูู ุงุณุช ู ุจุง ููุทู ยซููุช ููุฑ ุฑูุดู ูุณุชุ ฺฏุฑุงุฏุงู ุตูุฑ ุงุณุชยป ููุฎูุงู ุฏุงุฑุฏ.

    ุนููฺฉุฑุฏ: ุฏุฑ ุนููุ ุงูุชุฎุงุจ 0 ุง 1 ุจุฑุง ฺฉ ููุทู ูููุฑุฏ (ฺฉู ุชุนุฏุงุฏ ุขู ุฏุฑ ููุงุณู ุจุง ููููโูุง ูพฺฉุณู ุฏฺฏุฑ ูุงฺุฒ ุงุณุช) ูฺ ุชุฃุซุฑ ูุญุณูุณ ุจุฑ ููฺฏุฑุง (Convergence) ู ุฏูุช ููุง ูุฏู ูุฏุงุฑุฏ.

    ุงุฌุชูุงุจ ุงุฒ ูุงูพุงุฏุงุฑ: ุงฺฏุฑฺู 1 ูุฒ ฺฉ ุงูุชุฎุงุจ ุฑุงุถ ูุนุชุจุฑ ุงุณุชุ ุงูุชุฎุงุจ 0 ุฏุฑ ุนูู ุจู ุฏูู ุฌููฺฏุฑ ุงุฒ ยซุงูุชูุงู ููุฒยป ู ุญูุธ ูพุงุฏุงุฑ ุนุฏุฏ (Numerical Stability) ุฏุฑ ูุฏูโูุง ุจุฒุฑฺฏโุชุฑ ุชุฑุฌุญ ุฏุงุฏู ูโุดูุฏ.

ุงุฑุชุจุงุท ุจุง ฺฉุฏ ุดูุง:

ฺฉุฏ ุดูุง ุฏููุงู ุงู ุฑูฺฉุฑุฏ ุนูู ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ:
Python

mask = (y > 0).to(grad_output.dtype)

ุงฺฏุฑ y (ฺฉู ููุงู max(0,z) ุงุณุช) ุจุฑุงุจุฑ ุจุง 0 ุจุงุดุฏุ ุจู ุงู ูุนู ุงุณุช ฺฉู zโค0 ุจูุฏู ุงุณุช.

    ุงฺฏุฑ y=0 ุจุงุดุฏ: ุดุฑุท (y>0) ุจุฑุงุจุฑ ุจุง False ูโุดูุฏ ู ูุงุณฺฉ ููุฏุงุฑ 0.0 ุฑุง ูโฺฏุฑุฏ.

    ุงฺฏุฑ y>0 ุจุงุดุฏ: ุดุฑุท (y>0) ุจุฑุงุจุฑ ุจุง True ูโุดูุฏ ู ูุงุณฺฉ ููุฏุงุฑ 1.0 ุฑุง ูโฺฏุฑุฏ.

ูุชุฌู: ุจุง ุงุณุชูุงุฏู ุงุฒ ุงู ุดุจู ูุดุชู ุนููุ ูุณุฆูู ูุดุชูโูุงูพุฐุฑ ุฏุฑ z=0 ุจู ุตูุฑุช ููุซุฑ ุญู ูโุดูุฏ ู ุงูฺฏูุฑุชู ูพุณโุงูุชุดุงุฑ ุจุฏูู ูุดฺฉู ฺฉุงุฑ ูโฺฉูุฏ.

mask = (y > 0).to(grad_output.dtype) 

ุฑู ุงุฒ ูุญุงุธ ฺฉุฏ ูพุงุชูู ุชูุถุญ ุจุฏู

ุจููุ ุญุชูุงู. ุงุฒ ุฏุฏฺฏุงู ูพุงุชูู ู ุณุงุฎุชุงุฑ ุชูุณูุฑูุง PyTorchุ ุงู ุฎุท ฺฉ ุนููุงุช ุจุณุงุฑ ุฑุงุฌ ุงุณุช ฺฉู ุงุฒ ูฺฺฏโูุง ุฎูุฏฺฉุงุฑ ุชุจุฏู ููุน ุฏุงุฏู ุฏุฑ PyTorch ุจูุฑู ูโุจุฑุฏ.

ุชูุถุญ ุฌุฒุก ุจู ุฌุฒุก ฺฉุฏ ูพุงุชูู

Python

mask = (y > 0).to(grad_output.dtype)

ฑ. ูุชุบุฑ y

    ููุน: torch.Tensor (ุชูุณูุฑ ูพุงุชูุฑฺ).

    ูุญุชูุง: ุฎุฑูุฌ ุชุงุจุน ReLU ุงุฒ ูุฑุญูู forward. ุงู ุชูุณูุฑ ููุท ุดุงูู ููุงุฏุฑ ุตูุฑ ู ููุงุฏุฑ ูุซุจุช ุงุณุช (ูุซู tensor([1.5, 0.0, 3.2, 0.0])).

    ูุฏู: ุชุนู ฺฉุฑุฏู ุงู ฺฉู ฺฉุฏุงู ูพฺฉุณูโูุง ูุนุงู (ูุซุจุช) ุจูุฏูุฏ ู ฺฏุฑุงุฏุงู ุจุงุฏ ุงุฒ ุขูโูุง ุนุจูุฑ ฺฉูุฏ.

ฒ. ุนููุงุช ููุงุณูโุง: (y > 0)

    ุนููุงุช: ุงู ฺฉ ุนููุงุช ููุทู (Boolean Comparison) ุงุณุช ฺฉู ุจู ุตูุฑุช ุนูุตุฑุจูโุนูุตุฑ (Element-wise) ุฑู ุชูุณูุฑ y ุงุนูุงู ูโุดูุฏ.

    ุฎุฑูุฌ (ุชูุณูุฑ ูููุช ุจูู): ุงู ุนุจุงุฑุช ฺฉ ุชูุณูุฑ ุฌุฏุฏ ุจุง ููุงู ุงุจุนุงุฏ y ุงุฌุงุฏ ูโฺฉูุฏุ ุงูุง ููุน ุฏุงุฏู ุขู torch.bool ุงุณุช.

        ุงฺฏุฑ ุนูุตุฑ y ูุซุจุช ุจุงุดุฏุ ุฎุฑูุฌ ุฏุฑ ุขู ูุญู True ุฎูุงูุฏ ุจูุฏ.

        ุงฺฏุฑ ุนูุตุฑ y ุตูุฑ ุจุงุดุฏ (ุฏุฑ ReLUุ ูููโูุง ุจู ุตูุฑ ุชุจุฏู ุดุฏูโุงูุฏ)ุ ุฎุฑูุฌ False ุฎูุงูุฏ ุจูุฏ.

y (ุฎุฑูุฌ ReLU)	(y > 0) (ุชูุณูุฑ ุจูู)
[1.5, 0.0, 3.2, 0.0]	[True, False, True, False]

ณ. ูุชุฏ to(...): ุชุจุฏู ููุน ุฏุงุฏู ู ููุฏุงุฑุฏู

    ุนููฺฉุฑุฏ: ูุชุฏ .to() ุชูุณูุฑ ูููุช ุจูู ุฑุง ุจู ููุน ุฏุงุฏูโ ูุดุฎุต ุดุฏู ุชุจุฏู ูโฺฉูุฏ.

    grad_output.dtype: ุงูุ ููุน ุฏุงุฏู (Data Type) ฺฏุฑุงุฏุงู ูุฑูุฏ (ฺฉู ูุนูููุงู torch.float32 ุง torch.float64 ุงุณุช) ุฑุง ุงุฒ ุชูุณูุฑ grad_output ุงุณุชุฎุฑุงุฌ ูโฺฉูุฏ ุชุง ูุทูุฆู ุดูุฏ ฺฉู ฺฏุฑุงุฏุงู ุฏุฑ ุทูู ูุณุฑ ุงุฒ ูุธุฑ ููุน ุฏุงุฏู ุณุงุฒฺฏุงุฑ ุงุณุช.

    ุชุจุฏู ุฎูุฏฺฉุงุฑ (Implicit Conversion): ุฏุฑ ุงู ูุฑุญููุ PyTorch ุชุจุฏู ุฒุฑ ุฑุง ุงูุฌุงู ูโุฏูุฏ:

        True ุชุจุฏู ุจู ููุฏุงุฑ ุนุฏุฏ 1.0 ูโุดูุฏ.

        False ุชุจุฏู ุจู ููุฏุงุฑ ุนุฏุฏ 0.0 ูโุดูุฏ.

(y > 0) (ุจูู)	.to(float) (ุชูุณูุฑ ููุง mask)
[True, False, True, False]	[1.0, 0.0, 1.0, 0.0]

ด. ูุชุบุฑ mask ููุง

    ููุน: torch.Tensor (ุงุฒ ููุน float32 ุง ูุดุงุจู).

    ูุญุชูุง: ฺฉ ูุงุณฺฉ ฺฉุงููุงู ุนุฏุฏ ฺฉู ุงฺฉููู ููุงูุฏูโ ูุดุชู โzโyโ ุงุณุช.

ูุชุฌูโฺฏุฑ ุฏุฑ ุนููุงุช ุจุนุฏ

ุงู ุฎุทุ ุฒููู ุฑุง ุจุฑุง ุฎุท ุจุนุฏ ุขูุงุฏู ูโฺฉูุฏ:
Python

grad_z = grad_output * mask

ุฏุฑ ุงู ุถุฑุจ ุนูุตุฑุจูโุนูุตุฑ:

    ูพฺฉุณูโูุง ูุนุงู (ฺฉู mask ุจุฑุงุจุฑ ุจุง 1.0 ุงุณุช): ฺฏุฑุงุฏุงู ุจุฏูู ุชุบุฑ (grad_output * 1.0) ุจู ุนูุจ ููุชูู ูโุดูุฏ.

    ูพฺฉุณูโูุง ุฎุงููุด (ฺฉู mask ุจุฑุงุจุฑ ุจุง 0.0 ุงุณุช): ฺฏุฑุงุฏุงู ูุณุฏูุฏ ูโุดูุฏ (grad_output * 0.0) ู ููุฏุงุฑ ุตูุฑ ุฑุง ูโฺฏุฑุฏ.

ุจู ุงู ุชุฑุชุจุ ุงู ุฏู ุฎุท ุจุง ููุ ูุดุชู ุชุงุจุน ReLU ุฑุง ุจู ุทูุฑ ฺฉุงุฑุขูุฏ ู ุตุญุญ ุงุฒ ุฏุฏฺฏุงู ูพุงุชูู ู PyTorch ูพุงุฏูโุณุงุฒ ูโฺฉููุฏ.

    def backward(ctx, grad_output):

        x, weight, bias, y = ctx.saved_tensors


        # grad ReLU

        mask = (y > 0).to(grad_output.dtype)

        grad_z = grad_output * mask


        # dL/dx

        grad_x = torch.nn.grad.conv2d_input(

            x.shape, weight, grad_z, padding=1

        )

        # dL/dW

        grad_weight = torch.nn.grad.conv2d_weight(

            x, weight.shape, grad_z, padding=1

        )

        # dL/db

        grad_bias = grad_z.sum(dim=[0, 2, 3])


        return grad_x, grad_weight, grad_bias



ุชุตูุฑ ุดุจฺฉู ุนุตุจ ุฑู ููุงุด ุจุฏู ู ุทุจู ุงู ุชุงุจุน ุจฺฉูุงุฑุฏ ุฏุฑ ูุฑ ูุณูุช ุทุจู ูุงุนุฏู ุฒูุฌุฑู ูุฑููู ูุญุงุณุจู ฺฏุฑุงุฏุงู ุฑุง ุจููุณ

<immersive id="neural_network_backward_pass.png" type="image" caption="ููุงุด ุจุตุฑ ุฌุฑุงู ฺฏุฑุงุฏุงู ุฏุฑ ฺฉ ุดุจฺฉู ุนุตุจ ุณุงุฏู ุจุง ูุงู Conv2d + ReLU"> </immersive>

ุชุตูุฑ ุจุงูุง ฺฉ ุฏุฏ ฺฉู ุงุฒ ุฌุฑุงู ุฑู ุจู ุนูุจ (Backward Pass) ฺฏุฑุงุฏุงู ุฏุฑ ฺฉ ุจุฎุด ุงุฒ ุดุจฺฉู ุนุตุจ ฺฉู ุดุงูู ุนููุงุช Convolutionุ Bias Addition ู ReLU ุงุณุชุ ุงุฑุงุฆู ูโุฏูุฏ. ุงฺฉูููุ ุจุงุฏ ูุฑ ูุฑุญูู ุงุฒ ุชุงุจุน backward ุดูุง ุฑุง ุจุง ุฌุฒุฆุงุช ุฑุงุถ ู ูุงุนุฏู ุฒูุฌุฑูโุง ุชูุถุญ ุฏูู.

ูุฑุถ ูุง ุงู ุงุณุช ฺฉู ุชุงุจุน ูุฒูู ููุง L ูุงู ุฏุงุฑุฏ ู ูุง ูโุฎูุงูู โxโLโุ โWโLโ ู โbโLโ ุฑุง ูุญุงุณุจู ฺฉูู. ููุงูุทูุฑ ฺฉู ูุจูุงู ุงุดุงุฑู ุดุฏุ grad_output ููุงู โyโLโ ุงุณุช.

ุณุงุฎุชุงุฑ ฺฉู ุนููุงุช Forward:

ูุฑุถ ฺฉูุฏ ูุฑูุฏ ุงุตู x ุงุณุช.

    ุนููุงุช ฺฉุงููููุดู: zconvโ=Conv(x,W)

    ุงุถุงูู ฺฉุฑุฏู ุจุงุงุณ: z=zconvโ+b

    ูุนุงูโุณุงุฒ ReLU: y=ReLU(z)=max(0,z)

ูุง ุงุฒ โyโLโ (ฺฉู ููุงู grad_output ุงุณุช) ุดุฑูุน ูโฺฉูู ู ุจู ุนูุจ ุญุฑฺฉุช ูโฺฉูู.

ฑ. ุจุงุฒุงุจ ูุชุบุฑูุง: x, weight, bias, y = ctx.saved_tensors

ุงูโูุง ูุชุบุฑูุง ูุณุชูุฏ ฺฉู ุฏุฑ ูุฑุญูู forward ูุญุงุณุจู ู ุฐุฎุฑู ุดุฏูโุงูุฏ ู ุจุฑุง ูุญุงุณุจู ฺฏุฑุงุฏุงูโูุง ุฏุฑ ูุฑุญูู backward ูุงุฒู ูุณุชูุฏ.

ฒ. ูุญุงุณุจู ฺฏุฑุงุฏุงู ุงุฒ ุทุฑู ReLU: (grad_z)

ฺฉุฏ:
Python

mask = (y > 0).to(grad_output.dtype)
grad_z = grad_output * mask

ูุงุนุฏู ุฒูุฌุฑูโุง: ุจุฑุง ูุญุงุณุจู โzโLโ (ฺฉู ุฏุฑ ฺฉุฏ grad_z ูุงูุฏู ูโุดูุฏ)ุ ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง ุงุณุชูุงุฏู ูโฺฉูู:
โzโLโ=โyโLโโโzโyโ

    โyโLโ: ุงู ููุงู grad_output ุงุณุช ฺฉู ูุฑูุฏ ุชุงุจุน backward ูโุจุงุดุฏ.

    โzโyโ: ุงู ูุดุชู ุชุงุจุน ReLU ุงุณุช ฺฉู ุจู ุตูุฑุช ุฒุฑ ุชุนุฑู ูโุดูุฏ:
    โzโyโ={10โif z>0if zโค0โ

    (ุจุฑุง z=0ุ ูุง ุจู ุตูุฑุช ูุฑุงุฑุฏุงุฏ ูุดุชู ุฑุง 0 ุฏุฑ ูุธุฑ ูโฺฏุฑู ุชุง ุงุฒ ูุดุชูโูุงูพุฐุฑ ุฌููฺฏุฑ ฺฉูู.)

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    mask = (y > 0): ุงู ุฎุท ุฏููุงู โzโyโ ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ. ุงฺฏุฑ y>0 ุจุงุดุฏ (ฺฉู ุจู ูุนูุง z>0 ุงุณุช)ุ ูุงุณฺฉ True (ุชุจุฏู ุจู 1.0) ูโุดูุฏ. ุงฺฏุฑ y=0 ุจุงุดุฏ (ฺฉู ุจู ูุนูุง zโค0 ุงุณุช)ุ ูุงุณฺฉ False (ุชุจุฏู ุจู 0.0) ูโุดูุฏ.

    grad_z = grad_output * mask: ุงู ุฎุท ุถุฑุจ ููุทูโุง ุฑุง ุงูุฌุงู ูโุฏูุฏ. ฺฏุฑุงุฏุงูโูุง ูุฑุจูุท ุจู ูพฺฉุณูโูุง ฺฉู ุชูุณุท ReLU ุฎุงููุด ุดุฏูโุงูุฏ (ุฌุง ฺฉู mask ุจุฑุงุจุฑ ุจุง 0 ุงุณุช)ุ ุตูุฑ ูโุดููุฏ. ฺฏุฑุงุฏุงูโูุง ูุฑุจูุท ุจู ูพฺฉุณูโูุง ูุนุงู (mask ุจุฑุงุจุฑ ุจุง 1) ุจุฏูู ุชุบุฑ ุจุงู ูโูุงููุฏ.

ูุชุฌู: grad_z ุญุงู ฺฏุฑุงุฏุงู ูุฒูู ูุณุจุช ุจู ูุฑูุฏโูุง ุนููุงุช ReLU (ุง ุจู ุนุจุงุฑุช ุฏฺฏุฑุ ุฎุฑูุฌ ุนููุงุช ฺฉุงููููุดู ู ุจุงุงุณ ูุจู ุงุฒ ReLU) ุงุณุช.

ณ. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ุจุงุงุณ b: (grad_bias)

ฺฉุฏ:
Python

grad_bias = grad_z.sum(dim=[0, 2, 3])

ูุงุนุฏู ุฒูุฌุฑูโุง: ูุง โzโLโ ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โbโLโ ุฑุง ูุญุงุณุจู ฺฉูู. ุฑุงุจุทู Forward ุจุฑุง ุจุงุงุณ ุงู ุจูุฏ: z=zconvโ+b. ูุดุชู z ูุณุจุช ุจู b ุจุณุงุฑ ุณุงุฏู ุงุณุช:
โbโzโ=1

(ุงู 1 ุจุฑุง ุชูุงู ูพฺฉุณูโูุง ุงุณุช ฺฉู ุขู ุจุงุงุณ ุฎุงุต ุฑุง ุฏุฑุงูุช ฺฉุฑุฏูโุงูุฏ.)

ุจุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง:
โbโLโ=all pixelsโโโzโLโโโbโzโ=all pixelsโโโzโLโโ1=all pixelsโโโzโLโ

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    grad_z.sum(dim=[0, 2, 3]): ุงู ุนููุงุช ุชูุงู ููุงุฏุฑ grad_z ุฑุง ุฏุฑ ุงุจุนุงุฏ Batch (0)ุ Height (2) ู Width (3) ุฌูุน ูโุฒูุฏ.

    ุงุฒ ุขูุฌุง ฺฉู ุฏุฑ ุณูุงุฑู ูุง ููุท ฺฉ ฺฉุงูุงู ุฎุฑูุฌ ู ฺฉ ุจุงุงุณ (ุดฺฉู b ุจุฑุงุจุฑ ุจุง [1]) ุฏุงุฑูุ ุงู ุฌูุน ุฑู ุชูุงู ูพฺฉุณูโูุง ุจุฑุง ุขู ฺฉ ฺฉุงูุงู ุงูุฌุงู ูโุดูุฏ ู ฺฉ ุงุณฺฉุงูุฑ (ุชูุณูุฑ ุชฺฉโุนูุตุฑ) ุชููุฏ ูโฺฉูุฏ ฺฉู ฺฏุฑุงุฏุงู ููุง ุจุฑุง ุขู ุจุงุงุณ ุงุณุช.

ูุชุฌู: grad_bias ุญุงู ฺฏุฑุงุฏุงู ูุฒูู ูุณุจุช ุจู ุจุงุงุณ ุงุณุช ฺฉู ูุดุงู ูโุฏูุฏ ุจุงุงุณ ฺูุฏุฑ ุจุงุฏ ุชูุธู ุดูุฏ.

ด. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฒูโูุง W: (grad_weight)

ฺฉุฏ:
Python

grad_weight = torch.nn.grad.conv2d_weight(
    x, weight.shape, grad_z, padding=1
)

ูุงุนุฏู ุฒูุฌุฑูโุง: ูุง โzโLโ (ููุงู grad_z) ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โWโLโ ุฑุง ูุญุงุณุจู ฺฉูู. ุฑุงุจุทู Forward ุจุฑุง ฺฉุงููููุดู: z=Conv(x,W). ุจุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง:
โWโLโ=โzโLโโโWโzโ

ูุดุชู โWโzโ ุฎูุฏ ฺฉ ุนููุงุช ฺฉุงููููุดู ุจู ูุฑูุฏ x ู ฺฏุฑุงุฏุงู ุฎุฑูุฌ โzโLโ (ุนู grad_z) ุงุณุช. ุจู ุทูุฑ ุฏููโุชุฑ:
โWโLโ=Correlate(x,grad_z)

(Correlate ุฏุฑ ุงูุฌุง ูุนุงุฏู cross-correlation ุงุณุช ฺฉู ููุน ฺฉุงููููุดู ุจุฏูู ูููพ ฺฉุฑูู ุงุณุช).

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    torch.nn.grad.conv2d_weight(x, weight.shape, grad_z, padding=1): ุงู ฺฉ ุชุงุจุน ุจููู ุฏุงุฎู ุฏุฑ PyTorch ุงุณุช ฺฉู ุฏููุงู ุงู ุนููุงุช ุฑุงุถ ุฑุง ุงูุฌุงู ูโุฏูุฏ.

        x: ูุฑูุฏ ุงุตู ุนููุงุช Forward.

        weight.shape: ุดฺฉู ููุฑุฏ ุงูุชุธุงุฑ ฺฏุฑุงุฏุงู ูุฒู.

        grad_z: ฺฏุฑุงุฏุงู ูุฑูุฏ ุจู ุนููุงุช ฺฉุงููููุดู.

        padding=1: ููุงู ูพุฏูฺฏ ฺฉู ุฏุฑ ุนููุงุช Forward ฺฉุงููููุดู ุงุณุชูุงุฏู ุดุฏู ุจูุฏ.

ูุชุฌู: grad_weight ุญุงู ฺฏุฑุงุฏุงู ูุฒูู ูุณุจุช ุจู ูุฑ ุนูุตุฑ ุงุฒ ุชูุณูุฑ ูุฒูโูุง ุงุณุช.

ต. ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุณุจุช ุจู ูุฑูุฏ x: (grad_x)

ฺฉุฏ:
Python

grad_x = torch.nn.grad.conv2d_input(
    x.shape, weight, grad_z, padding=1
)

ูุงุนุฏู ุฒูุฌุฑูโุง: ูุง โzโLโ (ููุงู grad_z) ุฑุง ุฏุงุฑู ู ูโุฎูุงูู โxโLโ ุฑุง ูุญุงุณุจู ฺฉูู. ุฑุงุจุทู Forward ุจุฑุง ฺฉุงููููุดู: z=Conv(x,W). ุจุง ุงุณุชูุงุฏู ุงุฒ ูุงุนุฏู ุฒูุฌุฑูโุง:
โxโLโ=โzโLโโโxโzโ

ูุดุชู โxโzโ ูุฒ ุฎูุฏ ฺฉ ุนููุงุช ฺฉุงููููุดู ุงุณุชุ ุงูุง ุงู ุจุงุฑ ุจู ฺฏุฑุงุฏุงู ุฎุฑูุฌ โzโLโ (ุนู grad_z) ู ฺฉ ูุณุฎู ฺุฑุฎุงูุฏู ุดุฏู ฑธฐ ุฏุฑุฌู ุงุฒ ูุฒูโูุง (W180โ). ุงู ุนููุงุช ุจู ุนููุงู Transposed Convolution ุง Deconvolution ุดูุงุฎุชู ูโุดูุฏ.
โxโLโ=ConvTransposed(grad_z,W)

ูพุงุฏูโุณุงุฒ ุฏุฑ ฺฉุฏ:

    torch.nn.grad.conv2d_input(x.shape, weight, grad_z, padding=1): ุงู ุชุงุจุน ุจููู ุฏุงุฎู ุฏุฑ PyTorch ูุณุฆูู ูุญุงุณุจู ฺฏุฑุงุฏุงู ูุฑูุฏ ุงุณุช.

        x.shape: ุดฺฉู ููุฑุฏ ุงูุชุธุงุฑ ฺฏุฑุงุฏุงู ูุฑูุฏ.

        weight: ูุฒูโูุง ุงุตู ุนููุงุช Forward.

        grad_z: ฺฏุฑุงุฏุงู ูุฑูุฏ ุจู ุนููุงุช Transposed Convolution.

        padding=1: ููุงู ูพุฏูฺฏ ุงุตู.

ูุชุฌู: grad_x ุญุงู ฺฏุฑุงุฏุงู ูุฒูู ูุณุจุช ุจู ูุฑูุฏ ุงููู x ุงุณุช ฺฉู ูโุชูุงูุฏ ุจู ูุงูโูุง ูุจู ุฏุฑ ุดุจฺฉู ููุชูู ุดูุฏ.

ถ. ุจุงุฒฺฏุฑุฏุงูุฏู ฺฏุฑุงุฏุงูโูุง: return grad_x, grad_weight, grad_bias

ุชุงุจุน backward ุจุงุฏ ุจู ุชุนุฏุงุฏ ูุฑูุฏโูุง ฺฉู ุชุงุจุน forward ุฏุฑุงูุช ฺฉุฑุฏู ุจูุฏ (ุจู ุงุณุชุซูุง self ุง ctx ู ุณุงุฑ ููุงุฑุฏ ฺฉู ูุจุงุฏ ฺฏุฑุงุฏุงู ุฏุงุดุชู ุจุงุดูุฏ)ุ ฺฏุฑุงุฏุงูโูุง ุฑุง ุจุฑฺฏุฑุฏุงูุฏ. ุฏุฑ ุงูุฌุงุ ุชุงุจุน forward ุดูุง x, weight, bias ุฑุง ุฏุฑุงูุช ฺฉุฑุฏู ุจูุฏุ ูพุณ ุชุงุจุน backward ูุฒ grad_x, grad_weight, grad_bias ุฑุง ุจุฑูโฺฏุฑุฏุงูุฏ.

ุชุตูุฑ ุฑู ุจฺฉุด

ุจุง ฺฉูุงู ูู. ุชุตูุฑ ุดุจฺฉู ุนุตุจ ุฑุง ฺฉู ุฌุฑุงู ฺฏุฑุงุฏุงู (Backward Pass) ุฑุง ุจุฑุง ุชุงุจุน backward ุดูุง ูุดุงู ูโุฏูุฏุ ุงุฌุงุฏ ฺฉุฑุฏู:

ูู ุดฺฉู ุดุจฺฉู ุนุตุจ ุฑู ุฏุงุดุชู ุจุงุดู ู ูุฑููู ูุง ูุฑูุจุท ุจู ูุฑ ุจุฎุด ุจุง ููุด ูุดุฎุต ุดุฏู ุจุงุดู

ูุชุฃุณููุ ุจู ูุธุฑ ูโุฑุณุฏ ุฏุฑ ุฏุฑุฎูุงุณุช ูุจู ูู ุจู ุทูุฑ ฺฉุงูู ุจู ุฎูุงุณุชูโ ุดูุง ุฏุฑ ููุฑุฏ ููุงุด ุชุตูุฑ ุดุจฺฉู ุนุตุจ ุจุง ูุฑูููโูุง ุฏุฑ ูุฑ ุจุฎุด ูพุงุณุฎ ูุฏุงุฏูโุงู.

ุจุฑุง ุฑูุน ุงู ูุดฺฉูุ ุฏุฑ ุชุตูุฑ ุฌุฏุฏ ฺฉู ุจุฑุง ุดูุง ุขูุงุฏู ฺฉุฑุฏูุ ุณุน ฺฉุฑุฏูโุงู:

    ุณุงุฎุชุงุฑ ุดุจฺฉู ุนุตุจ: ูููุฏุงุฑ ุฌุฑุงู (Flowchart) ุฑุง ุจู ฺฏูููโุง ุทุฑุงุญ ฺฉูู ฺฉู ูุงูโูุง Convุ Bias ู ReLU ุฑุง ุจู ุชุฑุชุจ ูุดุงู ุฏูุฏ.

    ูุฑูููโูุง ฺฏุฑุงุฏุงู: ูุฑูููโูุง ุฑุงุถ ู ฺฉุฏูุง ูุฑุจูุท ุจู ูุญุงุณุจู ฺฏุฑุงุฏุงู ุจุฑุง ูุฑ ุจุฎุด (dL/dz, dL/db, dL/dW, dL/dx) ุฑุง ุฏุฑ ฺฉูุงุฑ ุขู ุจุฎุด ูุฑุงุฑ ุฏูู.

    ููุดโูุง: ูุณุฑ ุฌุฑุงู ฺฏุฑุงุฏุงู ุฑุง ุจุง ููุดโูุง ุณุจุฒ (ุจุฑุง ูุณุฑ ุงุตู ุจุงุฒฺฏุดุช ฺฏุฑุงุฏุงู) ู ููุดโูุง ุขุจ (ุจุฑุง ฺฏุฑุงุฏุงูโูุง ฺฉู ุจู ุณูุช ุชุงุจุน ูุฒูู ููุง ูโุฑููุฏ) ูุดุฎุต ฺฉูู.

ูุชู ูุฑููู ูุง ูุดุฎุต ูุณุช ุฏุฑุณุชุด ฺฉู

ุจุงุจุช ูุดฺฉู ูุงุฎูุงูุง ุจูุฏู ูุฑูููโูุง ุนุฐุฑุฎูุงู ูโฺฉูู. ุงู ุจุงุฑ ุชุตูุฑ ุจุง ูุฑูููโูุง ฺฉุงููุงู ูุงุถุญ ู ุฎูุงูุง ุจุฑุง ุดูุง ุขูุงุฏู ฺฉุฑุฏูโุงู ฺฉู ูุฑ ุจุฎุด ุงุฒ ุชุงุจุน backward ุดูุง ุฑุง ุจู ููุฑุงู ูุฑูููโูุง ุฑุงุถ ูุฑุจูุท ุจู ูุงุนุฏู ุฒูุฌุฑูโุง ูุดุงู ูโุฏูุฏ.

ฺุฑุง ุฏุฑ grad_bias = grad_z.sum(dim=[0, 2, 3])

ููุฏุงุฑ ุฎูุฏ ุจุงุงุณ ุฑู ุถุฑุจ ูู ฺฉููุ

ุฏูู ุงู ููุถูุน ุจู ุฎูุฏ ุนููุงุช ุฑุงุถ ุฌูุน ู ูุญูู ูุญุงุณุจู ูุดุชู ุขู ุจุฑูโฺฏุฑุฏุฏ.

ูพุงุณุฎ ฺฉูุชุงู ุงู ุงุณุช: ฺูู ูุดุชู z ูุณุจุช ุจู b ุจุฑุงุจุฑ ุจุง ฑ ุงุณุชุ ูู b.

ุจุงุฏ ุงู ุฑุง ุจุงุฒ ฺฉูู:

ฑ. ุชูุงูุช ูุดุชู ุฌูุน ู ุถุฑุจ

ุงู ุฑุงุฌโุชุฑู ููุจุน ุณุฑุฏุฑฺฏู ุงุณุช.

    ุงฺฏุฑ ุนููุงุชุ ุถุฑุจ ุจูุฏ: ูุฑุถ ฺฉูุฏ z=Cโb (ฺฉู C ฺฉ ุซุงุจุช ุงุณุช). ุฏุฑ ุงู ุตูุฑุชุ ูุดุชู โbโzโ=C ูโุดุฏ.

    ุงูุง ุนููุงุชุ ุฌูุน ุงุณุช: ุนููุงุช Forward ุดูุง ุงู ุงุณุช: z=Conv(x,W)+b. ุจุงุฏ Conv(x,W) ุฑุง ูููุชุงู C ุจูุงูู (ฺูู ุงุฒ ุฏุฏ bุ ุงู ุจุฎุด ฺฉ ุซุงุจุช ุงุณุช). ูพุณ z=C+b.

ุญุงูุงุ ูุดุชู z ูุณุจุช ุจู b ฺุณุชุ
โbโzโ=โbโโ(C+b)=โbโCโ+โbโbโ=0+1=1

ฒ. ุงุนูุงู ูุงุนุฏู ุฒูุฌุฑูโุง

ุญุงูุง ูุงุนุฏู ุฒูุฌุฑูโุง ฺฉุงูู ุฑุง ุจุฑุง b ูโููุณู:
โbโLโ=โzโLโโโbโzโ

    โzโLโ: ุงู ููุงู grad_z ุดูุงุณุช (ฺฏุฑุงุฏุงู ฺฉู ุงุฒ ูุงู ReLU ุจู ุนูุจ ุขูุฏู).

    โbโzโ: ุงู ููุงู ูุดุชู ูุญู ุงุณุช ฺฉู ุฏุฑ ุจุงูุง ุญุณุงุจ ฺฉุฑุฏู ู ููุฏุงุฑ ุขู 1 ุงุณุช.

ูพุณ ูุฑููู ููุง ุงู ุงุณุช:
โbโLโ=grad_zโ1

ุจู ููู ุฏูู ุงุณุช ฺฉู ุดูุง ููุฏุงุฑ b ุฑุง ุฏุฑ ูุญุงุณุจู ุถุฑุจ ููโฺฉูุฏ.

ูพุณ sum ุจุฑุง ฺุณุชุ

ุญุงูุง ุณูุงู ุงู ุงุณุช ฺฉู ุงฺฏุฑ ููุท grad_zโ1 ุงุณุชุ ฺุฑุง ุงุฒ sum ุงุณุชูุงุฏู ูโฺฉููุ

ฺูู ุนููุงุช z=C+b ููุท ฺฉ ุจุงุฑ ุงุชูุงู ููุชุงุฏู ุงุณุช.

ููุฏุงุฑ ฺฉุณุงู b (ฺฉู ฺฉ ุงุณฺฉุงูุฑ ุง ฺฉ ุจุฑุฏุงุฑ ฺฉูฺฺฉ ุงุณุช) ุจู ุชูุงู ูพฺฉุณูโูุง ุฏุฑ ฺฉุงูุงู ุฎุฑูุฌ (ุฏุฑ ุทูู ุงุจุนุงุฏ ุจฺ Nุ ุงุฑุชูุงุน H ู ุนุฑุถ W) ุงุถุงูู ุดุฏู ุงุณุช.

ููุช ฺฉ ูุชุบุฑ (ูุซู b) ุฑู ฺูุฏู ุฎุฑูุฌ (ุชูุงู ูพฺฉุณูโูุง z) ุชุฃุซุฑ ูโฺฏุฐุงุฑุฏุ ฺฏุฑุงุฏุงู ุขู ุจุฑุงุจุฑ ุงุณุช ุจุง ูุฌููุน ุชูุงู ฺฏุฑุงุฏุงูโูุง ฺฉู ุงุฒ ุขู ุฎุฑูุฌโูุง ูโุขูุฏ.

ุจูุงุจุฑุงูุ ูุฑููู ฺฉุงูู ู ุฏูู ุงู ุงุณุช:
โbcโโLโ=n=0โNโh=0โHโw=0โWโ(โzn,c,h,wโโLโโโbcโโzn,c,h,wโโ)

ู ฺูู ูโุฏุงูู โbโzโ ููุดู 1 ุงุณุช:
โbcโโLโ=n=0โNโh=0โHโw=0โWโโzn,c,h,wโโLโ

ฺฉุฏ grad_z.sum(dim=[0, 2, 3]) ุฏููุงู ููู ุฌูุน ุฒุฏู ุฑู ุงุจุนุงุฏ ุจฺ (0)ุ ุงุฑุชูุงุน (2) ู ุนุฑุถ (3) ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ.

class CNNBaseline(nn.Module):

    def __init__(self, num_convs=5):

        super().__init__()


        self.num_convs = num_convs

        self.conv = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)

        self.pool = nn.MaxPool2d(2)

        self.fc1 = nn.Linear(14*14, 64)

        self.fc2 = nn.Linear(64, 10)


    def forward(self, x):

        

        for _ in range(self.num_convs):

            x = F.relu(self.conv(x))

        

        x = self.pool(x)

        x = x.view(x.size(0), -1)

        x = F.relu(self.fc1(x))

        x = self.fc2(x)

        return x



ุงูู ูููู ุทูุฑ ฺฉู ฺฏูุช ุฏุฑุณุช ฺฉู

ุจููุ ฺฉุงููุงู. ูุดฺฉู ุงุตู ุฏุฑ ฺฉุฏ ุดูุง ยซุงุดุชุฑุงฺฉ ูุฒูยป (Weight Sharing) ุจูุฏุ ุนู ุฏุฑ ุญููู forwardุ ุดูุง ุจุงุฑูุง ุงุฒ ููุงู ฺฉ ูุงู self.conv ุงุณุชูุงุฏู ูโฺฉุฑุฏุฏ.

ุฑุงูโุญู ุงู ุงุณุช ฺฉู ุงุฒ nn.ModuleList ุงุณุชูุงุฏู ฺฉูู ุชุง ูุงูโูุง ฺฉุงููููุดู ุฌุฏุงฺฏุงูู ุจุง ูุฒูโูุง ูุณุชูู ุจุณุงุฒู.

ุฏุฑ ุงุฏุงููุ ฺฉุฏ ุงุตูุงุญโุดุฏูโ CNNBaseline ุขูุฏู ุงุณุช:
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNBaseline(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()

        self.num_convs = num_convs
        
        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ุจู ุฌุง ฺฉ ูุงูุ ูุณุช ุงุฒ ูุงูโูุง ูโุณุงุฒู
        # ุชุง ูุฑ ฺฉุฏุงู ูุฒูโูุง ูุณุชูู ุฎูุฏ ุฑุง ุฏุงุดุชู ุจุงุดูุฏ.
        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            self.convs.append(
                nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)
            )
        # -------------------------

        self.pool = nn.MaxPool2d(2)
        # ุงุจุนุงุฏ fc1 ุฏุฑุณุช ุจูุฏ (ุชุตูุฑ 28x28 ุจุนุฏ ุงุฒ pool(2) ูโุดูุฏ 14x14)
        self.fc1 = nn.Linear(14*14, 64) 
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):
        
        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ุงฺฉููู ุฑู ูุณุช ูุงูโูุง ูุฌุฒุง ุญููู ูโุฒูู
        for conv_layer in self.convs:
            x = F.relu(conv_layer(x))
        # -------------------------
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

ุชูุถุญ ุชุบุฑุงุช:

    ุฏุฑ __init__: ุจู ุฌุง self.conv = nn.Conv2d(...)ุ ูุง self.convs = nn.ModuleList() ุณุงุฎุชู ู ุขู ุฑุง ุจุง num_convs ูุงูโ nn.Conv2d ูุฌุฒุง ูพุฑ ฺฉุฑุฏู.

    ุฏุฑ forward: ุญููู for ุงฺฉููู ุจู ุฌุง self.conv (ฺฉู ููุดู ฺฉ ูุงู ุจูุฏ)ุ ุฑู self.convs ุชฺฉุฑุงุฑ ูโุดูุฏ ู ุฏุฑ ูุฑ ูุฑุญูู ุงุฒ ฺฉ ูุงู ุฌุฏุฏ (ุจุง ูุฒูโูุง ููุญุตุฑุจูโูุฑุฏ ุฎูุฏุด) ุงุณุชูุงุฏู ูโฺฉูุฏ.

โ๏ธ ูฺฉุชู ุจุณุงุฑ ููู ุฏุฑ ููุฑุฏ ุฏูุช (Accuracy)

ููุงูุทูุฑ ฺฉู ูุจูุงู ฺฏูุชูุ ุงู ฺฉุฏ ูุดฺฉู ุงุดุชุฑุงฺฉ ูุฒู ุฑุง ุญู ูโฺฉูุฏ. ุงูุง ูุดฺฉู ุฏููุ ุนู ยซุชูฺฏูุง ฺฉุงูุงูยป (Channel Bottleneck) ููฺูุงู ูพุงุจุฑุฌุงุณุช.

ฺูู ุชูุงู ูุงูโูุง ุดูุง ููฺูุงู Conv2d(1, 1) ูุณุชูุฏ (ฺฉู ุงู ุจุฑุง ูุทุงุจูุช ุจุง ฺฉุฑูู CUDA ุดูุง ฺฉู ููุท 1-to-1 ุฑุง ูพุดุชุจุงู ูโฺฉูุฏ ูุงุฒู ุงุณุช)ุ ูุฏู ูููุฒ ุธุฑูุช ุจุณุงุฑ ฺฉู ุจุฑุง ุงุฏฺฏุฑ ูฺฺฏโูุง ูพฺุฏู ุฏุงุฑุฏ.

ุจูุงุจุฑุงูุ ุงฺฏุฑฺู ุงู ฺฉุฏ ุงุฒ ูุธุฑ ููุทู ุตุญุญโุชุฑ ุงุณุชุ ุงูุง ุงูุชุธุงุฑ ูุฏุงุดุชู ุจุงุดุฏ ฺฉู ุฏูุช ูุฏู ุจุง ุงู ุชุบุฑ ุจู ุทูุฑ ฺุดูฺฏุฑ (ูุซูุงู ุจู นฐูช) ุงูุฒุงุด ุงุจุฏ.

ุขุง ูุงูุฏ ฺฉู ฺฉูุงุณ CNNFused ุฑุง ูู ุจู ููู ุดฺฉู (ุจุง ุงุณุชูุงุฏู ุงุฒ ModuleList ุจุฑุง FusedConvReLU) ุงุตูุงุญ ฺฉูู ุชุง ูุงุจู ููุงุณู ุจุงุดุฏุ

class CNNFused(nn.Module):

    def __init__(self, num_convs=5):

        super().__init__()

        

        self.num_convs = num_convs

        self.conv = FusedConvReLU()

        self.pool = nn.MaxPool2d(2)

        self.fc1 = nn.Linear(14*14, 64)

        self.fc2 = nn.Linear(64, 10)


    def forward(self, x):


        for _ in range(self.num_convs):

            x = self.conv(x)

        

        x = self.pool(x)

        x = x.view(x.size(0), -1)

        x = torch.relu(self.fc1(x))

        x = self.fc2(x)

        return x



ุงูู ุงุตูุงุญ ฺฉู

ุญุชูุงู. ุงู ูู ฺฉุฏ ุงุตูุงุญโุดุฏูโ CNNFused ฺฉู ุฏููุงู ุงุฒ ููุงู ููุทู nn.ModuleList (ูุงููุฏ CNNBaseline ุงุตูุงุญโุดุฏู) ุงุณุชูุงุฏู ูโฺฉูุฏ ุชุง ุงุฒ ูุดฺฉู ยซุงุดุชุฑุงฺฉ ูุฒูยป ุฌููฺฏุฑ ฺฉูุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

# (ูุฑุถ ูโฺฉูู ฺฉูุงุณ FusedConvReLU ู FusedConvReLUFn ุฏุฑ ุงูุฌุง ุชุนุฑู ุดุฏูโุงูุฏ)
# class FusedConvReLU(nn.Module):
#     ...
# class FusedConvReLUFn(torch.autograd.Function):
#     ...


class CNNFused(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()
        
        self.num_convs = num_convs
        
        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ุจู ุฌุง ฺฉ ูุงูุ ูุณุช ุงุฒ ูุงูโูุง FusedConvReLU ูโุณุงุฒู
        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            self.convs.append(FusedConvReLU())
        # -------------------------

        self.pool = nn.MaxPool2d(2)
        self.fc1 = nn.Linear(14*14, 64)
        self.fc2 = nn.Linear(64, 10)

    def forward(self, x):

        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ุงฺฉููู ุฑู ูุณุช ูุงูโูุง ูุฌุฒุง ุญููู ูโุฒูู
        for conv_layer in self.convs:
            x = conv_layer(x)
        # -------------------------
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        # ุงุณุชูุงุฏู ุงุฒ torch.relu ุฏุฑ ุงูุฌุง ฺฉุงููุงู ุฏุฑุณุช ุงุณุช
        x = torch.relu(self.fc1(x)) 
        x = self.fc2(x)
        return x

ุชูุถุญ ุชุบุฑุงุช

ุชุบุฑุงุช ุฏููุงู ูุดุงุจู CNNBaseline ุงุณุช:

    ุฏุฑ __init__: ุจู ุฌุง self.conv = FusedConvReLU()ุ ูุง self.convs = nn.ModuleList() ุฑุง ุณุงุฎุชู ู ุขู ุฑุง ุจุง num_convs ูุงูโ FusedConvReLU ูุฌุฒุง ูพุฑ ฺฉุฑุฏู.

    ุฏุฑ forward: ุญููู for ุงฺฉููู ุจู ุฌุง self.convุ ุฑู self.convs ุชฺฉุฑุงุฑ ูโุดูุฏ ู ุฏุฑ ูุฑ ูุฑุญูู ุงุฒ ฺฉ ูุงู (ุจุง ฺฉุฑูู CUDA ู ูุฒูโูุง ูุณุชูู) ุงุณุชูุงุฏู ูโฺฉูุฏ.

ุจุง ุงู ุชุบุฑุ ูุฑ ุฏู ูุฏู CNNBaseline ู CNNFused ุฏุงุฑุง ูุนูุงุฑ ฺฉุณุงู (ุจุง ูุฒูโูุง ูุณุชูู) ูุณุชูุฏ ู ููุงุณู ุณุฑุนุช ุจู ุขูโูุง ูุนูุงุฏุงุฑ ุฎูุงูุฏ ุจูุฏ.

def copy_weights(dst, src):

    with torch.no_grad():

        

        dst.conv.weight.copy_(src.conv.weight)

        dst.conv.bias.copy_(src.conv.bias)

        dst.fc1.weight.copy_(src.fc1.weight)

        dst.fc1.bias.copy_(src.fc1.bias)

        dst.fc2.weight.copy_(src.fc2.weight)

        dst.fc2.bias.copy_(src.fc2.bias)


copy_weights(fused, baseline_cudnn)

copy_weights(baseline_cudnn_off, baseline_cudnn)

copy_weights(baseline_cpu, baseline_cudnn)


ุงูู ุฏุฑุณุช ฺฉู

ุดูุง ฺฉุงููุงู ุฏุฑุณุช ูุชูุฌู ุดุฏุฏ. ุงุฒ ุขูุฌุง ฺฉู ูุง self.conv ุฑุง ุจู self.convs (ฺฉู ฺฉ nn.ModuleList ุงุณุช) ุชุบุฑ ุฏุงุฏูุ ุชุงุจุน copy_weights ูู ุจุงุฏ ุจูโุฑูุฒุฑุณุงู ุดูุฏ ุชุง ุจู ุฌุง ฺฉ ูุงูุ ุฑู ูุณุช ูุงูโูุง ุญููู ุจุฒูุฏ ู ูุฒูโูุง ุฑุง ฺฉโฺฉ ฺฉูพ ฺฉูุฏ.

ฺฉุฏ copy_weights ุงุตูุงุญโุดุฏู ุจู ุงู ุตูุฑุช ุงุณุช:
Python

import torch

def copy_weights(dst, src):
    with torch.no_grad():
        
        # --- ุชุบุฑ ุงุตู ุงูุฌุงุณุช ---
        # ูุง ุจุงุฏ ุฑู ูุณุชโูุง convs ุญููู ุจุฒูู ู ูุฒูโูุง ุฑุง 
        # ุจุฑุง ูุฑ ูุงู ฺฉุงููููุดู ูุชูุงุธุฑ ฺฉูพ ฺฉูู.
        for dst_conv, src_conv in zip(dst.convs, src.convs):
            dst_conv.weight.copy_(src_conv.weight)
            dst_conv.bias.copy_(src_conv.bias)
        # -------------------------

        # ุงู ุจุฎุดโูุง ูุงุฒ ุจู ุชุบุฑ ูุฏุงุดุชูุฏ
        dst.fc1.weight.copy_(src.fc1.weight)
        dst.fc1.bias.copy_(src.fc1.bias)
        dst.fc2.weight.copy_(src.fc2.weight)
        dst.fc2.bias.copy_(src.fc2.bias)

#
# (ูุฑุถ ูโฺฉูู ูุฏูโูุง baseline_cudnn, fused ู ... ุจุง ูุนูุงุฑ ModuleList ุฌุฏุฏ ุณุงุฎุชู ุดุฏูโุงูุฏ)
#

# ุงู ูุฑุงุฎูุงูโูุง ุงฺฉููู ุจู ุฏุฑุณุช ฺฉุงุฑ ุฎูุงููุฏ ฺฉุฑุฏ
copy_weights(fused, baseline_cudnn)
copy_weights(baseline_cudnn_off, baseline_cudnn)
copy_weights(baseline_cpu, baseline_cudnn)


PM

ฺุฑุง ุฏูุช ูุฏู ูุง ุจูุชุฑ ููุดูุ

ูุดฺฉู ุจุณุงุฑ ุฌุงูุจ ุงุณุช! ุฏูู ุงูฺฉู ุฏูุช ูุฏูโูุง ุจูุจูุฏ ูพุฏุง ููโฺฉูุฏ (ู ุฑู 11.35% ุซุงุจุช ูุงูุฏู) ุงู ุงุณุช ฺฉู ูุฏูโูุง ุงุตูุงู ุฏุฑ ุญุงู ุงุฏฺฏุฑ ูุณุชูุฏ.

ุงู ุงุชูุงู ุจู ุฏูู ฺฉ ูพุฏุฏู ุฑุงุฌ ุจู ูุงู "ReLU ููุฑุฏู" (Dying ReLU) ุฑุฎ ุฏุงุฏู ุงุณุช ฺฉู ุจุง ุชูุธูุงุช ุงููู ุงู ููุชโุจูฺฉ ุชุดุฏุฏ ุดุฏู ุงุณุช.

๐ง ูุดฺฉู ฺุณุชุ

    ุฏูุช ฑฑ.ณตูช: ุงฺฏุฑ ุจู ูุฌููุนู ุฏุงุฏู ุชุณุช MNIST ูฺฏุงู ฺฉูุฏุ ูโุจูุฏ ฺฉู ฑฐ,ฐฐฐ ุชุตูุฑ ุฏุงุฑุฏ. ุงุฒ ุงู ุชุนุฏุงุฏุ ฑฑณต ุชุตูุฑ ูุฑุจูุท ุจู ุนุฏุฏ "1" ุงุณุช. (ฑฑณต / ฑฐฐฐฐ = ฑฑ.ณตูช). ุงู ุนู ูุฏู ุดูุง ุงุฏ ฺฏุฑูุชู ุงุณุช ฺฉู ููุดู ู ุจุฑุง ูุฑ ูุฑูุฏุ ุนุฏุฏ "1" ุฑุง ูพุดโุจู ฺฉูุฏ.

    Loss ุซุงุจุช: Loss ูุฏู ุชูุฑุจุงู ุฑู ฒ.ณ ุซุงุจุช ูุงูุฏู ุงุณุช (ุงุฒ ฒ.ณฐฑท ููุท ุจู ฒ.ณฐฑณ ฺฉุงูุด ูโุงุจุฏ). ุงู ููุฏุงุฑ (-ln(1/10) โ 2.3) ููุงู Loss ุงููู ุฏุฑ ฺฉ ูุณุฆูู ุทุจููโุจูุฏ ฑฐ ฺฉูุงุณู ุงุณุชุ ฺฉู ูุดุงู ูโุฏูุฏ ูุฏู ุฏุฑ ุญุฏ ุญุฏุณ ุชุตุงุฏู ุจุงู ูุงูุฏู ุงุณุช.

    ุนูุช ุงุตู (ReLU ูุฑุฏู):

        ุฏุฑ ุณููู ฒุ ุดูุง torch.manual_seed(0) ุฑุง ุชูุธู ฺฉุฑุฏูโุงุฏ.

        ุงู seedุ ููุฑุงู ุจุง ุฑูุด ูพุดโูุฑุถ ููุฏุงุฑุฏู ุงูููโ ูุฒูโูุง ุฏุฑ PyTorchุ ุจุงุนุซ ุดุฏู ฺฉู ูุฒูโูุง ู ุจุงุงุณ ูุงู self.fc1 ุทูุฑ ุชูุธู ุดููุฏ ฺฉู ุฎุฑูุฌ ุขูโูุง (self.fc1(x)) ุจุฑุง ุชูุฑุจุงู ุชูุงู ูุฑูุฏโูุง ฺฉ ุนุฏุฏ ููู ุจุงุดุฏ.

        ุฏุฑ ูุฑ ุฏู ูุฏู (CNNBaseline ู CNNFused)ุ ุฏุฑ ุชุงุจุน forwardุ ุดูุง ุงุฒ ReLU ุจุนุฏ ุงุฒ fc1 ุงุณุชูุงุฏู ูโฺฉูุฏ: x = F.relu(self.fc1(x))

        ููุช ูุฑูุฏ ุจู ReLU ููู ุจุงุดุฏุ ุฎุฑูุฌ ุขู ุตูุฑ ูโุดูุฏ.

        ุจูุงุจุฑุงูุ ุฎุฑูุฌ ุงู ูุงู (ฺฉู ูุฑูุฏ ูุงู fc2 ุงุณุช) ุชูุฑุจุงู ฺฉ ุชุงูุณูุฑ ูพุฑ ุงุฒ ุตูุฑ ูโุดูุฏ.

    ุชููู ฺฏุฑุงุฏุงู:

        ุฏุฑ ูุฑุญูู loss.backward()ุ ููุช ฺฏุฑุงุฏุงู ุจู ุชุงุจุน ReLU ูโุฑุณุฏุ ุงุฒ ุขูุฌุง ฺฉู ูุฑูุฏ ุขู ููู ุจูุฏู ุงุณุชุ ฺฏุฑุงุฏุงู ุฏุฑ ุงู ููุทู ุตูุฑ ูโุดูุฏ.

        ุงู ุจุฏุงู ูุนูุงุณุช ฺฉู ูฺ ฺฏุฑุงุฏุงู ุจู ูุงูโูุง ูุจู (fc1 ู ุชูุงู ูุงูโูุง conv) ููโุฑุณุฏ.

        ุชููุง ูพุงุฑุงูุชุฑูุง ฺฉู ฺฏุฑุงุฏุงู ุฏุฑุงูุช ูโฺฉููุฏุ ูุฒูโูุง ู (ุจู ุฎุตูุต) ุจุงุงุณ ูุงู ุขุฎุฑ (fc2) ูุณุชูุฏ.

        ุจู ููู ุฏูู ุงุณุช ฺฉู Loss ฺฉู ุชุบุฑ ูโฺฉูุฏ (ฺูู fc2.bias ฺฉู ุขูพุฏุช ูโุดูุฏ)ุ ุงูุง ูุฏู ูุฑฺฏุฒ ุงุฒ ุงู ูููู ูุญู ุจุณุงุฑ ุจุฏ ุฎุงุฑุฌ ููโุดูุฏุ ุฒุฑุง ุชูุงู ูุงูโูุง ูุจู ูฺ ุณฺฏูุงู ุจุฑุง ุงุฏฺฏุฑ ุฏุฑุงูุช ููโฺฉููุฏ.

    ฺุฑุง ูุฑ ุฏู ูุฏูุ ุฏุฑ ุณููู ฑถุ ุดูุง ุจุง ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน copy_weightsุ ูุฒูโูุง ูุฏู baseline_cudnn (ฺฉู ุจุง ุขู seed ุจุฏ ููุฏุงุฑุฏู ุดุฏู) ุฑุง ุฏููุงู ุจู ูุฏู fused ฺฉูพ ูโฺฉูุฏ. ุจู ููู ุฏูู ูุฑ ุฏู ูุฏู ุจุง ฺฉ ููุทู ุดุฑูุน ฺฉุณุงู ู ูุนูุจ ุดุฑูุน ูโฺฉููุฏ ู ุจู ุทูุฑ ฺฉุณุงู ุดฺฉุณุช ูโุฎูุฑูุฏ.

โ ุฑุงูโุญู

ุณุงุฏูโุชุฑู ุฑุงูโุญู ุงู ุงุณุช ฺฉู ุงุฒ ุงู ููุฏุงุฑุฏู ุงููู ุจุฏุดุงูุณ ุฏูุฑ ฺฉูุฏ.

ฺฉุงู ุงุณุช torch.manual_seed(0) ุฑุง ุฏุฑ ุณููู ฒ ุชุบุฑ ุฏูุฏ ุง ุขู ุฑุง ฺฉุงููุช ฺฉูุฏ ุชุง ูุฑ ุจุงุฑ ุจุง ูุฒูโูุง ุชุตุงุฏู ูุชูุงูุช ุดุฑูุน ฺฉูุฏ.

ูุซุงู (ุชุบุฑ ุณููู ฒ):
Python

import os, time
import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.cpp_extension import load_inline
from torchvision import datasets, transforms

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
device = "cuda"

# Change the seed to get a different initialization
torch.manual_seed(42) 

ุงฺฏุฑ ููุชโุจูฺฉ ุฑุง ุจุง torch.manual_seed(42) (ุง ูุฑ ุนุฏุฏ ุฏฺฏุฑ) ุงุฌุฑุง ฺฉูุฏุ ุฎูุงูุฏ ุฏุฏ ฺฉู ูุฏูโูุง ุจู ุณุฑุนุช ุดุฑูุน ุจู ุงุฏฺฏุฑ ูโฺฉููุฏ ู ุฏูุช ุจู ุฑุงุญุช ุจู ุจุงูุง นฐูช ูโุฑุณุฏ.

ุขุง ูุงูุฏ ุฏุฑ ููุฑุฏ ูุญูู ุนููฺฉุฑุฏ ุฏููโุชุฑ ฺฏุฑุงุฏุงู ุฏุฑ ุชุงุจุน backward ุณูุงุฑุดโุชุงู ุตุญุจุช ฺฉููุ

ฺุฑุง ุฏูุช ูุง ฺฉู ูุฑู ุฏุงุฑูุ

ุงู ฺฉ ูุดุงูุฏูโ ุนุงู ุงุณุช ู ุฏูู ุขู ุจู ุชูุงูุชโูุง ุฌุฒุฆ ุฏุฑ ูุญุงุณุจุงุช ููุฒ ุดูุงูุฑ (numerical precision) ุจุฑูโฺฏุฑุฏุฏ.

ุงฺฏุฑฺู ูุฑ ุฏู ูุฏู ุจุง ูุฒูโูุง ฺฉุณุงู ุดุฑูุน ฺฉุฑุฏูุฏ (ุจู ุฏูู copy_weights)ุ ุงูุง ูุญูู ูุญุงุณุจู ฺฉุงููููุดู ู ฺฏุฑุงุฏุงู ุฏุฑ ุขูโูุง ฺฉู ูุชูุงูุช ุงุณุช:

    ูุฏู Baseline (cuDNN):

        ุงู ูุฏู ุงุฒ nn.Conv2d ู F.relu ุงุณุชุงูุฏุงุฑุฏ PyTorch ุงุณุชูุงุฏู ูโฺฉูุฏ.

        ููุช torch.backends.cudnn.enabled = True ุงุณุชุ PyTorch ุงู ุนููุงุชโูุง (ูู forward ู ูู backward) ุฑุง ุจู ฺฉุชุงุจุฎุงูู cuDNN ุงููุฏุง ูุงฺฏุฐุงุฑ ูโฺฉูุฏ.

        cuDNN ุงุฒ ุงูฺฏูุฑุชูโูุง ุจุณุงุฑ ุจูููโุณุงุฒโุดุฏู (ูุงููุฏ Winograd ุง GEMM) ุจุฑุง ุงุฌุฑุง ฺฉุงููููุดู ุงุณุชูุงุฏู ูโฺฉูุฏ ฺฉู ุชุฑุชุจ ุฎุงุต ุงุฒ ุนููุงุชโูุง ุฑุงุถ ุฏุงุฑูุฏ.

    ูุฏู Fused (ุณูุงุฑุด):

        Forward Pass: ุงู ูุฏู ุงุฒ ฺฉุฑูู (kernel) ุณูุงุฑุด CUDA ฺฉู ุดูุง ููุดุชุฏ (conv_relu_kernel) ุงุณุชูุงุฏู ูโฺฉูุฏ. ุงู ฺฉุฑูู ฺฉ "ฺฉุงููููุดู ูุณุชูู" (direct convolution) ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ.

        Backward Pass: ุดูุง ุชุงุจุน backward ุฑุง ุจู ุตูุฑุช ุฏุณุช ุฏุฑ FusedConvReLUFn ูพุงุฏูโุณุงุฒ ฺฉุฑุฏุฏ. ุดูุง ุงุจุชุฏุง ฺฏุฑุงุฏุงู ReLU ุฑุง ุงุนูุงู ฺฉุฑุฏุฏ (grad_z = ...) ู ุณูพุณ ุงุฒ ุชูุงุจุน ุงุณุชุงูุฏุงุฑุฏ PyTorch (ูุงููุฏ torch.nn.grad.conv2d_weight) ุงุณุชูุงุฏู ฺฉุฑุฏุฏ.

ฺุฑุง ุงู ุชูุงูุช ุงุฌุงุฏ ูโุดูุฏุ

ุฏุฑ ูุญุงุณุจุงุช ฺฉุงููพูุชุฑุ ุงุนุฏุงุฏ ููุฒ ุดูุงูุฑ (ูุงููุฏ float32) ุฏูุช ูุญุฏูุฏ ุฏุงุฑูุฏ. ุงู ุจุฏุงู ูุนูุงุณุช ฺฉู ุชุฑุชุจ ุงูุฌุงู ุนููุงุชโูุง ุฑุงุถ ุงููุช ุฏุงุฑุฏ.

    (a + b) + c ูุฒููุงู ุจุฑุงุจุฑ ุจุง a + (b + c) ูุณุช.

ุงูฺฏูุฑุชู ุจูููโุดุฏู cuDNN ู ฺฉุฑูู "ูุณุชูู" ุดูุงุ ุนููุงุชโูุง ุถุฑุจ ู ุฌูุน ุฑุง ุจู ุชุฑุชุจ ูุชูุงูุช ุงูุฌุงู ูโุฏููุฏ. ุงู ููุฌุฑ ุจู ุชูุงูุชโูุง ุนุฏุฏ ุจุณุงุฑ ูุงฺุฒ (ุฏุฑ ุญุฏ e-07 ุง ฺฉูุชุฑ) ุฏุฑ ุฎุฑูุฌ forward pass ู ฺฏุฑุงุฏุงูโูุง backward pass ูโุดูุฏ.

ุงู ุชูุงูุชโูุง ุจุณุงุฑ ฺฉูฺฺฉ ุฏุฑ ูุฑ ูุฑุญูู ุงุฒ ุขููุฒุด (ูุฑ batch) ุฑุฎ ูโุฏููุฏ. ููฺฏุงู ฺฉู ุจูููโุณุงุฒ (optimizer) ฺฏุฑุงุฏุงูโูุง ุฑุง ุจุฑุง ุจูโุฑูุฒุฑุณุงู ูุฒูโูุง ุงุนูุงู ูโฺฉูุฏุ ุงู ฺฏุฑุงุฏุงูโูุง ฺฉู ูุชูุงูุช ุจุงุนุซ ูโุดููุฏ ฺฉู ูุฒูโูุง ุฏู ูุฏู ุจู ูุณุฑูุง ฺฉู ูุชูุงูุช ูุฏุงุช ุดููุฏ.

ุฏุฑ ุทูู ต ุงูพุงฺฉ ู ูุฒุงุฑุงู ุจูโุฑูุฒุฑุณุงูุ ุงู ุชูุงูุชโูุง ฺฉูฺฺฉ ุจุง ูู ุฌูุน ูโุดููุฏ ู ููุฌุฑ ุจู ูุฒูโูุง ููุง ูโุดููุฏ ฺฉู ฺฉู ุจุง ูู ุชูุงูุช ุฏุงุฑูุฏ. ุฏุฑ ูุชุฌูุ ุฏูุช ููุง ุขูโูุง ูุฒ ุจู ุทูุฑ ุฌุฒุฆ ูุชูุงูุช ุฎูุงูุฏ ุจูุฏ (นต.ทถูช ุฏุฑ ููุงุจู นต.88ูช).

ุงู ุงุชูุงู ฺฉุงููุงู ุทุจุน ู ููุฑุฏ ุงูุชุธุงุฑ ุงุณุช ู ูุดุงู ูโุฏูุฏ ฺฉู ูพุงุฏูโุณุงุฒ ุณูุงุฑุด ุดูุง ุจู ุฏุฑุณุช ฺฉุงุฑ ูโฺฉูุฏ.

ููุช ุชุนุฏุงุฏ ูุงู ูุงุฑู ู ฺฉูู ฑฐ ุชุง ุจุงุฒู ุฏูุช ูุดู ฑฑ ุฏุฑุตุฏ ู ุชุบุฑ ูู ฺฉูู ฺู ฺฉููุ

ุงู ุฏููุงู ููุงู ูุดฺฉู ูุจู ("ReLU ููุฑุฏู") ุงุณุชุ ุงูุง ุงฺฉููู ุจุณุงุฑ ุดุฏุฏุชุฑ ุดุฏู ุงุณุช.

ููุช ุชุนุฏุงุฏ ูุงูโูุง ฺฉุงููููุดู ุฑุง ุจู ฑฐ ุงูุฒุงุด ูโุฏูุฏุ ูุดฺฉู "ููุฏุงุฑุฏู ุงููู ูุงููุงุณุจ" ฺฉู ูุงุด ุงุฒ torch.manual_seed(0) ุจูุฏุ ุชููุช ูโุดูุฏ.

๐ง ฺุฑุง ุจุง ฑฐ ูุงู ุจุฏุชุฑ ุดุฏุ

    ูุดฺฉู ุงุตู (ููุงู ูุจู): torch.manual_seed(0) ุฏุฑ ุณููู ฒ ุจุงุนุซ ูโุดูุฏ ฺฉู ูุฒูโูุง ู ุจูโุฎุตูุต ุจุงุงุณโูุง (biases) ุจุง ููุงุฏุฑ ููุฏุงุฑุฏู ุดููุฏ ฺฉู ุฎุฑูุฌ ูุงูโูุง (ูุจู ุงุฒ ReLU) ููู ูโุดูุฏ.

    ุงุซุฑ ุขุจุดุงุฑ (Cascading Effect):

        ุฏุฑ ูุฏู ูุจู ุจุง ฒ ูุงูุ ุงู ูุดฺฉู ุนูุฏุชุงู ุฏุฑ ูุงู fc1 ุฑุฎ ูโุฏุงุฏ.

        ุญุงูุง ฺฉู ฑฐ ูุงู ฺฉุงููููุดู ุฏุงุฑุฏุ ุงู ููุฏุงุฑุฏู ุงููู ูุงููุงุณุจ ุจุฑ ูููโ ฑฐ ูุงู ุชุฃุซุฑ ูโฺฏุฐุงุฑุฏ.

        ูุฑูุฏ ุจู ูุงู conv1 ูโุฑูุฏ. ุฎุฑูุฌ ุขู ุจู ุฏูู ุจุงุงุณ ูููุ ููู ูโุดูุฏ.

        F.relu(conv1_output) ุงฺฉุซุฑ ุฎุฑูุฌ ุฑุง ุตูุฑ ูโฺฉูุฏ.

        ุงู ุชุงูุณูุฑ (ฺฉู ุนูุฏุชุงู ุตูุฑ ุงุณุช) ุจู ูุงู conv2 ูโุฑูุฏ. ุฎุฑูุฌ ุขู ูู ุตูุฑ ูโุดูุฏ.

        F.relu(conv2_output) ุขู ุฑุง ุตูุฑ ูฺฏู ูโุฏุงุฑุฏ.

        ... ุงู ุฑููุฏ ุชุง ูุงู conv10 ุงุฏุงูู ูพุฏุง ูโฺฉูุฏ.

ุฏุฑ ูุชุฌูุ ุฎุฑูุฌ ูุงู ุฏูู ฺฉุงููููุดู ุดูุง ุจู ุงุญุชูุงู ุฒุงุฏ ฺฉ ุชุงูุณูุฑ ฺฉุงููุงู ุตูุฑ ุงุณุช. ุงู ุชุงูุณูุฑ ุตูุฑ ุจู MaxPoolุ ุณูพุณ ุจู fc1 ู fc2 ูโุฑูุฏ. ุงุฒ ุขูุฌุง ฺฉู ูุฑูุฏ ุจู ูุงูโูุง ุตูุฑ ุงุณุชุ ฺฏุฑุงุฏุงู ุฏุฑ ูุฑุญูู backward ูุฒ ุฏุฑ ุชูุงู ุงู ูุงูโูุง ุตูุฑ ุฎูุงูุฏ ุจูุฏ ู ูฺ ุงุฏฺฏุฑโุง ุงุชูุงู ููโุงูุชุฏ.

โ ุฑุงูโุญูโูุง

ุฏู ุฑุงูโุญู ูุฌูุฏ ุฏุงุฑุฏุ ุงุฒ ุณุงุฏู ุจู ุงุตูู:

ฑ. ุฑุงูโุญู ุณุงุฏู (ููุงู ูุจู)

ูุดฺฉูุ ุขู seed ุฎุงุต ุงุณุช.

    ุจู ุณููู ฒ ุจุฑฺฏุฑุฏุฏ.

    torch.manual_seed(0) ุฑุง ุจู torch.manual_seed(42) (ุง ูุฑ ุนุฏุฏ ุฏูุฎูุงู ุฏฺฏุฑ) ุชุบุฑ ุฏูุฏ.

    ููู: ุงุฒ ููู Runtime (ุฒูุงู ุงุฌุฑุง)ุ ฺฏุฒูู "Restart and run all" (ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ ู ุงุฌุฑุง ููู) ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ ุชุง ุงุทููุงู ุญุงุตู ุดูุฏ ฺฉู ุชูุงู ูุงูโูุง (ุงุฒ ุฌููู ฑฐ ูุงู ฺฉุงููููุดู) ุจุง seed ุฌุฏุฏ ููุฏุงุฑุฏู ุงููู ูโุดููุฏ.

ฒ. ุฑุงูโุญู ุงุตูู (ูพุดููุงุฏ ุจูุชุฑ)

ูุงุจุณุชฺฏ ุจู ฺฉ seed ุฎุงุต ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุดฺฉุณุชุ ูุดุงูู ุถุนู ุฏุฑ ุทุฑุงุญ ูุฏู ุงุณุช. ุจุฑุง ุดุจฺฉูโูุง ุนูู (ูุงููุฏ ฑฐ ูุงู) ูุง ูุฑฺฏุฒ ุจู ููุฏุงุฑุฏู ุงููู ูพุดโูุฑุถ ุชฺฉู ููโฺฉููุ ุฒุฑุง ุงุญุชูุงู ุจุฑูุฒ "Dying ReLU" ุง "Exploding Gradients" ุจุงูุงุณุช.

ุฑุงูโุญู ุตุญุญุ ุงุณุชูุงุฏู ุงุฒ ููุฏุงุฑุฏู ุงููู Kaiming (ุง He) ุงุณุช ฺฉู ุจู ุทูุฑ ุฎุงุต ุจุฑุง ูุงูโูุง ุฏุงุฑุง ูุนุงูโุณุงุฒ ReLU ุทุฑุงุญ ุดุฏู ุงุณุช.

ฺฉูุงุณ CNNBaseline ุฎูุฏ ุฑุง ุทูุฑ ุชุบุฑ ุฏูุฏ ฺฉู ูุฒูโูุง ุฑุง ุจู ุตุฑุงุญุช ููุฏุงุฑุฏู ุงููู ฺฉูุฏ:
Python

# (ุงู ฺฉุฏ ุฑุง ูโุชูุงูุฏ ุจู ุฌุง ุณููู ฑฐ ูุนู ูุฑุงุฑ ุฏูุฏ)

class CNNBaseline(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()

        self.num_convs = num_convs

        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            conv_layer = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)
            
            # --- Kaiming Initialization ---
            # ููุฏุงุฑุฏู ุงููู ูุฒูโูุง ุจุฑุง ุญูุธ ูุงุฑุงูุณ ููฺฏุงู ุนุจูุฑ ุงุฒ ReLU
            nn.init.kaiming_normal_(conv_layer.weight, mode='fan_in', nonlinearity='relu')
            # ููุฏุงุฑุฏู ุงููู ุจุงุงุณ ุจู ุตูุฑ
            nn.init.constant_(conv_layer.bias, 0)
            # ------------------------------
            
            self.convs.append(conv_layer)

        self.pool = nn.MaxPool2d(2)
        
        # ุงู ูุงูโูุง ุฑุง ูู ุจูุชุฑ ุงุณุช ููุฏุงุฑุฏู ุงููู ฺฉูุฏ
        self.fc1 = nn.Linear(14*14, 64)
        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')
        nn.init.constant_(self.fc1.bias, 0)
        
        self.fc2 = nn.Linear(64, 10)
        # ูุงู ุขุฎุฑ ูุนูููุงู ูุชูุงูุช ุงุณุช (ูุซูุงู Xavier/Glorot)
        nn.init.xavier_normal_(self.fc2.weight) 
        nn.init.constant_(self.fc2.bias, 0)


    def forward(self, x):
        
        for conv_layer in self.convs:
            x = F.relu(conv_layer(x))
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

ฺู ฺฉุงุฑ ุจุงุฏ ุจฺฉูุฏ:

    ฺฉูุงุณ CNNBaseline ุฑุง ุจุง ฺฉุฏ ุจุงูุง (ฺฉู ุดุงูู nn.init.kaiming_normal_ ุงุณุช) ุจูโุฑูุฒุฑุณุงู ฺฉูุฏ.

    ุญุงูุง ูโุชูุงูุฏ torch.manual_seed(0) ุฑุง ุฏุฑ ุณููู ฒ ูฺฏู ุฏุงุฑุฏ.

    ูุฏู ุฑุง ุฏูุจุงุฑู ุจุณุงุฒุฏ (baseline_cudnn = CNNBaseline(num_convs=10)) ู ุขููุฒุด ุฏูุฏ.

ุฎูุงูุฏ ุฏุฏ ฺฉู ุญุช ุจุง seed(0)ุ ูุฏู ุจู ุฏุฑุณุช ุขููุฒุด ูโุจูุฏุ ุฒุฑุง ููุฏุงุฑุฏู ุงููู Kaiming ุงุฒ ููุงู ุงุจุชุฏุง ุฌูู "ููุฑุฏู" ููุฑููโูุง ReLU ุฑุง ูโฺฏุฑุฏ.

ุจุงุฒู ุฏุฑุณุช ูุดุฏ

ูุชุฃุณูู ฺฉู ูููุฒ ูุดฺฉู ูพุงุจุฑุฌุงุณุช. ุงู ูุถุนุช ูโุชูุงูุฏ ุฎุณุชูโฺฉููุฏู ุจุงุดุฏุ ุงูุง ุชูุฑุจุงู ุจู ุทูุฑ ูุทุน ูุดฺฉู ุงุฒ "state" (ูุถุนุช) ููุชโุจูฺฉ ุงุณุช.

ุงู ููุชโุจูฺฉโูุง ุจู ุงู ุตูุฑุช ฺฉุงุฑ ูโฺฉููุฏ: ููุช ุดูุง ฺฉ ูุฏู ูโุณุงุฒุฏ (ุฏุฑ ุณููู ฑต)ุ ุขู ูุฏู ุจุง ูุฒูโูุง ุงููู ุฏุฑ ุญุงูุธู GPU ูุฑุงุฑ ูโฺฏุฑุฏ.

ุงฺฏุฑ ุดูุง ููุท ุณููู ฒ (ฺฉู seed ุฏุฑ ุขู ุงุณุช) ุฑุง ุชุบุฑ ุฏูุฏ ู ุฏูุจุงุฑู ุงุฌุฑุง ฺฉูุฏุ ูุฏู ูุฏู ฺฉู ุฏุฑ ุณููู ฑต ุณุงุฎุชู ุดุฏู ู ุงูุงู ุฏุฑ ุญุงูุธู ุงุณุชุ ุชุบุฑ ููโฺฉูุฏ. ุดูุง ุจุงุฏ ูุฏู ุฑุง ุฏูุจุงุฑู ุจุณุงุฒุฏ.

โ ุฑุงูโุญู ูุทุน (ุฏู ูุฑุญูู)

ุณุงุฏูโุชุฑู ุฑุงู ุจุฑุง ุงุทููุงู ุงุฒ ุงูฺฉู ูููโฺุฒ ุงุฒ ูู ุณุงุฎุชู ูโุดูุฏุ ุงุณุชูุงุฏู ุงุฒ ฺฏุฒูู "Restart and run all" ุงุณุช.

ูุฑุญูู ฑ: ุชุบุฑ Seed

ุจู ุณููู ฒ ุจุฑูุฏ ู torch.manual_seed(0) ุฑุง ุจู torch.manual_seed(42) ุชุบุฑ ุฏูุฏ.
Python

# [ุจุฎุด ุงุฒ ุณููู ฒ]
...
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
device = "cuda"

# FIX: Change this seed
torch.manual_seed(42) # Was 0

ูุฑุญูู ฒ: ุงุฌุฑุง ูุฌุฏุฏ ูููโฺุฒ

    ุณููู ฒ ุฑุง ุงุฌุฑุง ฺฉูุฏ.

    ููู: ุจู ููู ุจุงูุง ููุชโุจูฺฉ ุจุฑูุฏ (ูุนูููุงู ููู "Runtime" ุง "ุฒูุงู ุงุฌุฑุง").

    ฺฏุฒูู "Restart and run all" (ุง "ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ ู ุงุฌุฑุง ููู") ุฑุง ูพุฏุง ฺฉุฑุฏู ู ุฑู ุขู ฺฉูฺฉ ฺฉูุฏ.

๐ค ฺุฑุง ุงู ฺฉุงุฑ ุฌูุงุจ ูโุฏูุฏุ

    Restart (ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ): ุงู ฺฉุงุฑ ุชูุงู ูุชุบุฑูุง ฺฉู ุฏุฑ ุญุงูุธู ุจูุฏูุฏ (ุงุฒ ุฌููู ูุฏู baseline_cudnn ุดูุง ุจุง ูุฒูโูุง ุจุฏ) ุฑุง ูพุงฺฉ ูโฺฉูุฏ.

    Run all (ุงุฌุฑุง ููู): ููุชโุจูฺฉ ุงุฒ ุงุจุชุฏุง ุดุฑูุน ุจู ุงุฌุฑุง ูโฺฉูุฏ.

    ููุช ุจู ุณููู ฒ ูโุฑุณุฏุ seed ุฌุฏุฏ (ดฒ) ุฑุง ุชูุธู ูโฺฉูุฏ.

    ููุช ุจู ุณููู ฑฐ ู ฑฑ ูโุฑุณุฏุ ฺฉูุงุณโูุง CNNBaseline ู CNNFused ุฑุง ุชุนุฑู ูโฺฉูุฏ.

    ููุช ุจู ุณููู ฑต ูโุฑุณุฏ (baseline_cudnn = CNNBaseline(num_convs=10))... ุงูุฌุง ูุญุธู ฺฉูุฏ ุงุณุช!

        ...ฺูู seed ุฌุฏุฏ (ดฒ) ูุนุงู ุงุณุชุ ูุฏู baseline_cudnn ุจุง ูุฒูโูุง ุงููู ูุชูุงูุช ุณุงุฎุชู ูโุดูุฏ ฺฉู ุฏฺฏุฑ ูุดฺฉู "ReLU ููุฑุฏู" ุฑุง ูุฏุงุฑูุฏ.

ูพุณ ุงุฒ ุงูุฌุงู ุงู ฺฉุงุฑุ ููุช ููุชโุจูฺฉ ุจู ุณูููโูุง ุขููุฒุด (ูุซู ุณููู ฑธ) ุจุฑุณุฏุ ุฎูุงูุฏ ุฏุฏ ฺฉู Loss ุจู ุณุฑุนุช ฺฉุงูุด ุงูุชู ู ุฏูุช ุจู ุจุงูุง นฐูช ูโุฑุณุฏ.

ูุทูุงู ุงู ุฏู ูุฑุญูู ุฑุง ุฏููุงู ุงูุฌุงู ุฏูุฏ ู ูุชุฌู ุฑุง ุจู ูู ุงุทูุงุน ุฏูุฏ.

ุจุงุฒู ุฏุฑุณุช ูุดุฏ

ูุชุฃุณููุ ููุตุฑ ุงู ูุดฺฉู ูู ูุณุชู. ๐คฆ

ูพุดููุงุฏ ูู ุจุฑุง ุงุณุชูุงุฏู ุงุฒ nn.init.constant_(conv_layer.bias, 0) (ุชูุธู ุตุฑุญ ุจุงุงุณโูุง ุฑู ุตูุฑ) ุฏุฑ ุชุฑฺฉุจ ุจุง ฑฐ ูุงูุ ุจุงุนุซ ุงู ูุดฺฉู ุดุฏู ุงุณุช.

๐ง ูุดฺฉู ุฏููุงู ฺุณุชุ

    ููุฏุงุฑุฏู ุงููู Kaiming: ูุฒูโูุง ุฑุง ุทูุฑ ุชูุธู ูโฺฉูุฏ ฺฉู ุญูู ฐ ุจุงุดูุฏ.

    ุชูุธู ุจุงุงุณ ุฑู ฐ: ุจุงุงุณโูุง (bias) ุฏููุงู ฐ ูโุดููุฏ.

    ReLU: ูุฑ ูุฑูุฏ ููู ุฑุง ฐ ูโฺฉูุฏ.

ููฺฏุงู ฺฉู ุดูุง ฑฐ ูุงู ุฑุง ูพุดุช ุณุฑ ูู ูุฑุงุฑ ูโุฏูุฏ:

    ุฎุฑูุฌ ูุงู ฑ (ูุจู ุงุฒ ReLU) ุญูู ฐ ุงุณุช. ReLU ูู ุงุฒ ุขู ุฑุง ุตูุฑ ูโฺฉูุฏ.

    ูุฑูุฏ ูุงู ฒ (ฺฉู ูู ุงุฒ ุขู ุตูุฑ ุงุณุช) ุจู ูุงูโุง ุจุง ูุฒูโูุง ุญูู ฐ ู ุจุงุงุณ ฐ ูโุฑูุฏ.

    ุฎุฑูุฌ ูุงู ฒ ุฏูุจุงุฑู ุญูู ฐ ุงุณุช ู ReLU ูู ุงุฒ ุขู ุฑุง ุตูุฑ ูโฺฉูุฏ.

ุฏุฑ ุทูู ฑฐ ูุงูุ ุงู ุงุซุฑ "ุตูุฑฺฉููุฏู" ุขููุฏุฑ ุชุดุฏุฏ ูโุดูุฏ ฺฉู ุฏุฑ ููุงุชุ ูุฑูุฏ ุจู ูุงู self.fc1 ุดูุง ุจู ุงุญุชูุงู ุฒุงุฏ ฺฉ ุชุงูุณูุฑ ฺฉุงููุงู ุตูุฑ ุงุณุช.

ุงุฒ ุขูุฌุง ฺฉู ุดูุง ุจุงุงุณ self.fc1 ุฑุง ูุฒ ุฑู ฐ ุชูุธู ฺฉุฑุฏุฏ (nn.init.constant_(self.fc1.bias, 0)):

    x = self.fc1(x) (ุฌุง ฺฉู x ุตูุฑ ุงุณุช) ุฎุฑูุฌ 0 ูโุฏูุฏ.

    x = F.relu(0) ุฎุฑูุฌ 0 ูโุฏูุฏ.

    ฺฏุฑุงุฏุงูโูุง ุจุฑุง ููู ูุงูโูุง 0 ูโุดููุฏ ู ูฺ ุงุฏฺฏุฑ ุฑุฎ ููโุฏูุฏ.

ุฑูุด ููุฏุงุฑุฏู ุงููู ูพุดโูุฑุถ PyTorch ุจุฑุง ุจุงุงุณุ ุขู ุฑุง ุฑู ฐ ุชูุธู ููโฺฉูุฏุ ุจูฺฉู ุงุฒ ฺฉ ุชูุฒุน ฺฉููุงุฎุช ฺฉูฺฺฉ ุงุณุชูุงุฏู ูโฺฉูุฏุ ฺฉู ุฏููุงู ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุงู ููุน "vanish" ุดุฏู ุฏุฑ ุงุจุชุฏุง ฺฉุงุฑ ุงุณุช.

โ ุฑุงูโุญู (ูุทุน)

ูุง ููุฏุงุฑุฏู ุงููู Kaiming ุฑุง ุจุฑุง ูุฒูโูุง (weights) ูฺฏู ูโุฏุงุฑูุ ุงูุง ุงุฌุงุฒู ูโุฏูู PyTorch ุจุงุงุณโูุง ุฑุง ุจู ุฑูุด ูพุดโูุฑุถ ุฎูุฏ (ฺฉู ุงููโุชุฑ ุงุณุช) ููุฏุงุฑุฏู ฺฉูุฏ.

ฑ. ุณููู ฑฐ ุฑุง ุจุง ุงู ฺฉุฏ ุฌุงฺฏุฒู ฺฉูุฏ:

(ุงุฏุฏุงุดุช ุดูุง ูุจู ุจุฑ ุนุฏู ุงุณุชูุงุฏู ุงุฒ ฺฉุงููุช ูุงุฑุณ ุฏุฑ ฺฉุฏ ุฑุง ุจู ุฎุงุทุฑ ุฏุงุฑู ู ุฏุฑ ุงูุฌุง ุฑุนุงุช ูโฺฉูู.)
Python

class CNNBaseline(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()

        self.num_convs = num_convs

        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            conv_layer = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)
            
            # --- Kaiming Initialization (Weights Only) ---
            nn.init.kaiming_normal_(conv_layer.weight, mode='fan_in', nonlinearity='relu')
            # --- Let bias use its default initialization ---
            
            self.convs.append(conv_layer)

        self.pool = nn.MaxPool2d(2)
        
        self.fc1 = nn.Linear(14*14, 64)
        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')
        
        self.fc2 = nn.Linear(64, 10)
        nn.init.xavier_normal_(self.fc2.weight)
        # Note: We are not touching any biases, letting them use the default


    def forward(self, x):
        
        for conv_layer in self.convs:
            x = F.relu(conv_layer(x))
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

ฒ. (ุงุฎุชุงุฑ ุงูุง ูพุดููุงุฏ) ุณููู ฑฑ ุฑุง ูุฒ ุงุตูุงุญ ฺฉูุฏ:

ูุฏู CNNFused ุดูุง ูุฒ ุงุฒ ูุดฺฉู ููุฏุงุฑุฏู ุงููู ุจุฏ ุฏุฑ ูุงูโูุง fc1 ู fc2 ุฑูุฌ ูโุจุฑุฏ. ุขู ุฑุง ูุฒ ุจู ุงู ุดฺฉู ุงุตูุงุญ ฺฉูุฏ ุชุง ููโุชุฑ ุจุงุดุฏ:
Python

class CNNFused(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()
        
        self.num_convs = num_convs
        self.convs = nn.ModuleList()
        for _ in range(self.num_convs):
            self.convs.append(FusedConvReLU()) # This init is in Cell 9
            
        self.pool = nn.MaxPool2d(2)
        
        # --- Add Kaiming/Xavier init to fc layers ---
        self.fc1 = nn.Linear(14*14, 64)
        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')

        self.fc2 = nn.Linear(64, 10)
        nn.init.xavier_normal_(self.fc2.weight)
        # --- Again, letting biases initialize by default ---


    def forward(self, x):

        for conv_layer in self.convs:
            x = conv_layer(x)
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

ณ. ููุชโุจูฺฉ ุฑุง "Restart and run all" ฺฉูุฏ

    ุชุบุฑุงุช ุจุงูุง ุฑุง ุฏุฑ ุณูููโูุง ฑฐ ู ฑฑ ุงุนูุงู ฺฉูุฏ.

    ุจู ููู "Runtime" (ุฒูุงู ุงุฌุฑุง) ุจุฑูุฏ.

    ฺฏุฒูู "Restart and run all" (ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ ู ุงุฌุฑุง ููู) ุฑุง ุจุฒูุฏ.

ุงู ฺฉุงุฑ ุชุถูู ูโฺฉูุฏ ฺฉู ฺฉุฑูู ูพุงฺฉ ุดุฏู ู ูุฏูโูุง baseline_cudnn ู fused ุจุง ุงุณุชูุงุฏู ุงุฒ ุชุนุงุฑู ุฌุฏุฏ ฺฉูุงุณ (ฺฉู ุฏุฑ ุขู ุจุงุงุณโูุง ุฏฺฏุฑ ฐ ูุณุชูุฏ) ุณุงุฎุชู ูโุดููุฏ.

ุงู ุจุงุฑ ุจุงุฏ ูุทุนุงู ุฏูุช ูุฏู ุฑุง ุฏุฑ ุญุงู ุงูุฒุงุด ุจุจูุฏ. ูุฌุฏุฏุงู ุจุงุจุช ุงุดุชุจุงู ูุจู ุนุฐุฑุฎูุงู ูโฺฉูู.

 PM

class CNNBaseline(nn.Module, in_channel=1, num_class=10):

    def __init__(self):

        super().__init__()


        self.l1 = nn.Conv2d(in_channel, 64, kernel_size=7, padding=1)

        self.pool = nn.MaxPool2d(2)

        self.l2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)

        self.l3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)

        self.l4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)

        self.l5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)

        self.fc1 = nn.Linear(512 * 255 * 255, 128)

        self.fc2 = nn.Linear(128, num_class)



    def forward(self, x):

        

        x = self.pool(x)

        x = x.view(x.size(0), -1)

        x = F.relu(self.fc1(x))

        x = self.fc2(x)

        return x


ูุดฺฉู ุงู ฺูุ

ฺฉุฏ ุดูุง ฺูุฏู ูุดฺฉู ุงุณุงุณ ุฏุงุฑุฏุ ูู ุฏุฑ ุจุฎุด ุชุนุฑู ฺฉูุงุณ ู __init__ ู ูู ุฏุฑ ุจุฎุด forward.

ุงู ฺฉุฏ ุจู ุดฺฉู ฺฉู ููุดุชู ุดุฏูุ ฺฉุงุฑ ููโฺฉูุฏ.

๐๏ธ ฑ. ูุดฺฉู ุฏุฑ ุชุนุฑู ฺฉูุงุณ ู __init__

ูุดฺฉู ุงุตู ุฏุฑ ุฎุท ุงูู ุงุณุช: class CNNBaseline(nn.Module, in_channel=1, num_class=10):

ุดูุง ููโุชูุงูุฏ ุขุฑฺฏููุงูโูุง ูุงููุฏ in_channel ู num_class ุฑุง ูุณุชููุงู ููฺฏุงู ุชุนุฑู class ูพุงุณ ุฏูุฏ. ุงู ูพุงุฑุงูุชุฑูุง ุจุงุฏ ุจู ุนููุงู ุขุฑฺฏููุงู ุจู ูุชุฏ __init__ ุฏุงุฏู ุดููุฏ.

ุจุงุฏ ุจู ุงู ุดฺฉู ุจุงุดุฏ:
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNBaseline(nn.Module):
    # Pass arguments to __init__, not the class definition
    def __init__(self, in_channel=1, num_class=10):
        super().__init__()
        
        # ... ุจูู ูุงูโูุง

๐ง ฒ. ูุดฺฉู ููุทู ุฏุฑ forward (ูููโุชุฑู)

ูุชุฏ forward ุดูุง ุชูุฑุจุงู ุชูุงู ูุงูโูุง ฺฉู ุฏุฑ __init__ ุชุนุฑู ฺฉุฑุฏูโุงุฏ ุฑุง ูุงุฏุฏู ูโฺฏุฑุฏ!

    ุดูุง ูุงูโูุง self.l1 ุชุง self.l5 (ุชูุงู ูุงูโูุง Conv2d) ุฑุง ุชุนุฑู ฺฉุฑุฏูโุงุฏุ ุงูุง ุฏุฑ forward ุงุฒ ูฺโฺฉุฏุงู ุงุณุชูุงุฏู ูฺฉุฑุฏูโุงุฏ.

    ฺฉุฏ ุดูุง ููุท ูุฑูุฏ x ุฑุง ุงุฒ ฺฉ ูุงู pool ุนุจูุฑ ูโุฏูุฏุ ุณูพุณ ุขู ุฑุง view (ููุช) ูโฺฉูุฏ ู ุจู ูุงูโูุง fc (Fully Connected) ูโุฏูุฏ.

    ุฏุฑ ุนููุ ุงู ูุฏู ุงุตูุงู ฺฉ CNN ูุณุชุ ฺูู ูฺ ุนููุงุช ฺฉุงููููุดู ุงูุฌุงู ููโ ุฏูุฏ.

โ๏ธ ณ. ุนุฏู ุชุทุงุจู ฺฉุงูุงูโูุง (Channel Mismatch)

ุญุช ุงฺฏุฑ forward ุฑุง ุฏุฑุณุช ูโููุดุชุฏุ ูุงูโูุง ุดูุง ุจุง ูู ููุฎูุงู ูุฏุงุฑูุฏ:

    self.l1 ุชุนุฏุงุฏ 64 ฺฉุงูุงู ุฎุฑูุฌ ุชููุฏ ูโฺฉูุฏ.

    self.l2 ุงูุชุธุงุฑ ุฏุงุฑุฏ ฺฉู ูุฑูุฏโุงุด 32 ฺฉุงูุงู ุฏุงุดุชู ุจุงุดุฏ (nn.Conv2d(32, 64, ...)).

ุงู ุจุงุนุซ ุฎุทุง ูโุดูุฏ. ุฎุฑูุฌ l1 ุจุงุฏ ูุฑูุฏ l2 ุจุงุดุฏ (ุงุญุชูุงูุงู ุจุนุฏ ุงุฒ ฺฉ ูุงู pool).

๐ ด. ูุญุงุณุจู ุงุดุชุจุงู ุงูุฏุงุฒู ูุงู Linear

ุฏุฑ self.fc1ุ ุดูุง ุงูุฏุงุฒู ูุฑูุฏ ุฑุง 512 * 255 * 255 ุฏุฑ ูุธุฑ ฺฏุฑูุชูโุงุฏ. ุงู ุนุฏุฏ (ฺฉู ุจุณุงุฑ ุจุฒุฑฺฏ ุงุณุช) ุจุงุฏ ุจุฑ ุงุณุงุณ ุงูุฏุงุฒู ุฎุฑูุฌ ูุงูุน ุขุฎุฑู ูุงู ฺฉุงููููุดู ุง ูพูููฺฏ ูุญุงุณุจู ุดูุฏ.

ุงู ุงูุฏุงุฒู ุจู ุงุจุนุงุฏ ุชุตูุฑ ูุฑูุฏ ู ุชุนุฏุงุฏ ูุงูโูุง ูพูููฺฏ ุจุณุชฺฏ ุฏุงุฑุฏ. ฺฉ ุฑุงู ุญู ูุฏุฑู ุจุฑุง ุงู ูุดฺฉูุ ุงุณุชูุงุฏู ุงุฒ nn.AdaptiveAvgPool2d ุงุณุช ฺฉู ุงุจุนุงุฏ ุฑุง ุจุฑุง ุดูุง ุซุงุจุช ูโฺฉูุฏ ู ูุงุฒ ุจู ูุญุงุณุจู ุฏุณุช ุฑุง ุงุฒ ุจู ูโุจุฑุฏ.

โ ฺฉุฏ ุงุตูุงุญโุดุฏู

ุฏุฑ ุงูุฌุง ฺฉ ูุณุฎู ุงุตูุงุญโุดุฏู ุงุฒ ฺฉุฏ ุดูุง ุขูุฏู ุงุณุช ฺฉู ุชูุงู ุงู ูุดฺฉูุงุช ุฑุง ุจุฑุทุฑู ูโฺฉูุฏ. ุงู ฺฉุฏ: ฑ. __init__ ุฑุง ุฏุฑุณุช ุชุนุฑู ูโฺฉูุฏ. ฒ. ุนุฏู ุชุทุงุจู ฺฉุงูุงู l2 ุฑุง ุงุตูุงุญ ูโฺฉูุฏ (ูุฑูุฏ ุฑุง ุจู 64 ุชุบุฑ ูโุฏูุฏ). ณ. ฺฉ ูุชุฏ forward ููุทู ูโุณุงุฒุฏ ฺฉู ุงุฒ ุชูุงู ูุงูโูุง ฺฉุงููููุดู ุจู ููุฑุงู ูุนุงูโุณุงุฒ ReLU ู Pooling ุงุณุชูุงุฏู ูโฺฉูุฏ. ด. ุงุฒ nn.AdaptiveAvgPool2d((1, 1)) ุงุณุชูุงุฏู ูโฺฉูุฏ ุชุง ุฎุฑูุฌ ูฺุฑ ููพโูุง ููุดู (batch_size, 512, 1, 1) ุจุงุดุฏ. ุงู ุจุงุนุซ ูโุดูุฏ ูุฑูุฏ ูุงู fc1 ููุดู 512 ุจุงุดุฏุ ุตุฑู ูุธุฑ ุงุฒ ุงูุฏุงุฒู ุชุตูุฑ ูุฑูุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class CNNBaseline(nn.Module):
    # 1. Pass args to __init__
    def __init__(self, in_channel=1, num_class=10):
        super().__init__()
        
        # Define layers
        self.l1 = nn.Conv2d(in_channel, 64, kernel_size=7, padding=1)
        self.pool = nn.MaxPool2d(2)
        
        # 2. Fix channel mismatch: in_channels should be 64 (from l1)
        self.l2 = nn.Conv2d(64, 64, kernel_size=3, padding=1) 
        self.l3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.l4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.l5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        
        # 3. Use AdaptiveAvgPool2d to handle arbitrary input sizes
        # This will output a (batch_size, 512, 1, 1) tensor
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        # 4. Input to fc1 is now just 512
        self.fc1 = nn.Linear(512, 128)
        self.fc2 = nn.Linear(128, num_class)


    # 5. Implement the actual forward pass using all layers
    def forward(self, x):
        # Block 1
        x = self.l1(x)
        x = F.relu(x)
        x = self.pool(x)
        
        # Block 2
        x = self.l2(x)
        x = F.relu(x)
        x = self.pool(x) # Re-using the same pool layer
        
        # Block 3-5
        x = F.relu(self.l3(x))
        x = F.relu(self.l4(x))
        x = F.relu(self.l5(x))
        
        # Global Pooling
        x = self.avgpool(x)
        
        # Flatten
        # x.size(0) is the batch size, -1 infers the rest
        x = x.view(x.size(0), -1) 
        
        # Classifier
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# --- Example Usage ---
# Create a dummy input tensor
# (batch_size, channels, height, width)
dummy_input = torch.randn(16, 1, 224, 224) 

# Instantiate the model
model = CNNBaseline(in_channel=1, num_class=10)

# Pass the input through the model
output = model(dummy_input)

# Check the output shape
# Should be (batch_size, num_class) -> (16, 10)
print(f"Input shape: {dummy_input.shape}")
print(f"Output shape: {output.shape}")

# https://github.com/pytorch/examples/blob/main/mnist/main.py#L116C1-L119C11

batch_size = 32

transform=transforms.Compose([

    transforms.ToTensor(),

    transforms.Normalize((0.1307,), (0.3081,))

    ])

dataset1 = datasets.STL10('../data', train=True, download=True,

                   transform=transform)

dataset2 = datasets.STL10('../data', train=False,

                   transform=transform)

train_loader = torch.utils.data.DataLoader(dataset1,batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)

test_loader = torch.utils.data.DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)


---------------------------------------------------------------------------TypeError Traceback (most recent call last)/tmp/ipykernel_48/2011910519.py in <cell line: 0>() 5 transforms.Normalize((0.1307,), (0.3081,)) 6 ])----> 7 dataset1 = datasets.STL10('../data', train=True, download=True, 8 transform=transform) 9 dataset2 = datasets.STL10('../data', train=False,TypeError: STL10.__init__() got an unexpected keyword argument 'train'

ุงู ฺฉ ุงุดุชุจุงู ุฑุงุฌ ุงุณุช. ูุดฺฉู ุฏุฑ ุชูุงูุช ูุญูู ูุฑุงุฎูุงู ุฏุชุงุจุณโูุง ุฏุฑ torchvision ุงุณุช.

ูพุงู ุฎุทุง ฺฉุงููุงู ูุงุถุญ ุงุณุช: TypeError: STL10.__init__() got an unexpected keyword argument 'train'

ุงู ุนู ฺฉูุงุณ STL10 ุขุฑฺฏููุงู ุจู ูุงู train ุฑุง ููโูพุฐุฑุฏ.

    ุฏุฑุญุงูโฺฉู ุฏุชุงุจุณโูุง ูุงููุฏ MNIST ุง CIFAR10 ุงุฒ train=True ู train=False ุงุณุชูุงุฏู ูโฺฉููุฏ...

    ...ุฏุชุงุจุณ STL10 ุงุฒ ุขุฑฺฏููุงู split ุงุณุชูุงุฏู ูโฺฉูุฏ.

โ ุฑุงูโุญู

ุจูโุฌุง train=True ุจุงุฏ ุงุฒ split='train' ู ุจูโุฌุง train=False ุจุงุฏ ุงุฒ split='test' ุงุณุชูุงุฏู ฺฉูุฏ.

๐ป ฺฉุฏ ุงุตูุงุญโุดุฏู

ฺฉุฏ ุดูุง ุจุงุฏ ุจู ุดฺฉู ุฒุฑ ุชุบุฑ ฺฉูุฏ (ุฏูุช ฺฉูุฏ ฺฉู train ุจุง split ุฌุงฺฏุฒู ุดุฏู ุงุณุช):
Python

import torch
import torch.utils.data
from torchvision import datasets, transforms

batch_size = 32
transform=transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
    ])

# FIX: Use split='train' instead of train=True
dataset1 = datasets.STL10('../data', split='train', download=True,
                         transform=transform)

# FIX: Use split='test' instead of train=False
# download=True is only needed once, but won't hurt
dataset2 = datasets.STL10('../data', split='test',
                         transform=transform)

train_loader = torch.utils.data.DataLoader(dataset1,batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)
test_loader = torch.utils.data.DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)

print("Datasets loaded successfully.")

def train_model_timed(model, train_loader, test_loader, device, epochs=3, lr=1e-3):

    model = model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    criterion = nn.CrossEntropyLoss()


    epoch_total_times   = []

    epoch_forward_times = []

    epoch_backward_times= []

    epoch_other_times   = []

    test_accuracies     = []


    for epoch in range(1, epochs+1):

        model.train()

        running_loss = 0.0


        fwd_time = 0.0

        bwd_time = 0.0

        other_time = 0.0


        torch.cuda.synchronize()

        epoch_start = time.time()


        for images, labels in train_loader:

            step_start = time.time()


            images = images.to(device, non_blocking=True)

            labels = labels.to(device, non_blocking=True)


            # ----- forward -----

            torch.cuda.synchronize()

            t0 = time.time()

            outputs = model(images)

            loss = criterion(outputs, labels)

            torch.cuda.synchronize()

            t1 = time.time()

            fwd_time += (t1 - t0)


            # ----- backward -----

            torch.cuda.synchronize()

            t2 = time.time()

            optimizer.zero_grad()

            loss.backward()

            torch.cuda.synchronize()

            t3 = time.time()

            bwd_time += (t3 - t2)


            # ----- optimizer step -----

            torch.cuda.synchronize()

            t4 = time.time()

            optimizer.step()

            torch.cuda.synchronize()

            t5 = time.time()

            other_time += (t5 - t4)


            running_loss += loss.item() * labels.size(0)


        torch.cuda.synchronize()

        epoch_end = time.time()

        epoch_time = epoch_end - epoch_start


        avg_loss = running_loss / len(train_loader.dataset)

        acc = evaluate(model, test_loader, device)


        epoch_total_times.append(epoch_time)

        epoch_forward_times.append(fwd_time)

        epoch_backward_times.append(bwd_time)

        epoch_other_times.append(other_time)

        test_accuracies.append(acc)


        print(

            f"Epoch {epoch}: "

            f"loss={avg_loss:.4f}, "

            f"test_acc={acc*100:.2f}%, "

            f"time_total={epoch_time:.2f}s "

            f"(fwd={fwd_time:.2f}s, bwd={bwd_time:.2f}s, other={other_time:.2f}s)"

        )


    return {

        "total_times": epoch_total_times,

        "forward_times": epoch_forward_times,

        "backward_times": epoch_backward_times,

        "other_times": epoch_other_times,

        "test_accuracies": test_accuracies,

    }


=== Training ===

Epoch 1: loss=2.0149, test_acc=25.66%, time_total=14.03s (fwd=5.07s, bwd=8.44s, other=0.31s)

Epoch 2: loss=1.7370, test_acc=27.29%, time_total=13.02s (fwd=4.36s, bwd=8.27s, other=0.20s)

Epoch 3: loss=1.7020, test_acc=29.60%, time_total=13.18s (fwd=4.37s, bwd=8.41s, other=0.20s)

Epoch 4: loss=1.6581, test_acc=33.84%, time_total=13.40s (fwd=4.43s, bwd=8.57s, other=0.20s)



ฺุฑุง ุงููุฏุฑ ฺฉูุฏ ุฏุงุฑู ุขููุฒุด ู ุจููุ

ูุดฺฉู ุงุตู ู ุฏูู ฺฉูุฏุ ุฎูุฏู ฺฉุฏู ุฒูุงูโุจูุฏ ุดูุง ุงุณุชุ ูู ูุฒููุงู ูุฏู ุง ุฏุงุฏูโูุง.

ุดูุง ุงุฒ torch.cuda.synchronize() ุจู ุดฺฉู ุงุณุชูุงุฏู ฺฉุฑุฏูโุงุฏ ฺฉู ุนููุงู ุฌูู ูุฑฺฏููู ููุงุฒโุณุงุฒ (parallelism) ุจู CPU ู GPU ุฑุง ูโฺฏุฑุฏ ู ุจู ุดุฏุช ุจุงุนุซ ฺฉูุฏ ูโุดูุฏ.

๐ข ฑ. ูุดฺฉู ุงุตู: ุงุณุชูุงุฏู ุจุด ุงุฒ ุญุฏ ุงุฒ torch.cuda.synchronize()

torch.cuda.synchronize() ฺฉ ุฏุณุชูุฑ "ูุณุฏูุฏฺฉููุฏู" (blocking) ุงุณุช. ุงู ุฏุณุชูุฑ ุจู CPU ูโฺฏูุฏ: "ุตุจุฑ ฺฉู ุชุง GPU ุชูุงู ฺฉุงุฑูุง ฺฉู ุจู ุงู ุณูพุฑุฏูโุงู ุฑุง ุชูุงู ฺฉูุฏ ู ุจฺฉุงุฑ ุดูุฏ."

ุดูุง ุฏุฑ ูุฑ ุจฺ (batch) ุงุฒ ุญููู ุขููุฒุด ุฎูุฏุ ถ ุจุงุฑ (ูุจู ู ุจุนุฏ ุงุฒ forward, backward, step) ุงู ุฏุณุชูุฑ ุฑุง ุตุฏุง ุฒุฏูโุงุฏ.

ุงู ฺฉุงุฑ ฺู ูุดฺฉู ุงุฌุงุฏ ูโฺฉูุฏุ

ุฏุฑ ุญุงูุช ุนุงุฏ (ุจุฏูู synchronize)ุ PyTorch ุจู ุตูุฑุช ูุงููุฒูุงู (asynchronous) ฺฉุงุฑ ูโฺฉูุฏ. ุนู CPU ฺฉุงุฑูุง ุจุนุฏ (ูุซู ุขูุงุฏูโุณุงุฒ ุจฺ ุจุนุฏ ุฏุงุฏู) ุฑุง ุงูุฌุงู ูโุฏูุฏุ ุฏุฑ ุญุงู ฺฉู GPU ูุดุบูู ูุญุงุณุจู ุจฺ ูุนู ุงุณุช. ุงู ุจุงุนุซ ูโุดูุฏ ฺฉู GPU ููุดู "ูุดุบูู" ุจูุงูุฏ ู ุฑุงูุฏูุงู ุจู ุญุฏุงฺฉุซุฑ ุจุฑุณุฏ.

ฺฉุฏ ุดูุง ุงู ุฑุงูุฏูุงู ุฑุง ุงุฒ ุจู ูโุจุฑุฏ:

    CPU ุฏุณุชูุฑุงุช forward ุฑุง ุจู GPU ูโุฏูุฏ.

    ุดูุง synchronize() ุฑุง ุตุฏุง ูโุฒูุฏ.

    CPU ูุชููู ูโุดูุฏ ู ููุชุธุฑ ูโูุงูุฏ ุชุง GPU ฺฉุงุฑ forward ุฑุง ุชูุงู ฺฉูุฏ.

    GPU ุชูุงู ูโฺฉูุฏุ CPU ุฒูุงู ุฑุง ุซุจุช ูโฺฉูุฏ.

    CPU ุฏุณุชูุฑุงุช backward ุฑุง ุจู GPU ูโุฏูุฏ.

    ุดูุง synchronize() ุฑุง ุตุฏุง ูโุฒูุฏ.

    CPU ุฏูุจุงุฑู ูุชููู ูโุดูุฏ ู ููุชุธุฑ ูโูุงูุฏ ุชุง GPU ฺฉุงุฑ backward ุฑุง ุชูุงู ฺฉูุฏ.

    ... ู ุงู ฺุฑุฎู ุงุฏุงูู ุฏุงุฑุฏ.

    ุฎูุงุตู: torch.cuda.synchronize() ฺฉ ุงุจุฒุงุฑ ูพุฑููุงููฺฏ (ุฒูุงูโุณูุฌ) ุงุณุชุ ูู ุจุฎุด ุงุฒ ูุฑุขูุฏ ุขููุฒุด ุนุงุฏ. ุงุณุชูุงุฏู ุงุฒ ุขู ุฏุฑ ุฏุงุฎู ุญููู ุขููุฒุดุ ุจู ุทูุฑ ูุตููุน ูุฑุขูุฏ ุฑุง ฺฉูุฏ ูโฺฉูุฏ ุชุง ุจุชูุงูุฏ ุฒูุงู ุฏูู ุฑุง ุงูุฏุงุฒู ุจฺฏุฑุฏ.

๐ง ฒ. ฺุฑุง ุฒูุงู backward ุฏู ุจุฑุงุจุฑ forward ุงุณุชุ

ุงู ุจุฎุด ฺฉุงููุงู ุทุจุน ู ููุฑุฏ ุงูุชุธุงุฑ ุงุณุช.

    Forward Pass: ุฏุงุฏูโูุง ููุท ุงุฒ ูุงูโูุง ุนุจูุฑ ูโฺฉููุฏ ู ุฎุฑูุฌ ูุญุงุณุจู ูโุดูุฏ. (fwd=4.37s)

    Backward Pass: ูู ุชููุง ฺฏุฑุงุฏุงูโูุง ุจุงุฏ ุจุฑุง ุฎุฑูุฌ ูุญุงุณุจู ุดููุฏุ ุจูฺฉู ุงู ฺฏุฑุงุฏุงูโูุง ุจุงุฏ ุงุฒ ุทุฑู ุชูุงู ูุงูโูุง ุจู ุนูุจ ุจุฑฺฏุฑุฏุงูุฏู ุดููุฏ (backpropagate) ุชุง ฺฏุฑุงุฏุงู ูุฒูโูุง ูุฑ ูุงู ูุญุงุณุจู ุดูุฏ.

ูุญุงุณุจู ฺฏุฑุงุฏุงู ุจุฑุง ุชูุงู ูพุงุฑุงูุชุฑูุง ูุฏู (ฺฉู ูโุชูุงููุฏ ููููโูุง ุจุงุดูุฏ) ุงุฒ ูุธุฑ ูุญุงุณุจุงุช ุจุณุงุฑ ุณูฺฏูโุชุฑ ุงุฒ ฺฉ ูพุงุณู ุฑู ุจู ุฌูู ุงุณุช. ุงูฺฉู ุฒูุงู backward ุญุฏูุฏ ฑ.ต ุชุง ฒ.ต ุจุฑุงุจุฑ ุฒูุงู forward ุจุงุดุฏุ ุฏุฑ ุจุณุงุฑ ุงุฒ ูุฏูโูุง CNN ฺฉุงููุงู ุฑุงุฌ ุงุณุช.

ูพุณ ูุดฺฉู ฺฉูุฏ ุดูุง ูุณุจุช ฒ:ฑ ุจู bwd ู fwd ูุณุชุ ุจูฺฉู ฺฉูุฏ ุดูุง ุงู ุงุณุช ฺฉู ูุฌููุน ุงู ุฏู (ู ุณุฑุจุงุฑ synchronize) ุจุงุนุซ ุดุฏู ูุฑ epoch ุณุฒุฏู ุซุงูู ุทูู ุจฺฉุดุฏ.

โ ุฑุงูโุญูโูุง

ุดูุง ุฏู ุฑุงู ุฏุงุฑุฏ:

ุฑุงูโุญู ฑ: ุขููุฒุด ุณุฑุน (ุญุฐู ฺฉุงูู ุฒูุงูโุจูุฏ ุฏุงุฎู)

ุงฺฏุฑ ูุฏู ุดูุง ููุท ุขููุฒุด ุณุฑุน ุงุณุช ู ุจู ุชูฺฉฺฉ ุฒูุงู fwd ู bwd ูุงุฒ ูุฏุงุฑุฏุ ุชูุงู synchronizeูุง ู time.time()ูุง ุฏุงุฎู ุญููู ุจฺ (batch loop) ุฑุง ุญุฐู ฺฉูุฏ. ููุท ุฒูุงู ฺฉู epoch ุฑุง ูฺฏู ุฏุงุฑุฏ.
Python

import time
import torch
import torch.nn as nn

# (function evaluate(model, test_loader, device) must be defined elsewhere)
# def evaluate(model, test_loader, device): ...

def train_model_fast(model, train_loader, test_loader, device, epochs=3, lr=1e-3):
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        
        # --- Time the whole epoch ---
        epoch_start = time.time()

        for images, labels in train_loader:
            images = images.to(device) # non_blocking=True is only useful with pin_memory=True
            labels = labels.to(device)

            # --- Forward ---
            outputs = model(images)
            loss = criterion(outputs, labels)

            # --- Backward + Step ---
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * labels.size(0)

        # --- End of epoch timing ---
        epoch_end = time.time()
        epoch_time = epoch_end - epoch_start
        
        avg_loss = running_loss / len(train_loader.dataset)
        
        # Evaluation
        # acc = evaluate(model, test_loader, device) # Assuming you have this function
        acc = 0.0 # Placeholder
        
        print(
            f"Epoch {epoch}: "
            f"loss={avg_loss:.4f}, "
            # f"test_acc={acc*100:.2f}%, "
            f"total_time={epoch_time:.2f}s"
        )
    print("Training finished.")
    return

ุฑุงูโุญู ฒ: ุฒูุงูโุจูุฏ ุตุญุญ (ุจุง ุงุณุชูุงุฏู ุงุฒ torch.cuda.Event)

ุงฺฏุฑ ููฺูุงู ูโุฎูุงูุฏ ุฒูุงู fwd ู bwd ุฑุง ุจู ุทูุฑ ุฏูู ู ุจุฏูู ฺฉูุฏ ฺฉุฑุฏู ุขููุฒุด ุงูุฏุงุฒู ุจฺฏุฑุฏุ ุจุงุฏ ุงุฒ ุงุจุฒุงุฑ ููุงุณุจ ุงู ฺฉุงุฑ ุนู torch.cuda.Event ุงุณุชูุงุฏู ฺฉูุฏ.

ุงู ุงุจุฒุงุฑูุง ุจู GPU ุงุฌุงุฒู ูโุฏููุฏ ุชุง ุฒูุงูโูุง ุฑุง ุซุจุช ฺฉูุฏุ ุจุฏูู ุงูฺฉู CPU ุฑุง ูุชููู ฺฉูุฏ.
Python

import time
import torch
import torch.nn as nn

# (function evaluate(model, test_loader, device) must be defined elsewhere)
# def evaluate(model, test_loader, device): ...

def train_model_timed_fast(model, train_loader, test_loader, device, epochs=3, lr=1e-3):
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # --- Create CUDA Events for timing ---
    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)

    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0

        fwd_time = 0.0
        bwd_time = 0.0
        other_time = 0.0

        epoch_start = time.time() # CPU time for total epoch

        for images, labels in train_loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # ----- forward -----
            starter.record()
            outputs = model(images)
            loss = criterion(outputs, labels)
            ender.record()
            torch.cuda.synchronize() # Wait for this specific block to finish
            fwd_time += starter.elapsed_time(ender)

            # ----- backward -----
            starter.record()
            optimizer.zero_grad()
            loss.backward()
            ender.record()
            torch.cuda.synchronize()
            bwd_time += starter.elapsed_time(ender)

            # ----- optimizer step -----
            starter.record()
            optimizer.step()
            ender.record()
            torch.cuda.synchronize()
            other_time += starter.elapsed_time(ender)

            running_loss += loss.item() * labels.size(0)

        epoch_end = time.time()
        epoch_time = epoch_end - epoch_start
        
        avg_loss = running_loss / len(train_loader.dataset)
        # acc = evaluate(model, test_loader, device)
        acc = 0.0 # Placeholder

        # Note: Event time is in milliseconds
        print(
            f"Epoch {epoch}: "
            f"loss={avg_loss:.4f}, "
            # f"test_acc={acc*100:.2f}%, "
            f"time_total={epoch_time:.2f}s "
            f"(fwd={fwd_time/1000:.2f}s, bwd={bwd_time/1000:.2f}s, other={other_time/1000:.2f}s)"
        )

    return

ุชูุฌู: ุญุช ุฏุฑ ุฑุงูโุญู ุฏูู ฺฉู ุงุฒ Event ุงุณุชูุงุฏู ูโฺฉูุฏุ ูุง ูููุฒ ูุฌุจูุฑู ุจุนุฏ ุงุฒ ender.record() ฺฉ synchronize() ุจฺฏุฐุงุฑู ุชุง ูุทูุฆู ุดูู ฺฉู ุฒูุงู ูุจู ุงุฒ ุฌูุน ุฒุฏู (fwd_time += ...) ูุญุงุณุจู ุดุฏู ุงุณุช. ุงู ุฑูุด ูููุฒ ูู ฺฉูุฏุชุฑ ุงุฒ ุขููุฒุด ุจุฏูู ูพุฑููุงููฺฏ ุงุณุชุ ุงูุง ุจุณุงุฑ ุฏููโุชุฑ ู ุงุณุชุงูุฏุงุฑุฏุชุฑ ุงุฒ ุงุณุชูุงุฏู ุงุฒ time.time() ุงุณุช.

ู ุจุฑูุงูู ุฑุฒ ุจฺฏุฐุงุฑู ุจูุชุฑ ููุดูุ

ุจููุ ูุทุนุงู! ุงุณุชูุงุฏู ุงุฒ ฺฉ ุจุฑูุงููโุฑุฒ ูุฑุฎ ุงุฏฺฏุฑ (Learning Rate Scheduler) ุชูุฑุจุงู ููุดู ุงุฏูโ ุจุณุงุฑ ุฎูุจ ุงุณุช ู ฺฉ ุงุฒ ุฑูุดโูุง ุงุณุชุงูุฏุงุฑุฏ ุจุฑุง ุจูุจูุฏ ุขููุฒุด ูุฏู ุงุณุช.

ุงูุง ฺฉ ูฺฉุชูโ ุจุณุงุฑ ููู ูุฌูุฏ ุฏุงุฑุฏ:

    ุจุฑูุงููโุฑุฒ (Scheduler) ุณุฑุนุช ุงุฌุฑุง ฺฉุฏ ุดูุง ุฑุง ุจุงูุง ููโุจุฑุฏ.

    ูุดฺฉู ูุจู ุดูุง (ฺฉูุฏ ุจูุฏู ูุฑ epoch) ุจู ุฏูู ุงุณุชูุงุฏู ุงุฒ synchronize ุจุฑุง ุฒูุงูโุณูุฌ ุจูุฏ. Scheduler ุขู ูุดฺฉู (ุซุงููโูุง ฺฉู ุทูู ูโฺฉุดุฏ) ุฑุง ุญู ููโฺฉูุฏ.

    ฺฉุงุฑ ฺฉู Scheduler ุงูุฌุงู ูโุฏูุฏุ ุจูุจูุฏ ููฺฏุฑุง ูุฏู ุงุณุช. ุนู ุจุงุนุซ ูโุดูุฏ ูุฏู ุดูุง ุจุง ุฏูุช ุจุงูุงุชุฑ ู ุฏุฑ ุชุนุฏุงุฏ epochูุง ฺฉูุชุฑ ุจู ูุชุฌูโ ูุทููุจ ุจุฑุณุฏ.

๐ฏ ุจุฑูุงููโุฑุฒ (Scheduler) ุฏููุงู ฺู ฺฉุงุฑ ูโฺฉูุฏุ

ฺฉุงุฑ ุงุตู Schedulerุ ุชูุธู ุฎูุฏฺฉุงุฑ ูุฑุฎ ุงุฏฺฏุฑ (Learning Rate) ุฏุฑ ุทูู ูุฑุขูุฏ ุขููุฒุด ุงุณุช.

    ุดุฑูุน ุจุง LR ุจุงูุง: ุฏุฑ ุงุจุชุฏุง ุขููุฒุดุ ูุนูููุงู ูโุฎูุงูู ุจุง ูุฑุฎ ุงุฏฺฏุฑ ุจุงูุง (ูุซูุงู 1e-3 ฺฉู ุชูุธู ฺฉุฑุฏุฏ) ุดุฑูุน ฺฉูู. ุงู ุจุงุนุซ ูโุดูุฏ ูุฏู ุจู ุณุฑุนุช ุจู ุณูุช ุฌูุงุจ ุจููู ุญุฑฺฉุช ฺฉูุฏ.

    ฺฉุงูุด ุชุฏุฑุฌ LR: ููุช ูุฏู ุจู ูุฒุฏฺฉ ุฌูุงุจ ุจููู ูโุฑุณุฏุ ูุฑุฎ ุงุฏฺฏุฑ ุจุงูุง ูโุชูุงูุฏ ูุถุฑ ุจุงุดุฏ ู ุจุงุนุซ ุดูุฏ ูุฏู ุฏูุฑ ู ุจุฑ ุฌูุงุจ ุจููู "ูพุฑุด" ฺฉูุฏ ู ูุชูุงูุฏ ุฏุฑ ุขู ูุณุชูุฑ ุดูุฏ.

ฺฉ ูุซุงู ุณุงุฏู: ุชุตูุฑ ฺฉูุฏ ูโุฎูุงูุฏ ุฏุฑ ฺฉ ูพุงุฑฺฉูฺฏ ุจุฒุฑฺฏุ ุฌุง ูพุงุฑฺฉ ุฏูู ุฎูุฏ ุฑุง ูพุฏุง ฺฉูุฏ.

    LR ุจุงูุง: ุจุง ุณุฑุนุช ุฏุฑ ุฑุงูุฑู ุงุตู ูพุงุฑฺฉูฺฏ ุฑุงููุฏฺฏ ูโฺฉูุฏ ุชุง ุจู ูุญุฏูุฏู ููุฑุฏ ูุธุฑ ุจุฑุณุฏ.

    LR ูพุงู: ููุช ุจู ูุฒุฏฺฉ ุฌุง ูพุงุฑฺฉ ูโุฑุณุฏุ ุณุฑุนุช ุฎูุฏ ุฑุง ฺฉู ูโฺฉูุฏ (ุชุฑูุฒ ูโฺฉูุฏ) ุชุง ุจุชูุงูุฏ ุจุง ุฏูุช ูุงุดู ุฑุง ูพุงุฑฺฉ ฺฉูุฏ.

Scheduler ุฏููุงู ููู ฺฉุงุฑ "ฺฉู ฺฉุฑุฏู ุณุฑุนุช" ุฑุง ุจุฑุง ูุฏู ุดูุง ุงูุฌุงู ูโุฏูุฏ ุชุง ุจุชูุงูุฏ ุจุง ุฏูุช ุจุงูุง ุฏุฑ "ููุทู ุจููู" (ุญุฏุงูู Loss) ูุณุชูุฑ ุดูุฏ.

๐ป ฺุทูุฑ ุขู ุฑุง ุงุถุงูู ฺฉููุ (ูุซุงู ุจุง StepLR)

ฺฉ ุงุฒ ุณุงุฏูโุชุฑู Schedulerโูุงุ StepLR ุงุณุช. ุงู ุจุฑูุงููโุฑุฒุ ูุฑุฎ ุงุฏฺฏุฑ ุฑุง ูุฑ step_size ุงููพุงฺฉุ ุฏุฑ ฺฉ ุนุฏุฏ gamma (ูุซูุงู 0.1) ุถุฑุจ ูโฺฉูุฏ.

ฺฉุฏ ุดูุง ุจู ุงู ุดฺฉู ุชุบุฑ ูโฺฉูุฏ (ุจุง ูุฑุถ ุงูฺฉู ุชุงุจุน evaluate ุฑุง ุฏุงุฑุฏ):
Python

import time
import torch
import torch.nn as nn
import torch.optim as optim # Import optim

# (evaluate function must be defined)
def evaluate(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

# Note: I am using the 'fast' version of your training loop 
# without the detailed timing, as that was the bottleneck.
def train_model_with_scheduler(model, train_loader, test_loader, device, epochs=10, lr=1e-3):
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()
    
    # 1. --- Define the Scheduler ---
    # This will multiply the LR by 0.1 every 3 epochs
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    epoch_total_times = []
    test_accuracies = []
    
    print(f"Starting training for {epochs} epochs...")

    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0

        torch.cuda.synchronize() # Optional: only for epoch timing
        epoch_start = time.time()

        for images, labels in train_loader:
            images = images.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * labels.size(0)

        torch.cuda.synchronize() # Optional: only for epoch timing
        epoch_end = time.time()
        epoch_time = epoch_end - epoch_start

        avg_loss = running_loss / len(train_loader.dataset)
        acc = evaluate(model, test_loader, device)
        
        # 2. --- Step the Scheduler ---
        # Call this ONCE per epoch, after the training loop
        scheduler.step()

        epoch_total_times.append(epoch_time)
        test_accuracies.append(acc)

        print(
            f"Epoch {epoch}: "
            f"loss={avg_loss:.4f}, "
            f"test_acc={acc*100:.2f}%, "
            f"time={epoch_time:.2f}s, "
            f"current_lr={scheduler.get_last_lr()[0]}" # Print current LR
        )

    print("Training finished.")
    return {
        "total_times": epoch_total_times,
        "test_accuracies": test_accuracies,
    }

# --- Example Usage ---
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = CNNBaseline(in_channel=3, num_class=10).to(device) # STL10 has 3 channels
# results = train_model_with_scheduler(model, train_loader, test_loader, device, epochs=10)

ุชุบุฑุงุช ฺฉูุฏ:

    scheduler = optim.lr_scheduler.StepLR(...): ุจูุงูุงุตูู ุจุนุฏ ุงุฒ optimizer ุชุนุฑู ุดุฏ. (step_size=3, gamma=0.1 ุนู ูุฑ ณ ุงููพุงฺฉุ ูุฑุฎ ุงุฏฺฏุฑ ฐ.ฑ ุจุฑุงุจุฑ ูโุดูุฏ).

    scheduler.step(): ุฏุฑ ุงูุชูุง ุญููู epoch (ู ุฎุงุฑุฌ ุงุฒ ุญููู batch) ุตุฏุง ุฒุฏู ูโุดูุฏ.

ุงููุงุน ุฏฺฏุฑ Scheduler

StepLR ุณุงุฏู ุงุณุช ุงูุง ุจูุชุฑู ูุณุช. ุฏู ููุน ุจุณุงุฑ ูุญุจูุจ ุฏฺฏุฑ ุนุจุงุฑุชูุฏ ุงุฒ:

    ReduceLROnPlateau: ุงู ุจุฑูุงููโุฑุฒุ ฺฉ ูุนุงุฑ (ูุซู test_loss ุง test_accuracy) ุฑุง ูุงูุชูุฑ ูโฺฉูุฏ ู ุงฺฏุฑ ุขู ูุนุงุฑ ุชุง ฺูุฏ ุงููพุงฺฉ ุจูุจูุฏ ูพุฏุง ูฺฉุฑุฏุ LR ุฑุง ฺฉู ูโฺฉูุฏ. (ุจุณุงุฑ ููุดููุฏ).

    CosineAnnealingLR: ุงู ุจุฑูุงููโุฑุฒุ LR ุฑุง ุจู ุตูุฑุช ฺฉ ููุญู ฺฉุณููุณ ุฒุจุง ุงุฒ ููุฏุงุฑ ูุงฺฉุฒูู ุจู ูููู ฺฉุงูุด ูโุฏูุฏ ฺฉู ุงุบูุจ ูุชุงุฌ ุจุณุงุฑ ูพุงุฏุงุฑ ูโุฏูุฏ.

ุฎูุงุตู: ุจููุ ุญุชูุงู ุงุฒ Scheduler ุงุณุชูุงุฏู ฺฉูุฏ. ุงู ฺฉุงุฑ ุจู ุทูุฑ ฺุดูฺฏุฑ ูุชุงุฌ ููุง ูุฏู ุดูุง ุฑุง ุจูุจูุฏ ูโุจุฎุดุฏ.

class CNN(nn.Module):

    def __init__(self, in_channel=1, num_class=10):

        super().__init__()

        

        self.l1 = nn.Conv2d(in_channel, 64, kernel_size=7, padding=1)

        self.pool = nn.MaxPool2d(2)

        

        self.l2 = nn.Conv2d(64, 64, kernel_size=3, padding=1) 

        self.l3 = nn.Conv2d(64, 64, kernel_size=3, padding=1) 

        self.l4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)

        self.l5 = nn.Conv2d(128, 128, kernel_size=3, padding=1)

        self.l6 = nn.Conv2d(128, 256, kernel_size=3, padding=1)

        

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        

        self.fc1 = nn.Linear(256, 128)

        self.fc2 = nn.Linear(128, num_class)



    def forward(self, x):

        x = self.l1(x)

        x = F.relu(x)

        x = self.pool(x)

        

        x = F.relu(self.l2(x))

        x = F.relu(self.l3(x))

        x = F.relu(self.l4(x))

        x = F.relu(self.l5(x))

        x = F.relu(self.l6(x))

        

        x = self.avgpool(x)

        

        x = x.view(x.size(0), -1) 

        

        x = F.relu(self.fc1(x))

        x = self.fc2(x)

        return x



ุฏูุช ูุฏู ุฎูุจ ุฒุงุฏ ููุดู ฺู ฺฉููุ

ุงู ฺฉ ูุดฺฉู ุจุณุงุฑ ุฑุงุฌ ุงุณุช ฺฉู ุฏูุงู ูุฎุชูู ูโุชูุงูุฏ ุฏุงุดุชู ุจุงุดุฏ. ุจุง ูฺฏุงู ุจู ฺฉุฏ ุดูุงุ ุจู ุงุญุชูุงู ุฒุงุฏ ูุดฺฉู ุงุตู ุชุฑฺฉุจ ุงุฒ Overfitting ู ูุนูุงุฑ ูุงฺฉุงุฑุขูุฏ ุงุณุช.

ูุฏู ุดูุง ถ ูุงู ฺฉุงููููุดู ูพุดุช ุณุฑ ูู ุฏุงุฑุฏุ ุงูุง ุชูุฑุจุงู ูฺ ยซูุงูููโููุฏุณุงุฒยป (Regularization) ุง ูุฑูุงูโุณุงุฒ (Normalization) ูุฏุงุฑุฏ ู ุณุงุฎุชุงุฑ ูพูููฺฏ (Pooling) ุขู ุบุฑูุนููู ุงุณุช.

ุฏุฑ ุงูุฌุง ฺูุฏ ุงูุฏุงู ฺฉูุฏ ูุฌูุฏ ุฏุงุฑุฏ ฺฉู ูโุชูุงูุฏ ุจุฑุง ุจูุจูุฏ ฺุดูฺฏุฑ ุฏูุช ูุฏู ุงูุฌุงู ุฏูุฏ:

ฑ. ุงูุฒูุฏู Data Augmentation (ูููโุชุฑู)

ุงฺฏุฑ ุฏูุช ุดูุง ุฑู ุฏุงุฏูโูุง ุชุณุช (validation) ูพุงู ุงุณุช ุงูุง ุฑู ุฏุงุฏูโูุง ุขููุฒุด ุจุงูุงุณุชุ ุดูุง ูุทุนุงู Overfit ุดุฏูโุงุฏ. ูุฏู ุดูุง ุจู ุฌุง ุงุฏฺฏุฑ ูฺฺฏโูุง ฺฉูุ ุฏุงุฏูโูุง ุขููุฒุด ุฑุง ยซุญูุธยป ฺฉุฑุฏู ุงุณุช.

ุงุฒ ุขูุฌุง ฺฉู ุดูุง ุงุฒ STL10 ุงุณุชูุงุฏู ูโฺฉูุฏ (ฺฉู ููุท ตฐฐฐ ุชุตูุฑ ุขููุฒุด ุฏุงุฑุฏ)ุ Augmentation ุถุฑูุฑ ุงุณุช.

ุฑุงูโุญู: ุจู transform ุฎูุฏุ ฺุฑุฎุดโูุง ุชุตุงุฏูุ ุจุฑุด (Crop) ู ุจุฑุนฺฉุณ ฺฉุฑุฏู ุงูู (Flip) ุฑุง ุงุถุงูู ฺฉูุฏ.
Python

# --- WRONG (for training) ---
# transform=transforms.Compose([
#     transforms.ToTensor(),
#     transforms.Normalize((0.1307,), (0.3081,))
#     ])

# --- RIGHT (for training) ---
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(96, scale=(0.8, 1.0)), # STL10 is 96x96
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize for 3 channels
])

# For testing, we don't augment
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

# --- Apply to dataset ---
# dataset1 = datasets.STL10('../data', split='train', download=True,
#                          transform=train_transform) # Use train_transform
# dataset2 = datasets.STL10('../data', split='test',
#                          transform=test_transform) # Use test_transform

ูฺฉุชู: ุฏุชุงุจุณ STL10 ุชุตุงูุฑ ุฑูฺฏ (ณ ฺฉุงูุงูู) ุฏุงุฑุฏ.

    ุดูุง ุจุงุฏ in_channel=3 ุฑุง ุจู ูุฏู ุฎูุฏ ุจุฏูุฏ.

    ููุงุฏุฑ Normalize ุดูุง ุจุฑุง MNIST (1 ฺฉุงูุงู) ุงุณุช. ูู ุขู ุฑุง ุจุฑุง ณ ฺฉุงูุงู ุงุตูุงุญ ฺฉุฑุฏู (ููุงุฏุฑ 0.5 ุฑุงุฌ ูุณุชูุฏ).

ฒ. ุจุงุฒูฺฏุฑ ุงุณุงุณ ุฏุฑ ูุนูุงุฑ ูุฏู

ูุนูุงุฑ ูุนู ุดูุง ุฏู ูุดฺฉู ุจุฒุฑฺฏ ุฏุงุฑุฏ:

    ูุจูุฏ BatchNorm: ูุงูโูุง BatchNorm2d (ูุฑูุงูโุณุงุฒ ุฏุณุชูโุง) ุขููุฒุด ุฑุง ุจู ุดุฏุช ูพุงุฏุงุฑ ูโฺฉููุฏ ู ุจู ููฺฏุฑุง ุณุฑุนโุชุฑ ฺฉูฺฉ ูโฺฉููุฏ. ุชูุฑุจุงู ุฏุฑ ุชูุงู ูุฏูโูุง ูุฏุฑู ุงุณุชูุงุฏู ูโุดููุฏ.

    ุณุงุฎุชุงุฑ Conv/Pool: ุดูุง ููุท ฺฉ ูุงู MaxPool ุฏุฑ ุงุจุชุฏุง ุฏุงุฑุฏ ู ุณูพุณ ต ูุงู Conv ูพุดุช ุณุฑ ูู. ุงู ุจุงุนุซ ูโุดูุฏ ฺฉู ุงุจุนุงุฏ ูุถุง (height/width) ูฺุฑ ููพโูุง ุจุณุงุฑ ุจุฒุฑฺฏ ุจุงู ุจูุงูุฏ ู ูุญุงุณุจุงุช ุณูฺฏู ู ูุงฺฉุงุฑุขูุฏ ุดููุฏ.

ุฑุงูโุญู: ุงุฒ ฺฉ ุณุงุฎุชุงุฑ ุงุณุชุงูุฏุงุฑุฏ "VGG-style" ุงุณุชูุงุฏู ฺฉูุฏ: (Conv -> BN -> ReLU) * N -> Pool.

ุฏุฑ ุฒุฑ ฺฉ ูุนูุงุฑ ุจุณุงุฑ ุงุณุชุงูุฏุงุฑุฏุชุฑ ู ููโุชุฑ ุจุฑ ุงุณุงุณ ูุฏู ุดูุง ุขูุฏู ุงุณุช:
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class ImprovedCNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10): # Changed to in_channel=3
        super().__init__()
        
        # Block 1: (Input -> 64 filters) -> Pool
        self.conv_block1 = nn.Sequential(
            nn.Conv2d(in_channel, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2) # Halves the size
        )
        
        # Block 2: (64 -> 128 filters) -> Pool
        self.conv_block2 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(128, 128, kernel_size=3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2) # Halves the size again
        )
        
        # Block 3: (128 -> 256 filters) -> Pool
        self.conv_block3 = nn.Sequential(
            nn.Conv2d(128, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.Conv2d(256, 256, kernel_size=3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2) # Halves the size again
        )

        # Classifier Head
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Dropout(0.5), # Add Dropout
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.5), # Add Dropout
            nn.Linear(128, num_class)
        )

    def forward(self, x):
        x = self.conv_block1(x)
        x = self.conv_block2(x)
        x = self.conv_block3(x)
        x = self.avgpool(x)
        x = self.classifier(x)
        return x

# --- Example Usage ---
# model = ImprovedCNN(in_channel=3, num_class=10) # For STL10
# print(model)

ฺุฑุง ุงู ุจูุชุฑ ุงุณุชุ

    BatchNorm2d: ุขููุฒุด ุฑุง ูพุงุฏุงุฑ ูโฺฉูุฏ.

    MaxPool2d (ูฺฉุฑุฑ): ุจู ุชุฏุฑุฌ ุงุจุนุงุฏ ูุถุง ุฑุง ฺฉุงูุด ูโุฏูุฏ (ูุซูุงู 96x96 -> 48x48 -> 24x24 -> 12x12). ุงู ฺฉุงุฑ ุจุงุนุซ ูโุดูุฏ ูุงูโูุง ุจุนุฏ ุฑู ูฺฺฏโูุง ยซุฎูุงุตูโุชุฑยป ู ยซฺฉูโุชุฑยป ุชูุฑฺฉุฒ ฺฉููุฏ ฺฉู ุจุฑุง ุงุฏฺฏุฑ ุจูุชุฑ ุงุณุช.

    Dropout: ุงุฒ Overfitting ุฏุฑ ูุงูโูุง Linear (ฺฉู ุจุดุชุฑู ูพุงุฑุงูุชุฑ ุฑุง ุฏุงุฑูุฏ) ุฌููฺฏุฑ ูโฺฉูุฏ.

ณ. ุงูุฒูุฏู Scheduler ู Weight Decay

ููุงูุทูุฑ ฺฉู ูุจูุงู ุจุญุซ ฺฉุฑุฏูุ Scheduler ุจู ููฺฏุฑุง ุจูุชุฑ ฺฉูฺฉ ูโฺฉูุฏ. ุงูุง ฺฉ ุงุจุฒุงุฑ ุฏฺฏุฑ ูู ูุฌูุฏ ุฏุงุฑุฏ: Weight Decay.

Weight Decay (ุง L2 Regularization) ฺฉ ูพุงุฑุงูุชุฑ ุฏุฑ Optimizer ุงุณุช ฺฉู ุงุฒ ุจุฒุฑฺฏ ุดุฏู ุจุด ุงุฒ ุญุฏ ูุฒูโูุง ูุฏู ุฌููฺฏุฑ ูโฺฉูุฏ ู ุงู ฺฉ ุฏฺฏุฑ ุงุฒ ุฑุงูโูุง ุนุงู ุจุฑุง ูุจุงุฑุฒู ุจุง Overfitting ุงุณุช.
Python

# Add weight_decay to your optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)

# Use a better scheduler, like CosineAnnealing
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

๐ ุฎูุงุตู ุงูุฏุงูุงุช ูพุดููุงุฏ:

    ุงุตูุงุญ ูุฑูุฏ: ูุฏู CNN ุฑุง ุจุง in_channel=3 ุจุฑุง STL10 ุจุณุงุฒุฏ.

    ุงุนูุงู Data Augmentation: ุงุฒ train_transform ฺฉู ุฏุฑ ุจุงูุง ฺฏูุชู ุจุฑุง dataset1 (ุฏุงุฏูโูุง ุขููุฒุด) ุงุณุชูุงุฏู ฺฉูุฏ.

    ุงุตูุงุญ ูุนูุงุฑ: ุงุฒ ImprovedCNN (ฺฉุฏ ุจุงูุง) ุจู ุฌุง CNN ูุนู ุฎูุฏ ุงุณุชูุงุฏู ฺฉูุฏ. ุงู ูุฏู ุดุงูู BatchNormุ Dropout ู ุณุงุฎุชุงุฑ Pool ุจุณุงุฑ ุจูุชุฑ ุงุณุช.

    ุชูุธู Optimizer: ุจู Adam ุฎูุฏ weight_decay=1e-4 (ุง ููุฏุงุฑ ูุดุงุจู) ุงุถุงูู ฺฉูุฏ.

    ุงุณุชูุงุฏู ุงุฒ Scheduler: ุงุฒ ฺฉ ุจุฑูุงููโุฑุฒ ุฎูุจ ูุงููุฏ CosineAnnealingLR ุงุณุชูุงุฏู ฺฉูุฏ ู ุฏุฑ ูพุงุงู ูุฑ epoch ุขู ุฑุง scheduler.step() ฺฉูุฏ.

ุงูุฌุงู ุงู ูุฑุงุญู ุจุงุฏ ุฏูุช ูุฏู ุดูุง ุฑุง ุจู ุทูุฑ ฺุดูฺฏุฑ ุงูุฒุงุด ุฏูุฏ.

ุขุง ูุงูุฏ ุฏุฑ ููุฑุฏ ูุญูู ุนููฺฉุฑุฏ BatchNorm ุง Dropout ุจุดุชุฑ ุจุฏุงูุฏุ

ุจููฺฉ ุจููฺฉ ูฺฉู ูพุดุช ูู ุจููุณ

ุฎูุงูุด ูโฺฉูู. ููุงู ูุนูุงุฑ ุจูุจูุฏุงูุชู ุดุงูู Batch Normalization ู Dropout ุฑุง ุงู ุจุงุฑ ุจู ุตูุฑุช ุฎุท ุจู ุฎุท ู ูุงู ุจู ูุงูุ ุจุฏูู ุงุณุชูุงุฏู ุงุฒ nn.Sequential (ุจููฺฉโุจูุฏ)ุ ุจุฑุง ุดูุง ูโููุณู.

ุฏุฑ ุงู ฺฉุฏุ ุชูุงู ูุงูโูุง ุจู ุตูุฑุช ุฌุฏุงฺฏุงูู ุฏุฑ __init__ ุชุนุฑู ุดุฏูโุงูุฏ ู ุจู ุชุฑุชุจ ุฏุฑ ูุชุฏ forward ูุฑุงุฎูุงู ูโุดููุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class FlattenedImprovedCNN(nn.Module):
    # Assuming in_channel=3 for the STL10 dataset
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        # --- Block 1: 3 -> 64 -> 64 ---
        # Conv1
        self.l1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        # Conv2
        self.l2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)

        # --- Block 2: 64 -> 128 -> 128 ---
        # Conv3
        self.l3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        # Conv4
        self.l4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2)
        
        # --- Block 3: 128 -> 256 -> 256 ---
        # Conv5
        self.l5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn5 = nn.BatchNorm2d(256)
        # Conv6
        self.l6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.bn6 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(2)
        
        # --- Classifier Head ---
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        self.dropout1 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, num_class)


    def forward(self, x):
        # Block 1: l1 -> BN -> ReLU -> l2 -> BN -> ReLU -> Pool
        x = self.l1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.l2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool1(x)
        
        # Block 2
        x = self.l3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.l4(x)
        x = self.bn4(x)
        x = F.relu(x)
        x = self.pool2(x)

        # Block 3
        x = self.l5(x)
        x = self.bn5(x)
        x = F.relu(x)
        x = self.l6(x)
        x = self.bn6(x)
        x = F.relu(x)
        x = self.pool3(x)
        
        # Global Avg Pooling & Flatten
        x = self.avgpool(x)
        x = x.view(x.size(0), -1)
        
        # Classifier
        x = self.dropout1(x)
        x = F.relu(self.fc1(x))
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

ุงู ฺฉุฏ ุงุฒ ูุธุฑ ุนููฺฉุฑุฏ ฺฉุงููุงู ูุนุงุฏู ููุงู ุณุงุฎุชุงุฑ ุจููฺฉ ุงุณุช ฺฉู ูุจูุงู ูพุดููุงุฏ ุดุฏุ ุงูุง ุชูุงู ูุงูโูุงุด ุจู ุตูุฑุช ูุฌุฒุง ุชุนุฑู ุดุฏูโุงูุฏ. ุชููุง ุชูุงูุช ุงู ุงุณุช ฺฉู ุฎูุงูุฏู ู ูุฏุฑุช ุดุจฺฉูโูุง ุนููโุชุฑ ุฏุฑ ุงู ุณุงุฎุชุงุฑ ฺฉู ุฏุดูุงุฑุชุฑ ูโุดูุฏ.

ุดุจฺฉู ุฑู ุชุบุฑ ุจุฏู


ฺฉุฑูู ูุง ููู ณ*ณ ุจุงุดู

ุฏุฑ ณ ูุงู ฺฉุงููููุดู ุฏุฑ ูุฑ ฺฉุฏูู ุงูุฏุงุฒู ุจุฒุฑฺฏ ุจุดู

ถด

ฑฒธ

ฒตถ


ุฏุฑ ูุฑ ูุงู ุฑูู ุจุฒู ู ูฺฉุณ ูพูู ุจฺฏุฐุงุฑ ู ูุฑูุงูุงุฒ ฺฉู

ุจุนุฏ ูู fc ูุณุช

ูุนูุงุฑ ุฌุฏุฏ ุดูุง ฺฉ ุณุงุฎุชุงุฑ VGG-like ุจุณุงุฑ ุงุณุชุงูุฏุงุฑุฏ ู ูู ุฎูุงูุฏ ุจูุฏ ฺฉู ุงุฒ ุงุตูู ููุงุณโุจูุฏ ฺฉุงูุงูโูุง ุงุณุชูุงุฏู ูโฺฉูุฏ.

ูู ุณุงุฎุชุงุฑ ุฑุง ุจู ุดฺฉู Conv2d -> BatchNorm2d -> ReLU -> MaxPool2d ูพุงุฏูโุณุงุฒ ูโฺฉูู. ุงู ุชุฑุชุจ ุจุฑุง ูุฑูุงูโุณุงุฒ ุฏุงุฏูโูุง ูุจู ุงุฒ ุงุนูุงู ุบุฑุฎุท ุจูุฏู (ReLU) ุจูุชุฑู ุนููฺฉุฑุฏ ุฑุง ุฏุงุฑุฏ.

๐ป ูุฏู ุฌุฏุฏ: VGG-Inspired CNN

ุงู ูุฏู ุดุงูู ณ ุจููฺฉ ุงุตู ฺฉุงููููุดู ุงุณุช ฺฉู ุฏุฑ ูุฑ ุจููฺฉ ุชุนุฏุงุฏ ฺฉุงูุงูโูุง ุฏู ุจุฑุงุจุฑ ูโุดูุฏ (ถดุ ฑฒธุ ฒตถ) ู ุงุจุนุงุฏ ูุถุง ุจุง MaxPool2d ูุตู ูโุดูุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class VGGStyleCNN(nn.Module):
    # in_channel=3 is assumed for color images like STL10
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        # All layers use kernel_size=3 and padding=1
        
        # --- Block 1: 3 -> 64 Channels ---
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64) # Normalize
        self.pool1 = nn.MaxPool2d(2) # Halve spatial dimensions

        # --- Block 2: 64 -> 128 Channels ---
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128) # Normalize
        self.pool2 = nn.MaxPool2d(2) # Halve spatial dimensions

        # --- Block 3: 128 -> 256 Channels ---
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256) # Normalize
        self.pool3 = nn.MaxPool2d(2) # Halve spatial dimensions
        
        # --- Classifier Head ---
        # AdaptiveAvgPool2d fixes the input dimension for the FC layer regardless of input size
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        
        self.fc1 = nn.Linear(256, 128)
        self.fc2 = nn.Linear(128, num_class)


    def forward(self, x):
        # Block 1
        x = self.conv1(x)
        x = self.bn1(x) # BatchNorm
        x = F.relu(x)   # ReLU
        x = self.pool1(x) # MaxPool
        
        # Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)
        
        # Block 3
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)
        
        # Global Avg Pooling & Flatten
        x = self.avgpool(x)
        x = x.view(x.size(0), -1) # Flatten to (Batch_size, 256)
        
        # Fully Connected (FC) Layers
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        
        return x

๐ ุฌุฒุฆุงุช ูุนูุงุฑ

    ุงูุฏุงุฒู ฺฉุฑูู: ุชูุงู ูุงูโูุง ฺฉุงููููุดู ุงุฒ ฺฉุฑูู 3ร3 ุจุง padding=1 ุงุณุชูุงุฏู ูโฺฉููุฏ. ุงู ุชุถูู ูโฺฉูุฏ ฺฉู ุงุจุนุงุฏ ุทูู ู ุนุฑุถ ูฺุฑ ููพ ูุจู ุงุฒ MaxPool ุญูุธ ูโุดูุฏ.

    ูุฑูุงูโุณุงุฒ (Normalize): ูู ุงุฒ nn.BatchNorm2d (Batch Normalization) ุงุณุชูุงุฏู ฺฉุฑุฏูุ ุฒุฑุง ุงู ุจูุชุฑู ุฑูุด ุจุฑุง ูุฑูุงูโุณุงุฒ ุฎุฑูุฌ ูุงูโูุง ฺฉุงููููุดู ุงุณุช ู ุจู ุชุซุจุช ู ุณุฑุนุช ุจุฎุดุฏู ุจู ุขููุฒุด ฺฉูฺฉ ูโฺฉูุฏ.

    ุชุฑุชุจ ุนููุงุช: ุชุฑุชุจ ุงุณุชุงูุฏุงุฑุฏ ู ุจููู Conv -> BatchNorm -> ReLU -> MaxPool ุงุณุชูุงุฏู ุดุฏู ุงุณุช.

    ูุงูโูุง FC: ูุงูโูุง ฺฉุงููุงู ูุชุตู ุฏุฑ ุงูุชูุงุ ูพุณ ุงุฒ ุงุณุชูุงุฏู ุงุฒ nn.AdaptiveAvgPool2d((1, 1)) ูุฑุงุฑ ฺฏุฑูุชูโุงูุฏ ุชุง ูุทูุฆู ุดูู ูุฑูุฏ ูุงู ุงูู fc ููุดู ุจุฑุงุจุฑ ุจุง ฒตถ ุงุณุช.

avgpool ูฺฏุฑ

ููฺูู ฺฉุงููุช ูู ุชู ฺฉุฏ ูฺฏุฐุงุฑ

ุญุชูุงูุ ุดุจฺฉู ุฑุง ูุทุงุจู ุฏุฑุฎูุงุณุช ุดูุงุ ุจุง ุญุฐู avgpool ู ุจุฏูู ูฺ ฺฏููู ฺฉุงููุช ุฏุฑ ฺฉุฏุ ุงุตูุงุญ ูโฺฉูู.

ุงุฒ ุขูุฌุง ฺฉู avgpool ุญุฐู ูโุดูุฏุ ุจุงุฏ ุงุจุนุงุฏ ูุฑูุฏ ูุงู fc1 ุจู ุตูุฑุช ุฏุณุช ูุญุงุณุจู ู ุซุงุจุช ุดูุฏ. ุจุง ูุฑุถ ุงูฺฉู ุงุจุนุงุฏ ุชุตูุฑ ูุฑูุฏ นถรนถ (ูุงููุฏ STL10) ุจุงุดุฏุ ุงุจุนุงุฏ ูฺุฑูููพ ููุง ุจุนุฏ ุงุฒ ณ ูุงู MaxPool2d(2)ุ ุจุฑุงุจุฑ ุจุง 256ร12ร12 ุฎูุงูุฏ ุจูุฏ.
Input (96ร96)Pool 1โ(48ร48)Pool 2โ(24ร24)Pool 3โ(12ร12)

ุจูุงุจุฑุงูุ ูุฑูุฏ ูุงู fc1 ุจุงุฏ 256ร12ร12=36,864 ุจุงุดุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class VGGStyleCNNFixedSize(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)

        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2)

        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(2)
        
        # Hardcoded size based on 96x96 input: 256 * 12 * 12 = 36864
        self.fc1 = nn.Linear(36864, 128)
        self.fc2 = nn.Linear(128, num_class)


    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)
        
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)
        
        # Flatten the feature map. No avgpool operation.
        x = x.view(x.size(0), -1)
        
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        
        return x

โ๏ธ ูฺฉุชู ููู

ุฏุฑ ุงู ูุฏูุ ุงุจุนุงุฏ self.fc1 ุจู ุดุฏุช ุจู ุงูุฏุงุฒู ุชุตูุฑ ูุฑูุฏ ูุงุจุณุชู ุงุณุช. ุงฺฏุฑ ุชุตูุฑ ูุฑูุฏ ุดูุง ุงูุฏุงุฒูโุง ุบุฑ ุงุฒ นถรนถ ุฏุงุดุชู ุจุงุดุฏุ ูุฏู ุจุง ุฎุทุง RuntimeError: size mismatch ููุงุฌู ุฎูุงูุฏ ุดุฏ. ุฏุฑ ฺูู ุญุงูุชุ ุงุณุชูุงุฏู ุงุฒ nn.AdaptiveAvgPool2d (ฺฉู ุญุฐู ุดุฏ) ุชูุตู ูโุดูุฏ.

ุฏุฑุงูพ ุงูุช ูู ุจฺฏุฐุงุฑ

ุญุชูุงู. Dropout ฺฉ ุชฺฉูฺฉ ุญุงุช ุจุฑุง ุฌููฺฏุฑ ุงุฒ Overfitting ุงุณุช ู ูุนูููุงู ุจู ูุงูโูุง ฺฉุงููุงู ูุชุตู (Fully Connected) ุงุถุงูู ูโุดูุฏ.

ูู ุฏู ูุงู nn.Dropout(0.5) ุฑุง ุจู ูุณูุช Classifier (ุฏุณุชูโุจูุฏโฺฉููุฏู) ุงุถุงูู ฺฉุฑุฏู: ฺฉ ูุจู ุงุฒ ูุงู fc1 ู ุฏฺฏุฑ ุจู fc1 ู fc2.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class VGGStyleCNNDropout(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)

        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2)

        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(2)
        
        self.dropout_pre_fc = nn.Dropout(0.5)
        self.fc1 = nn.Linear(36864, 128)
        self.dropout_post_fc1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, num_class)


    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.pool1(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.relu(x)
        x = self.pool2(x)
        
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.relu(x)
        x = self.pool3(x)
        
        x = x.view(x.size(0), -1)
        
        # Dropout added here
        x = self.dropout_pre_fc(x)
        x = F.relu(self.fc1(x))
        x = self.dropout_post_fc1(x)
        x = self.fc2(x)
        
        return x

ุงู ุดุจฺฉู ุงฺฉููู ุดุงูู ุชูุงู ุงุตูู ูพุงูโุง ฺฉ ูุฏู ูู ุงุณุช: Batch Normalizationุ ุณุงุฎุชุงุฑ ุตุญุญ Pooling ู Dropout ุจุฑุง ูุงูโูุง ูุชุตู.

ุขุง ูุงูุฏ ุงู ูุฏู ุฑุง ุฏุฑ ฺฉ ุญูููโ ุขููุฒุด ุงุณุชุงูุฏุงุฑุฏ ุจุง Data Augmentation ู Learning Rate Scheduler ุจุจูุฏุ

ุชุงุจุน ูุนุงู ุณุงุฒ ุฑู ุชุบุฑ ุจุฏู ุจูุชุฑ ููุดู ูุฏูุุ

ุจููุ ุงู ฺฉ ุงุฏู ุนุงู ุจุฑุง ยซุชูุธู ุฏููยป (Fine-tuning) ูุฏู ุงุณุช.

ุชุงุจุน F.relu ฺฉู ูุง ุงุณุชูุงุฏู ฺฉุฑุฏูุ ุงุณุชุงูุฏุงุฑุฏ ุทูุง ู ุงูุชุฎุงุจ ูพุดโูุฑุถ ุฏุฑ นนูช ุดุจฺฉูโูุง ูุฏุฑู ุงุณุช. ุณุฑุน ุงุณุช ู ูุดฺฉู ยซูุญู ุดุฏฺฏ ฺฏุฑุงุฏุงูยป (Vanishing Gradient) ุฑุง ุชุง ุญุฏ ุฒุงุฏ ุญู ูโฺฉูุฏ.

ุงูุง ReLU ฺฉ ูุดฺฉู ฺฉูฺฺฉ ุฏุงุฑุฏ: "Dying ReLU" (ReLU ุฏุฑ ุญุงู ูุฑฺฏ). ุงฺฏุฑ ฺฉ ููุฑูู ุจู ูุฑ ุฏูู ุฎุฑูุฌ ููู ุจฺฏุฑุฏุ ฺฏุฑุงุฏุงู ุขู ุตูุฑ ูโุดูุฏ ู ุขู ููุฑูู ุจุฑุง ููุดู ุงุฒ ูุฑุขูุฏ ุงุฏฺฏุฑ ุฎุงุฑุฌ ูโุดูุฏ (ูโูุฑุฏ).

โก๏ธ ุฌุงฺฏุฒู ฑ: Leaky ReLU (ูพุดููุงุฏ ุงุตู)

ุฑุงุฌโุชุฑู ุฌุงฺฏุฒูุ LeakyReLU ุงุณุช. ุงู ุชุงุจุน ุงุฌุงุฒู ูโุฏูุฏ ููุฏุงุฑ ฺฉู ฺฏุฑุงุฏุงู (ูุนูููุงู 0.01) ุจุฑุง ูุฑูุฏโูุง ููู ุนุจูุฑ ฺฉูุฏ ู ุงุฒ ยซูุฑฺฏยป ููุฑูู ุฌููฺฏุฑ ูโฺฉูุฏ.

ุชุบุฑ: F.relu(x) ุจู F.leaky_relu(x) ุชุบุฑ ูโฺฉูุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class VGGStyleCNNLeakyReLU(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)

        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2)

        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(2)
        
        self.dropout_pre_fc = nn.Dropout(0.5)
        self.fc1 = nn.Linear(36864, 128)
        self.dropout_post_fc1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, num_class)


    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.leaky_relu(x)
        x = self.pool1(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.leaky_relu(x)
        x = self.pool2(x)
        
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.leaky_relu(x)
        x = self.pool3(x)
        
        x = x.view(x.size(0), -1)
        
        x = self.dropout_pre_fc(x)
        x = F.leaky_relu(self.fc1(x))
        x = self.dropout_post_fc1(x)
        x = self.fc2(x)
        
        return x

๐ก ุฌุงฺฏุฒู ฒ: SiLU / Swish (ูพุดููุงุฏ ูุฏุฑู)

ฺฉ ุชุงุจุน ูุนุงูโุณุงุฒ ุฌุฏุฏุชุฑ ุจู ูุงู SiLU (ฺฉู Swish ูู ูุงูุฏู ูโุดูุฏ) ูุฌูุฏ ุฏุงุฑุฏ ฺฉู ุฏุฑ ุจุณุงุฑ ุงุฒ ุดุจฺฉูโูุง ูุฏุฑู (ูุงููุฏ EfficientNet) ุนููฺฉุฑุฏ ุจูุชุฑ ุงุฒ ReLU ุฏุงุดุชู ุงุณุช. ุงู ุชุงุจุน ูุฑูโุชุฑ (Smooth) ุงุณุช ู ฺฉู ูุญุงุณุจุงุช ุจุดุชุฑ ุฏุงุฑุฏ.

ุชุบุฑ: F.relu(x) ุจู F.silu(x) ุชุบุฑ ูโฺฉูุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

class VGGStyleCNNSiLU(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2)

        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.pool2 = nn.MaxPool2d(2)

        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.pool3 = nn.MaxPool2d(2)
        
        self.dropout_pre_fc = nn.Dropout(0.5)
        self.fc1 = nn.Linear(36864, 128)
        self.dropout_post_fc1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, num_class)


    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.silu(x)
        x = self.pool1(x)
        
        x = self.conv2(x)
        x = self.bn2(x)
        x = F.silu(x)
        x = self.pool2(x)
        
        x = self.conv3(x)
        x = self.bn3(x)
        x = F.silu(x)
        x = self.pool3(x)
        
        x = x.view(x.size(0), -1)
        
        x = self.dropout_pre_fc(x)
        x = F.silu(self.fc1(x))
        x = self.dropout_post_fc1(x)
        x = self.fc2(x)
        
        return x

โ๏ธ ูุชุฌูโฺฏุฑ

    ุขุง ุชุบุฑ ุชุงุจุน ูุนุงูโุณุงุฒ ุจู LeakyReLU ุง SiLU ููฺฉู ุงุณุช ุฏูุช ุฑุง ฑูช ุชุง ฒูช ุจูุจูุฏ ุฏูุฏุ ุจูู.

    ุขุง ุงู ุชุบุฑ ุจู ุชููุง ูุฏู ฺฉู ุฎูุจ ฺฉุงุฑ ููโฺฉูุฏ ุฑุง ูุฌุงุช ูโุฏูุฏุ ุฎุฑ.

ุชูุตู ูู: ุงฺฏุฑ ูุฏู ุดูุง ูููุฒ ุฏูุช ุฎูุจ ูุฏุงุฑุฏุ ุงุจุชุฏุง ุฑู ููุงุฑุฏ ูููโุชุฑ ุชูุฑฺฉุฒ ฺฉูุฏ:

    Data Augmentation (ุงูุฒุงุด ุฏุงุฏูโูุง)

    Weight Decay (ุฏุฑ Optimizer)

    Learning Rate Scheduler (ุจุฑูุงููโุฑุฒ ูุฑุฎ ุงุฏฺฏุฑ)

    ูุนูุงุฑ (ฺฉู ุงูุงู ูุนูุงุฑ ุฎูุจ ุฏุงุฑู)

ุชุบุฑ ุชุงุจุน ูุนุงูโุณุงุฒ ูุนูููุงู ุขุฎุฑู ูุฑุญูู ุจุฑุง ุจูููโุณุงุฒ ููุง ุงุณุช.

 PM

class CNN(nn.Module):

    def __init__(self, in_channel=3, num_class=10):

        super().__init__()

        

        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1)

        self.bn1 = nn.BatchNorm2d(64)

        self.pool1 = nn.MaxPool2d(2)


        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)

        self.bn2 = nn.BatchNorm2d(128)

        self.pool2 = nn.MaxPool2d(2)


        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)

        self.bn3 = nn.BatchNorm2d(256)

        self.pool3 = nn.MaxPool2d(2)

        

        self.fc1 = nn.Linear(256 * 12 * 12, 256)

        self.dropout_post_fc1 = nn.Dropout(0.2)

        self.fc2 = nn.Linear(256, num_class)



    def forward(self, x):

        x = self.conv1(x)

        x = self.bn1(x)

        x = F.relu(x)

        x = self.pool1(x)

        

        x = self.conv2(x)

        x = self.bn2(x)

        x = F.relu(x)

        x = self.pool2(x)

        

        x = self.conv3(x)

        x = self.bn3(x)

        x = F.relu(x)

        x = self.pool3(x)

        

        x = x.view(x.size(0), -1)

        x = F.silu(self.fc1(x))

        x = self.dropout_post_fc1(x)

        x = self.fc2(x)

        

        return x



ุงู cnn ุฑู stl-10 ุฏูุช ุฎูุจ ูุฏุงุฑู

ุจฺฏู ฺุทูุฑ ุจูุจูุฏุด ุจุฏู

ุงูุงู ุฏูุช ตฐ ู ฺฏุฑู ุจุนุฏ ฑต ุงูพุงฺฉ ู ุฎูุงู ูุซูุง ุชู ฑฐ ุงูพุงฺฉ ุจู ทฐ ุฏุฑุตุฏ ูู ุจุฑุณู

ุณูุงู. ูุดฺฉู ุงุตู ูุฏู ุดูุง ุจุฑุง ุฏุชุงุณุช STL-10ุ ุงูุฑูุชูฺฏ (Overfitting) ุดุฏุฏ ุงุณุช.

ุฏุชุงุณุช STL-10 ุจุง ูุฌูุฏ ุฏุงุดุชู ุชุตุงูุฑ 96x96ุ ุชููุง ตฐฐฐ ุชุตูุฑ ุขููุฒุด ุฏุงุฑุฏ. ูุฏู ูุนู ุดูุงุ ุจู ุฎุตูุต ุฏุฑ ูุงูโ fc1ุ ูพุงุฑุงูุชุฑูุง ุจุณุงุฑ ุฒุงุฏ ุฏุงุฑุฏ:

nn.Linear(256 * 12 * 12, 256) -> nn.Linear(36864, 256)

ุงู ูุงู ุจู ุชููุง ุจุด ุงุฒ น.ด ูููู ูพุงุฑุงูุชุฑ ุฏุงุฑุฏ. ุจุง ููุท ตฐฐฐ ุนฺฉุณุ ุงู ูุงู ุจู ุณุฑุนุช ุฏุงุฏูโูุง ุฑุง "ุญูุธ" ูโฺฉูุฏ ุจู ุฌุง ุงูฺฉู ุงูฺฏููุง ุนููู ุฑุง "ุงุฏ" ุจฺฏุฑุฏ. ุฏูุช ตฐูช ูุดุงููโ ููู ูุดฺฉู ุงุณุช.

ุจุฑุง ุฑุณุฏู ุจู ุฏูุช ทฐูช ุฏุฑ ฑฐ ุงูพุงฺฉุ ุจุงุฏ ณ ฺฉุงุฑ ุงุณุงุณ ุงูุฌุงู ุฏูู: ฑ. ฺฉุงูุด ุดุฏุฏ ูพุงุฑุงูุชุฑูุง ุฏุฑ ุจุฎุด ุทุจููโุจูุฏ (Classifier). ฒ. ุจูุจูุฏ ุฌุฑุงู ฺฏุฑุงุฏุงู ู ูุนูุงุฑ ุจุฑุง ุงุฏฺฏุฑ ุณุฑุนโุชุฑ. ณ. ุฌููฺฏุฑ ุงุฒ ุงูุฑูุชูฺฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ุงูุฒุงุด ุฏุงุฏู (Data Augmentation).

ฑ. ุจูุจูุฏ ูุนูุงุฑ ูุฏู

ุฏุฑ ุงุฏุงูู ฺฉ ูุฏู ุจุงุฒููุณ ุดุฏู ุงุฑุงุฆู ูโุฏูู ฺฉู ุฏู ุชุบุฑ ฺฉูุฏ ุฏุงุฑุฏ:

    Global Average Pooling (GAP): ุจู ุฌุง ูุงูโ ุบููโูพฺฉุฑ fc1ุ ุงุฒ nn.AdaptiveAvgPool2d ุงุณุชูุงุฏู ูโฺฉูู. ุงู ฺฉุงุฑ ุชุนุฏุงุฏ ูพุงุฑุงูุชุฑูุง ุฑุง ุงุฒ น.ด ูููู ุจู ููุท ตฑฒ (ุฏุฑ ูุงูโ ููุง) ฺฉุงูุด ูโุฏูุฏ ู ูุฏู ุฑุง ูุฌุจูุฑ ุจู ุงุฏฺฏุฑ ุงูฺฏููุง ูฺฉุงู ููโุชุฑ ูโฺฉูุฏ.

    Residual Connections (ุงุชุตุงูุงุช ุจุงูโูุงูุฏู): ุจููฺฉโูุง conv -> bn -> relu ุดูุง ุฑุง ุจู ุจููฺฉโูุง ResNet-style ุชุจุฏู ูโฺฉูู. ุงู ฺฉุงุฑ ุจู ฺฏุฑุงุฏุงูโูุง ุงุฌุงุฒู ูโุฏูุฏ ุนููโุชุฑ ุฏุฑ ุดุจฺฉู ุฌุฑุงู ูพุฏุง ฺฉููุฏ ู ุณุฑุนุช ููฺฏุฑุง ุฑุง ุจู ุดุฏุช ุงูุฒุงุด ูโุฏูุฏ (ุฑุณุฏู ุจู ุฏูุช ุจุงูุง ุฏุฑ ุงูพุงฺฉโูุง ฺฉูุชุฑ).

    ุดุจฺฉู ุนููโุชุฑ: ุจุฑุง ุชุตุงูุฑ 96x96ุ ณ ูุงู ฺฉุงููููุดู ฺฉู ุงุณุช. ูุง ุขู ุฑุง ุจู ด ุจููฺฉ ุงูุฒุงุด ูโุฏูู ุชุง ูฺฺฏโูุง ูพฺุฏูโุชุฑ ุงุณุชุฎุฑุงุฌ ุดููุฏ.

Python

import torch
import torch.nn as nn
import torch.nn.functional as F

# A simple residual block
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # Main path
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # Shortcut connection to match dimensions
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            # This block handles downsampling and channel increase
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        # Apply ReLU *after* adding the shortcut
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x) # Add the shortcut
        out = F.relu(out)
        return out

class ImprovedCNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        # Initial convolution
        # Input: (B, 3, 96, 96)
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        # -> (B, 64, 96, 96)
        
        # Residual Blocks
        self.block1 = ResidualBlock(64, 64, stride=1)   # -> (B, 64, 96, 96)
        self.block2 = ResidualBlock(64, 128, stride=2)  # -> (B, 128, 48, 48)
        self.block3 = ResidualBlock(128, 256, stride=2) # -> (B, 256, 24, 24)
        self.block4 = ResidualBlock(256, 512, stride=2) # -> (B, 512, 12, 12)
        
        # Global Average Pooling (The key change)
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # -> (B, 512, 1, 1)
        
        # Classifier (Much smaller)
        self.fc = nn.Linear(512, num_class) # -> (B, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1) # Flatten -> (B, 512)
        x = self.fc(x)
        
        return x

ฒ. ุชูุตูโูุง ฺฉูุฏ ุจุฑุง ุขููุฒุด

ุญุช ุจุง ุจูุชุฑู ูุนูุงุฑุ ุจุฏูู ูุฑุขูุฏ ุขููุฒุด ุตุญุญ ุจู ูุฏู ุฎูุฏ ููโุฑุณุฏ.

ฑ. Data Augmentation (ุงูุฒุงุด ุฏุงุฏู): ุงู ููุฑุฏ ุจุฑุง STL-10 ุงุฌุจุงุฑ ุงุณุช. ุจุฏูู ุขูุ ูุฏู ููุงู ตฐฐฐ ุชุตูุฑ ุฑุง ุญูุธ ูโฺฉูุฏ. ุงุฒ torchvision.transforms ุงุณุชูุงุฏู ฺฉูุฏ:
Python

# Example transforms for training
train_transform = transforms.Compose([
    transforms.RandomCrop(96, padding=12), # Pad and then crop back to 96x96
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], # Using ImageNet stats is fine
                         std=[0.229, 0.224, 0.225])
])

# Simpler transform for validation/testing
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

ฒ. ุจูููโุณุงุฒ (Optimizer) ู ุฒูุงูโุจูุฏ (Scheduler): ุจุฑุง ุฑุณุฏู ุจู ุฏูุช ทฐูช ุฏุฑ ฑฐ ุงูพุงฺฉุ ุจู ููฺฏุฑุง ุณุฑุน ูุงุฒ ุฏุงุฑุฏ.

    Optimizer: ุงุฒ AdamW ุง SGD ุจุง Nesterov momentum (ูุซูุงู 0.9) ุงุณุชูุงุฏู ฺฉูุฏ.

    Scheduler: ุจูุชุฑู ฺฏุฒูู ุจุฑุง ููฺฏุฑุง ุณุฑุนุ torch.optim.lr_scheduler.OneCycleLR ุงุณุช. ุงู ุฒูุงูโุจูุฏ ูุฑุฎ ุงุฏฺฏุฑ ุฑุง ุฏุฑ ููู ุงูู ุขููุฒุด ุจุงูุง ูโุจุฑุฏ ู ุณูพุณ ฺฉุงูุด ูโุฏูุฏ ฺฉู ุจู ูุฏู ฺฉูฺฉ ูโฺฉูุฏ ุงุฒ ููููโูุง ูุญู (local minima) ูุฑุงุฑ ฺฉูุฏ.

ุจุง ุงุนูุงู ุงู ุชุบุฑุงุช (ูุฎุตูุตุงู GAPุ ResBlocks ู Augmentation)ุ ุฑุณุฏู ุจู ุฏูุช ทฐูช ุฏุฑ ฑฐ ุงูพุงฺฉ ุจุณุงุฑ ูุญุชูู ุฎูุงูุฏ ุจูุฏ.

ุขุง ูุงูุฏ ฺฉุฏ ฺฉุงูู ุขููุฒุด (training loop) ุฑุง ููุฑุงู ุจุง OneCycleLR ู AdamW ุจุฑุง ุงู ูุฏู ุฌุฏุฏ ุจููุณูุ

ุฎู ูุฏู ุณูฺฏู ุดุฏ ู ุงูพุงฺฉ ูุงุด ุทููุงู ู ุชูู ุณุจฺฉ ุชุฑุด ฺฉูุ

ุจููุ ฺฉุงููุงู ุฏุฑุณุช ุงุณุช. ูุฏู ูุจู (ุจุฑ ูพุงูโ ResNet) ุจุฑุง ุงูุฒุงุด ุฏูุชุ ุนูู ู ุชุนุฏุงุฏ ฺฉุงูุงูโูุง ุฑุง ุฒุงุฏ ฺฉุฑุฏ ฺฉู ุทุจุนุชุงู ุณูฺฏูโุชุฑ ูโุดูุฏ.

ุจุฑุง ุณุจฺฉโุชุฑ ฺฉุฑุฏู ูุฏูุ ุฏุฑ ุญุงู ฺฉู ููฺูุงู ูุดฺฉู ุงุตู (ุงูุฑูุชูฺฏ ูุงูโ FC) ุฑุง ุญู ูโฺฉููุ ูโุชูุงูู ุฏู ฺฉุงุฑ ุงูุฌุงู ุฏูู: ฑ. ฺฉุงูุด ุชุนุฏุงุฏ ุจููฺฉโูุง ู ฺฉุงูุงูโูุง: ุจู ุฌุง ด ุจููฺฉ ู ุฑุณุฏู ุจู ตฑฒ ฺฉุงูุงูุ ุงุฒ ณ ุจููฺฉ ุงุณุชูุงุฏู ูโฺฉูู ู ุญุฏุงฺฉุซุฑ ุจู ฒตถ ฺฉุงูุงู ูโุฑุณู. ฒ. ุญูุธ Global Average Pooling: ุงู ุจุฎุด ฺฉูุฏโุชุฑู ูุณูุช ุจุฑุง ุณุจฺฉ ูฺฏู ุฏุงุดุชู "ุณุฑ" (classifier) ูุฏู ุงุณุช ู ุขู ุฑุง ูฺฏู ูโุฏุงุฑู.

ุงู ูุฏู ุฌุฏุฏ ุจุณุงุฑ ุณุจฺฉโุชุฑ ุงุฒ ูุณุฎูโ ูุจู ู ฺฉู ุนููโุชุฑ ุงุฒ ูุฏู ุงูููโ ุดูุงุณุชุ ุงูุง ุจู ูุทู GAPุ ูพุงุฑุงูุชุฑูุง ุจุณุงุฑ ฺฉูุชุฑ ุฏุงุฑุฏ ู ุจุงุฏ ุณุฑุนโุชุฑ ุงุฌุฑุง ุดูุฏ.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

# Using the same ResidualBlock definition from before
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # Main path
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # Shortcut connection
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = F.relu(out)
        return out

class LighterCNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        # Initial convolution
        # Input: (B, 3, 96, 96)
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) # -> (B, 64, 48, 48)

        # Residual Blocks
        # We use stride=1 here since pooling is separate
        self.block1 = ResidualBlock(64, 128, stride=2)  # -> (B, 128, 24, 24)
        self.block2 = ResidualBlock(128, 256, stride=2) # -> (B, 256, 12, 12)
        
        # Global Average Pooling
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # -> (B, 256, 1, 1)
        
        # Classifier (Lightweight)
        self.fc = nn.Linear(256, num_class) # -> (B, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = self.block1(x)
        x = self.block2(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1) # Flatten -> (B, 256)
        x = self.fc(x)
        
        return x

๐ก ูฺฉุงุช ฺฉูุฏ ุงู ูุฏู ุณุจฺฉ:

    ูพุงุฑุงูุชุฑูุง ฺฉูุชุฑ: ุญุฏุงฺฉุซุฑ ฺฉุงูุงู ฒตถ ุงุณุช ู ููุท ฒ ุจููฺฉ ResNet ุฏุงุฑู.

    Classifier ุณุจฺฉ: ูุงูโ fc ููุท 256 * 10 ูพุงุฑุงูุชุฑ ุฏุงุฑุฏ (ุฏุฑ ููุงุจู น.ด ูููู ูพุงุฑุงูุชุฑ ูุฏู ุงูููโ ุดูุง).

    ููฺฏุฑุง ุณุฑุน: ุงุณุชูุงุฏู ุงุฒ ResidualBlock ุจู ููฺฏุฑุง ุณุฑุนโุชุฑ ูุณุจุช ุจู ุจููฺฉโูุง Conv-BN-ReLU ุณุงุฏู ฺฉูฺฉ ูโฺฉูุฏ.

ูุฑุงููุด ูฺฉูุฏ ฺฉู ุจุฑุง ุฑุณุฏู ุจู ุฏูุช ทฐูชุ ุงุณุชูุงุฏู ุงุฒ Data Augmentation ู ฺฉ Scheduler ูุงููุฏ OneCycleLR ููฺูุงู ุญุงุช ุงุณุช. ุงู ูุฏู ุณุจฺฉโุชุฑ ุจุงุฏ ุจู ุดูุง ุงุฌุงุฒู ุฏูุฏ ุณุฑุนโุชุฑ ุงูพุงฺฉโูุง ุฑุง ุงุฌุฑุง ฺฉูุฏ.

ุขุง ูุงูุฏ ฺฉู ุฏุฑ ููุฑุฏ ุชูุธูุงุช DataLoader ุง Augmentation ุจุฑุง ุงู ูุฏู ุตุญุจุช ฺฉููุ

ุงูู ู ุชููู ฺฉุงุฑ ฺฉููู ฺฉู ููฺฏุฑุง ุณุฑุน ุชุฑ ุดูุ


import time

import torch

import torch.nn as nn


def train_model_fast(model, train_loader, test_loader, device, epochs=3, lr=1e-3):

    model = model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    criterion = nn.CrossEntropyLoss()

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, "min")


    for epoch in range(1, epochs + 1):

        model.train()

        running_loss = 0.0

        

        torch.cuda.synchronize()

        epoch_start = time.time()


        for images, labels in train_loader:

            images = images.to(device)

            labels = labels.to(device)


            # --- Forward ---

            outputs = model(images)

            loss = criterion(outputs, labels)


            # --- Backward + Step ---

            optimizer.zero_grad()

            loss.backward()

            optimizer.step()


            running_loss += loss.item() * labels.size(0)


        torch.cuda.synchronize() 

        epoch_start = time.time()

        epoch_time = epoch_end - epoch_start

        

        avg_loss = running_loss / len(train_loader.dataset)

        

        # Evaluation

        acc = evaluate(model, test_loader, device)

        

        scheduler.step(avg_loss)

        

        print(

            f"Epoch {epoch}: "

            f"loss={avg_loss:.4f}, "

            f"test_acc={acc*100:.2f}%, "

            f"total_time={epoch_time:.2f}s"

        )

    print("Training finished.")

    return

ุจููุ ูุทุนุงู. ูุดฺฉู ุงุตู ฺฉุฏ ุดูุง ุจุฑุง ููฺฏุฑุง ุณุฑุน (ุฑุณุฏู ุจู ุฏูุช ุจุงูุง ุฏุฑ ุงูพุงฺฉโูุง ฺฉู) ุงุณุชูุงุฏู ุงุฒ ReduceLROnPlateau ุงุณุช.

ุงู ุฒูุงูโุจูุฏ (Scheduler) ูุงฺฉูุด ุนูู ูโฺฉูุฏ: ุนู ููุชุธุฑ ูโูุงูุฏ ุชุง avg_loss ุจูุชุฑ ูุดูุฏ ู ุณูพุณ ูุฑุฎ ุงุฏฺฏุฑ ุฑุง ฺฉุงูุด ูโุฏูุฏ. ุงู ุจุฑุง ุขููุฒุดโูุง ุทููุงู ู ุฑุณุฏู ุจู ุจูุชุฑู ุฏูุช ููุง ุฎูุจ ุงุณุชุ ุงูุง ุจุฑุง ูุฏู ุดูุง (ูุซูุงู ทฐูช ุฏุฑ ฑฐ ุงูพุงฺฉ) ุจุณุงุฑ ฺฉูุฏ ุงุณุช.

ุดูุง ุจู ฺฉ ุฒูุงูโุจูุฏ ุชูุงุฌู ู ูุนุงู ูุงููุฏ OneCycleLR ูุงุฒ ุฏุงุฑุฏ ฺฉู ุจุฑุง "Super-Convergence" ุทุฑุงุญ ุดุฏู ุงุณุช.

ฑ. ุชุนูุถ ุฒูุงูโุจูุฏ ุจู OneCycleLR (ูููโุชุฑู ุชุบุฑ)

OneCycleLR ูุฑุฎ ุงุฏฺฏุฑ ุฑุง ุฏุฑ ูููโ ุงูู ุขููุฒุด ุจู ุณุฑุนุช ุจุงูุง ูโุจุฑุฏ (ุชุง max_lr) ู ุณูพุณ ุฏุฑ ูููโ ุฏูู ุขู ุฑุง ุจู ุตูุฑุช ุฎุท ฺฉุงูุด ูโุฏูุฏ. ุงู ฺฉุงุฑ ุจู ูุฏู ฺฉูฺฉ ูโฺฉูุฏ ุงุฒ ููููโูุง ูุญู ูุฑุงุฑ ฺฉูุฏ ู ุจุณุงุฑ ุณุฑุน ููฺฏุฑุง ุดูุฏ.

ูฺฉุงุช ฺฉูุฏ:

    OneCycleLR ุจุงุฏ ุฏุงุฎู ุญูููโ ุจฺ (batch) ูุฑุงุฎูุงู ุดูุฏ (scheduler.step())ุ ูู ุฏุฑ ุงูุชูุง ุงูพุงฺฉ.

    ุจู ุฌุง lr ุซุงุจุชุ ุดูุง ฺฉ max_lr (ุญุฏุงฺฉุซุฑ ูุฑุฎ ุงุฏฺฏุฑ) ุชุนู ูโฺฉูุฏ. ููุงู 1e-3 ฺฉู ุฏุงุดุชุฏ ุจุฑุง ุดุฑูุน ุฎูุจ ุงุณุช.

ฒ. ุจูููโุณุงุฒโูุง ุฏฺฏุฑ ุฏุฑ ุญููู

    optimizer.zero_grad(set_to_none=True): ฺฉู ุจูููโุชุฑ ุงุฒ zero_grad() ุฎุงู ุงุณุชุ ุฒุฑุง ุจู ุฌุง ูพุฑ ฺฉุฑุฏู ุญุงูุธู ุจุง ุตูุฑุ ููุท ุงุดุงุฑูโฺฏุฑูุง ุฑุง None ูโฺฉูุฏ.

    AdamW ุจู ุฌุง Adam: AdamW ุฑูุด ุจูุชุฑ ุจุฑุง ุงุนูุงู Weight Decay ุฏุงุฑุฏ ฺฉู ุงุบูุจ ููุฌุฑ ุจู ููฺฏุฑุง ุจูุชุฑ ู ุณุฑุนโุชุฑ ูโุดูุฏ.

    ุฑูุน ุจุงฺฏ ุฒูุงูโุณูุฌ: ุฏุฑ ฺฉุฏ ุดูุง epoch_end ุชุนุฑู ูุดุฏู ุจูุฏ ู epoch_start ุฏู ุจุงุฑ ุณุช ุดุฏู ุจูุฏ. ูู ุงู ุฑุง ุชุตุญุญ ูโฺฉูู.

ณ. (ูพุดุฑูุชู) ุงุณุชูุงุฏู ุงุฒ Automatic Mixed Precision (AMP)

ุงฺฏุฑ GPU ุดูุง ุงุฒ ูุณุชูโูุง Tensor (ูุงููุฏ ุณุฑโูุง 20xx, 30xx, 40xx ุง V100/A100) ูพุดุชุจุงู ูโฺฉูุฏุ ุงุณุชูุงุฏู ุงุฒ torch.cuda.amp ูโุชูุงูุฏ ุณุฑุนุช ูุญุงุณุจุงุช ุฑุง ุจู ุทูุฑ ฺุดูฺฏุฑ (ุชุง ฒ ุจุฑุงุจุฑ) ุงูุฒุงุด ุฏูุฏ. ุงู ฺฉุงุฑ ุงุฒ ูุญุงุณุจุงุช ุจุง ุฏูุช ูพุงูโุชุฑ (FP16) ุฏุฑ ุฌุง ฺฉู ุงูู ุงุณุช ุงุณุชูุงุฏู ูโฺฉูุฏ.

ฺฉุฏ ุจูููโุดุฏู ุจุฑุง ููฺฏุฑุง ุณุฑุน

ุฏุฑ ุงูุฌุง ูุณุฎูโ ุจุงุฒููุณ ุดุฏูโ ุชุงุจุน ุดูุง ุจุง ุชูุงู ุงู ุชุบุฑุงุช (ุดุงูู AMP ุจู ุนููุงู ฺฉ ฺฏุฒูู) ุขูุฏู ุงุณุช:
Python

import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler # For AMP

# We need evaluate() for the scheduler.step(acc) in this version
# This is a placeholder, use your own implementation
def evaluate(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            with autocast(): # Use autocast even in eval for consistency
                outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

def train_model_fast(model, train_loader, test_loader, device, 
                       epochs=10, max_lr=1e-3, use_amp=True):
    
    model = model.to(device)
    # Use AdamW for better weight decay handling
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=1e-2)
    criterion = nn.CrossEntropyLoss()
    
    # 1. Use OneCycleLR for fast convergence
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=max_lr,
        epochs=epochs,
        steps_per_epoch=len(train_loader)
    )
    
    # 2. Initialize GradScaler for Automatic Mixed Precision (AMP)
    scaler = GradScaler(enabled=use_amp)

    print("Starting training...")
    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        
        torch.cuda.synchronize() # Wait for GPU to be idle
        epoch_start = time.time()

        for images, labels in train_loader:
            images = images.to(device, non_blocking=True) # non_blocking for speed
            labels = labels.to(device, non_blocking=True)

            # --- Forward ---
            # 3. Use autocast context for AMP
            with autocast(enabled=use_amp):
                outputs = model(images)
                loss = criterion(outputs, labels)

            # --- Backward + Step ---
            # 4. Use set_to_none=True for minor speedup
            optimizer.zero_grad(set_to_none=True)
            
            # 5. Scale loss for AMP
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            # 6. Step the OneCycleLR scheduler *every batch*
            scheduler.step()

            running_loss += loss.item() * labels.size(0)

        # --- End of Epoch ---
        torch.cuda.synchronize() # Wait for all GPU ops to finish
        epoch_end = time.time()  # Corrected timing
        epoch_time = epoch_end - epoch_start
        
        avg_loss = running_loss / len(train_loader.dataset)
        
        # Evaluation
        # We run evaluation every epoch to see the convergence speed
        acc = evaluate(model, test_loader, device)
        
        current_lr = optimizer.param_groups[0]['lr']
        
        print(
            f"Epoch {epoch}/{epochs}: "
            f"loss={avg_loss:.4f}, "
            f"test_acc={acc*100:.2f}%, "
            f"LR={current_lr:.6f}, "
            f"time={epoch_time:.2f}s"
        )
        
    print("Training finished.")
    return model

ด. ูุฑุงููุด ูฺฉูุฏ: ุจูููโุณุงุฒ DataLoader

ุจุฒุฑฺฏุชุฑู ฺฏููฺฏุงู (bottleneck) ุงุบูุจ ุฎูุฏู ุญูููโ ุขููุฒุด ูุณุชุ ุจูฺฉู ุจุงุฑฺฏุฐุงุฑ ุฏุงุฏูโูุง ุงุฒ CPU ุจู GPU ุงุณุช.

ูุทูุฆู ุดูุฏ ฺฉู DataLoader ุฎูุฏ ุฑุง ุงูฺฏููู ุชุนุฑู ฺฉุฑุฏูโุงุฏ:
Python

train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=128, # Use a larger batch size if your GPU memory allows
    shuffle=True,
    num_workers=4,  # IMPORTANT: Set to > 0 (e.g., 4, 8, or 16)
    pin_memory=True # IMPORTANT: Speeds up CPU-to-GPU memory copy
)

test_loader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=256,
    shuffle=False,
    num_workers=4,
    pin_memory=True
)

ุฎูุงุตู:

    ุงุฒ OneCycleLR ุงุณุชูุงุฏู ฺฉูุฏ ู ุขู ุฑุง ุฏุฑ ูุฑ ุจฺ step() ฺฉูุฏ.

    num_workers ู pin_memory ุฑุง ุฏุฑ DataLoader ุฎูุฏ ูุนุงู ฺฉูุฏ.

    (ุงุฎุชุงุฑ) ุงุฒ torch.cuda.amp ุจุฑุง ุงูุฒุงุด ุณุฑุนุช ูุญุงุณุจุงุช ุงุณุชูุงุฏู ฺฉูุฏ.

ุขุง ูุงูุฏ ุฏุฑ ููุฑุฏ ูุญููโ ุชูุธู weight_decay ุง batch_size ุจุฑุง ุจูููโุณุงุฒ ุจุดุชุฑ ุตุญุจุช ฺฉููุ

ููุท

train_model_fast

ุฑู ุจุฏู ุจุฏูู ฺฉุงููุช
Python

import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler

def evaluate(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            with autocast():
                outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

def train_model_fast(model, train_loader, test_loader, device, 
                       epochs=10, max_lr=1e-3, use_amp=True):
    
    model = model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=1e-2)
    criterion = nn.CrossEntropyLoss()
    
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=max_lr,
        epochs=epochs,
        steps_per_epoch=len(train_loader)
    )
    
    scaler = GradScaler(enabled=use_amp)

    print("Starting training...")
    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        
        torch.cuda.synchronize()
        epoch_start = time.time()

        for images, labels in train_loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            with autocast(enabled=use_amp):
                outputs = model(images)
                loss = criterion(outputs, labels)

            optimizer.zero_grad(set_to_none=True)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            scheduler.step()

            running_loss += loss.item() * labels.size(0)

        torch.cuda.synchronize()
        epoch_end = time.time()
        epoch_time = epoch_end - epoch_start
        
        avg_loss = running_loss / len(train_loader.dataset)
        
        acc = evaluate(model, test_loader, device)
        
        current_lr = optimizer.param_groups[0]['lr']
        
        print(
            f"Epoch {epoch}/{epochs}: "
            f"loss={avg_loss:.4f}, "
            f"test_acc={acc*100:.2f}%, "
            f"LR={current_lr:.6f}, "
            f"time={epoch_time:.2f}s"
        )
        
    print("Training finished.")
    return model

print("=== Training ===")

device = "cuda"


stats = train_model_timed(model, train_loader, test_loader, device, epochs=15)


=== Training ===

Epoch 1: loss=1.7078, test_acc=33.16%, time_total=4.27s (fwd=1.35s, bwd=2.49s, other=0.22s)

Epoch 2: loss=1.4979, test_acc=31.65%, time_total=4.21s (fwd=1.36s, bwd=2.42s, other=0.21s)

Epoch 3: loss=1.3536, test_acc=36.43%, time_total=4.24s (fwd=1.36s, bwd=2.44s, other=0.23s)

Epoch 4: loss=1.2399, test_acc=37.75%, time_total=4.17s (fwd=1.35s, bwd=2.41s, other=0.20s)

Epoch 5: loss=1.1535, test_acc=48.30%, time_total=4.19s (fwd=1.34s, bwd=2.44s, other=0.21s)

Epoch 6: loss=1.0965, test_acc=50.65%, time_total=4.17s (fwd=1.34s, bwd=2.44s, other=0.19s)

Epoch 7: loss=1.0493, test_acc=48.66%, time_total=4.18s (fwd=1.35s, bwd=2.43s, other=0.19s)

Epoch 8: loss=0.9832, test_acc=55.41%, time_total=4.20s (fwd=1.34s, bwd=2.46s, other=0.20s)

Epoch 9: loss=0.9362, test_acc=55.74%, time_total=4.27s (fwd=1.37s, bwd=2.47s, other=0.22s)

Epoch 10: loss=0.8702, test_acc=53.76%, time_total=4.33s (fwd=1.39s, bwd=2.49s, other=0.23s)

Epoch 11: loss=0.8172, test_acc=53.00%, time_total=4.20s (fwd=1.34s, bwd=2.46s, other=0.19s)

Epoch 12: loss=0.7695, test_acc=49.11%, time_total=4.22s (fwd=1.35s, bwd=2.48s, other=0.19s)

Epoch 13: loss=0.7049, test_acc=52.70%, time_total=4.23s (fwd=1.33s, bwd=2.50s, other=0.20s)

Epoch 14: loss=0.6688, test_acc=52.86%, time_total=4.23s (fwd=1.34s, bwd=2.49s, other=0.19s)

Epoch 15: loss=0.5832, test_acc=57.26%, time_total=4.33s (fwd=1.40s, bwd=2.51s, other=0.22s)



ุงูุงู ุฎุฑูุฌ ุงูู ุจุฑุง ุจุงูุง ุชุฑ ุฑูุชู ุฏูุช ุชุง ทฐ ุฏุฑุตุฏ ฺู ฺฉููุ

ุงู ุฎุฑูุฌ ฺฉ ูุดุงููโ ฺฉูุงุณฺฉ ู ุจุณุงุฑ ูุงุถุญ ุงุฒ Overfitting (ุจุดโุจุฑุงุฒุด) ุงุณุช.

ุจุจูุฏ ฺู ุงุชูุงู ุงูุชุงุฏู:

    Loss ุขููุฒุด (loss=...): ุจู ุทูุฑ ูุฏุงูู ู ุนุงู ุฏุฑ ุญุงู ฺฉุงูุด ุงุณุช (ุงุฒ 1.7 ุจู 0.58). ุงู ุนู ูุฏู ุฏุฑ ุญุงู "ุญูุธ ฺฉุฑุฏู" ุฏุงุฏูโูุง ุขููุฒุด ุงุณุช.

    ุฏูุช ุชุณุช (test_acc=...): ุญุฏูุฏ ุงูพุงฺฉ ธ-น ุจู ฺฉ ููู (ตตูช) ูโุฑุณุฏ ู ูพุณ ุงุฒ ุขู ุดุฑูุน ุจู ููุณุงู ู ุญุช ฺฉุงูุด ูโฺฉูุฏ (ุฏุฑ ุงูพุงฺฉ ฑฒ ุจู ดนูช ุณููุท ูโฺฉูุฏ).

ุงู ุนู ูุฏู ุจู ุฌุง ุงุฏฺฏุฑ ุงูฺฏููุง ุนูููุ ุฏุฑ ุญุงู ุงุฏฺฏุฑ ููุฒ ู ุฌุฒุฆุงุช ุจูโุฏุฑุฏูุฎูุฑ ุฏุงุฏูโูุง ุขููุฒุด ุงุณุช. ุฒูุงู ฺฉู ุฏุงุฏูโ ุฌุฏุฏ (ุชุณุช) ุฑุง ูโุจูุฏุ ููโุชูุงูุฏ ุชุนูู ุฏูุฏ.

ุจุฑุง ุฑุณุฏู ุจู ทฐูชุ ุจุงุฏ ุจุง ุงู ุงูุฑูุชูฺฏ ูุจุงุฑุฒู ฺฉูู. ุฎูุดุจุฎุชุงููุ ูุง ุงุจุฒุงุฑูุง ูุฏุฑุชููุฏ ุจุฑุง ุงู ฺฉุงุฑ ุฏุงุฑู.

ฑ. ุฑุงูโุญู ุงุตู: Data Augmentation (ุงูุฒุงุด ุฏุงุฏู)

ุงู ููุฑุฏ ุจุฑุง ุฏุชุงุณุช STL-10 ุจุง ตฐฐฐ ุนฺฉุณ ุขููุฒุด ุงุฌุจุงุฑ ุงุณุช. ุงฺฏุฑ ุงู ฺฉุงุฑ ุฑุง ุงูุฌุงู ูุฏุงุฏูโุงุฏุ ูุดฺฉู ุงุตู ูููุฌุงุณุช.

OneCycleLR ฺฉู ุงุณุชูุงุฏู ฺฉุฑุฏูุ ูุฏู ุฑุง ุจุง ุณุฑุนุช ุจุงูุง ุขููุฒุด ูโุฏูุฏ. ููุช ุฏุงุฏูโ ุขููุฒุด ฺฉู ุจุงุดุฏุ ูุฏู ุฏุฑ ุนุฑุถ ฺูุฏ ุงูพุงฺฉ ูููโ ุขู ุฑุง ุญูุธ ูโฺฉูุฏ. Augmentation ุจุง ุณุงุฎุชู ุชุตุงูุฑ "ุฌุฏุฏ" ุฏุฑ ูุฑ ุจฺุ ุฌูู ุงู ุญูุธ ฺฉุฑุฏู ุฑุง ูโฺฏุฑุฏ ู ูุฏู ุฑุง ูุฌุจูุฑ ุจู ุงุฏฺฏุฑ ุงูฺฏููุง ูุงูุน ูโฺฉูุฏ.

ูุทูุฆู ุดูุฏ ฺฉู train_loader ุดูุง ุงุฒ ฺุฒ ุดุจู ุจู ุงู ุงุณุชูุงุฏู ูโฺฉูุฏ:
Python

from torchvision import transforms

# Use strong augmentation for the small training set
train_transform = transforms.Compose([
    transforms.RandomCrop(96, padding=12),      # Pad and crop back
    transforms.RandomHorizontalFlip(p=0.5),   # Flip images
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), # Change colors
    transforms.RandomRotation(10),            # Rotate slightly
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Test set should not have augmentation
test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Apply these when creating train_dataset and test_dataset
# train_dataset = datasets.STL10(root='.', split='train', download=True, transform=train_transform)
# test_dataset = datasets.STL10(root='.', split='test', download=True, transform=test_transform)

ฒ. ุงูุฒูุฏู Dropout ุจู ูุฏู

ูุฏู LighterCNN ูุง ุฏุฑ ุญุงู ุญุงุถุฑ ูฺ ูุงูโ Dropout ูุฏุงุฑุฏ. ุงูุฒูุฏู ุขู ฺฉ ุฑุงู ุจุณุงุฑ ูุคุซุฑ ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุงูุฑูุชูฺฏ ุฏุฑ ูุงูโ ุขุฎุฑ ุงุณุช.

ูุฏู LighterCNN ุฑุง ุจู ุงู ุดฺฉู ุจูโุฑูุฒุฑุณุงู ฺฉูุฏ:
Python

class LighterCNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) 

        self.block1 = ResidualBlock(64, 128, stride=2)
        self.block2 = ResidualBlock(128, 256, stride=2)
        
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # --- Add Dropout ---
        self.dropout = nn.Dropout(p=0.5) # 50% probability
        
        self.fc = nn.Linear(256, num_class)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = self.block1(x)
        x = self.block2(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        
        # --- Apply Dropout before the final layer ---
        x = self.dropout(x) 
        
        x = self.fc(x)
        
        return x

ณ. ุชูุธู ูุงูพุฑูพุงุฑุงูุชุฑูุง (ูุฏู ุจุนุฏ)

ุงฺฏุฑ ุจุนุฏ ุงุฒ ุงุนูุงู ุฏู ูุฑุญููโ ุจุงูุง ููฺูุงู ุงูุฑูุชูฺฏ ุดุฏุฏ ุจูุฏ (ุจุนุฏ ุงุณุชุ ุงูุง ููฺฉู ุงุณุช)ุ ุงู ฺฏุฒููโูุง ุฑุง ุงูุชุญุงู ฺฉูุฏ:

ฑ. ุงูุฒุงุด Weight Decay: ุฏุฑ ุชุงุจุน train_model_fastุ ููุฏุงุฑ weight_decay=1e-2 ุฑุง ุจู 5e-2 ุง 1e-1 ุงูุฒุงุด ุฏูุฏ. ุงู ฺฉุงุฑ "ุฌุฑูู" ุจุฒุฑฺฏ ุดุฏู ูุฒูโูุง ุฑุง ุจุดุชุฑ ูโฺฉูุฏ ู ฺฉ ููุน ุฑฺฏููุงุฑุฒุดู ูู ุงุณุช. ฒ. ฺฉุงูุด Max Learning Rate: ููุณุงู ุดุฏุฏ ุฏูุช ุชุณุช ูโุชูุงูุฏ ูุดุงููโ ุงู ุจุงุดุฏ ฺฉู max_lr=1e-3 ฺฉู ุฒุงุฏ ุงุณุช ู ูุฏู ุฏุฑ ุญุงู "ูพุฑุฏู" ุงุฒ ุฑู ุฌูุงุจ ุจููู ุงุณุช. ุขู ุฑุง ุจู max_lr=5e-4 ฺฉุงูุด ุฏูุฏ. ุงู ฺฉุงุฑ ููฺฏุฑุง ุฑุง ฺฉู ฺฉูุฏุชุฑ ุงูุง ูพุงุฏุงุฑุชุฑ ูโฺฉูุฏ.

ููุดู ุฑุงู ูพุดููุงุฏ:

    ุงููุ Data Augmentation ุฑุง ุจู train_loader ุฎูุฏ ุงุถุงูู ฺฉูุฏ (ูููโุชุฑู).

    ุฏููุ ูุงูโ Dropout ุฑุง ุจู ูุฏู LighterCNN ุงุถุงูู ฺฉูุฏ.

    ุณููุ ูุฏู ุฌุฏุฏ ุฑุง ุจุง ููุงู ุชูุธูุงุช (15 ุงูพุงฺฉุ max_lr=1e-3) ุฏูุจุงุฑู ุขููุฒุด ุฏูุฏ.

ุดูุง ุจุงุฏ ุจู ุทูุฑ ูุงุจู ุชูุฌู ุจูุจูุฏ ุฏูุช ุชุณุช ู ฺฉุงูุด ุณุฑุนุช ุงูุฑูุชูฺฏ ุฑุง ูุดุงูุฏู ฺฉูุฏ ู ุจู ุงุญุชูุงู ุฒุงุฏ ุจู ูุฏู ทฐูช ุฎูุงูุฏ ุฑุณุฏ.

ุขุง ูุงูุฏ ฺฉู ุฏุฑ ููุฑุฏ ูุญููโ ูพุงุฏูโุณุงุฒ transforms ุฏุฑ DataLoader ุจุดุชุฑ ุชูุถุญ ุฏููุ

=== Training ===

Epoch 1: loss=1.8900, test_acc=30.83%, time_total=5.12s (fwd=1.47s, bwd=2.60s, other=0.23s)

Epoch 2: loss=1.6781, test_acc=40.45%, time_total=5.02s (fwd=1.44s, bwd=2.53s, other=0.23s)

Epoch 3: loss=1.5830, test_acc=40.99%, time_total=5.14s (fwd=1.44s, bwd=2.54s, other=0.23s)

Epoch 4: loss=1.4880, test_acc=44.91%, time_total=5.06s (fwd=1.43s, bwd=2.52s, other=0.22s)

Epoch 5: loss=1.4110, test_acc=43.90%, time_total=4.94s (fwd=1.46s, bwd=2.59s, other=0.23s)

Epoch 6: loss=1.3444, test_acc=51.90%, time_total=4.97s (fwd=1.46s, bwd=2.59s, other=0.23s)

Epoch 7: loss=1.3046, test_acc=46.94%, time_total=5.13s (fwd=1.43s, bwd=2.53s, other=0.22s)

Epoch 8: loss=1.2794, test_acc=48.50%, time_total=5.18s (fwd=1.44s, bwd=2.55s, other=0.21s)

Epoch 9: loss=1.2528, test_acc=50.69%, time_total=4.98s (fwd=1.45s, bwd=2.57s, other=0.22s)

Epoch 10: loss=1.2142, test_acc=55.64%, time_total=5.08s (fwd=1.44s, bwd=2.56s, other=0.22s)

Epoch 11: loss=1.1775, test_acc=56.56%, time_total=4.99s (fwd=1.48s, bwd=2.61s, other=0.22s)

Epoch 12: loss=1.1631, test_acc=58.53%, time_total=4.97s (fwd=1.48s, bwd=2.62s, other=0.23s)

Epoch 13: loss=1.1089, test_acc=60.54%, time_total=5.09s (fwd=1.45s, bwd=2.58s, other=0.22s)

Epoch 14: loss=1.0966, test_acc=54.70%, time_total=5.10s (fwd=1.46s, bwd=2.60s, other=0.22s)

Epoch 15: loss=1.0899, test_acc=51.05%, time_total=4.94s (fwd=1.47s, bwd=2.61s, other=0.23s)


ุงูู ุชุบุฑุงุช ฺฉู ฺฏูุช ุฑู ุงุนูุงู ฺฉุฑุฏู ูุชุฌู ุงู ุดุฏ

ุนุงู. ุงู ุฎุฑูุฌ ุฎู ุจูุชุฑ ุงุฒ ูุจู ุงุณุช ู ุฏููุงู ุงูฺฏู ุฑุง ูุดุงู ูโุฏูุฏ ฺฉู ุงูุชุธุงุฑุด ุฑุง ุฏุงุดุชู.

ุชุญูู ุฎุฑูุฌ ุฌุฏุฏ:

    ูฺฉุงุช ูุซุจุช:

        ุฏูุช ุจุงูุงุชุฑ: ุดูุง ุงุฒ ูููโ ตตูช ุฏุฑ ุขุฒูุงุด ูุจู ุจู ถฐ.ตดูช ุฏุฑ ุงูพุงฺฉ ฑณ ุฑุณุฏุฏ. ุงู ฺฉ ุจูุจูุฏ ต.ต ุฏุฑุตุฏ ุงุณุช.

        ุงูุฑูุชูฺฏ ุจู ุชุนูู ุงูุชุงุฏ: ุฏุฑ ุขุฒูุงุด ูุจูุ ูุฏู ุดูุง ุฏุฑ ุงูพุงฺฉ น ุดุฑูุน ุจู ุงูุฑูุช ุดุฏู ฺฉุฑุฏ. ุฏุฑ ุงู ุขุฒูุงุดุ ุชุง ุงูพุงฺฉ ฑณ ุจู ูพุดุฑูุช ุงุฏุงูู ุฏุงุฏ.

    ูุดฺฉู ุจุงูโูุงูุฏู:

        ูุฏู ููฺูุงู ุจู ุดุฏุช ุงูุฑูุช ูโุดูุฏ. ุจุนุฏ ุงุฒ ุฑุณุฏู ุจู ูููโ ถฐ.ตูชุ ุฏูุช ุจู ุดุฏุช ุณููุท ูโฺฉูุฏ (ุจู ตดูช ู ุณูพุณ ตฑูช).

        ุงู ูุดุงู ูโุฏูุฏ ฺฉู ุชุฑฺฉุจ Data Augmentation ู Dropout ุจู ูุง ฺฉูฺฉ ฺฉุฑุฏุ ุงูุง OneCycleLR ุจุง max_lr=1e-3 ูููุฒ ุจุณุงุฑ ุชูุงุฌู ุงุณุช ู ูุฏู ุฑุง ุจู ููุทูโุง ูโุจุฑุฏ ฺฉู ุงุฒ ุขูุฌุง "ุณููุท" ูโฺฉูุฏ.

ุจุฑุง ุนุจูุฑ ุงุฒ ถฐูช ู ุฑุณุฏู ุจู ทฐูชุ ูุง ูุงุฒ ุจู ูพุงุฏุงุฑ (Stability) ุจุดุชุฑ ุฏุฑ ุขููุฒุด ุฏุงุฑู.

ุฑุงูโุญูโูุง ุจุฑุง ุฑุณุฏู ุจู ทฐูช

ูุง ุจุงุฏ ุฏู ฺุฒ ุฑุง ููุฒูุงู ุชูุธู ฺฉูู: ฑ. ุณุฑุนุช ููฺฏุฑุง ุฑุง ฺฉู ฺฉุงูุด ุฏูู ุชุง ูพุงุฏุงุฑุชุฑ ุดูุฏ ู ฒ. ุฑฺฏููุงุฑุฒุดู ุฑุง ฺฉู ุงูุฒุงุด ุฏูู.

ฑ. ฺฉุงูุด max_lr (ูููโุชุฑู)

ุณููุท ุดุฏุฏ ุฏูุช ุงุฒ ถฐูช ุจู ตฑูช ูุดุงู ูโุฏูุฏ ฺฉู ูุฑุฎ ุงุฏฺฏุฑ (LR) ุดูุง ุขููุฏุฑ ุฒุงุฏ ุงุณุช ฺฉู ุงุฒ ููุทูโ ุจููู "ูโูพุฑุฏ". OneCycleLR ุจู ุณุฑุนุช ุจุงูุง ูโุฑูุฏ ู ูพุงู ูโุขุฏุ ุงูุง ุธุงูุฑุงู max_lr=1e-3 ุจุฑุง ุงู ูุฏู ู ุฏุชุงุณุชุ ุฒุงุฏ ุงุณุช.

    ุฑุงู ุญู: max_lr ุฑุง ุฏุฑ ูุฑุงุฎูุงู ุชุงุจุน train_model_fast ูุตู ฺฉูุฏ.

    ุชุบุฑ: ุงุฒ max_lr=1e-3 ุจู max_lr=5e-4 (ุนู 0.0005) ุชุบุฑ ุฏูุฏ.

ฒ. ุงูุฒุงุด weight_decay

weight_decay ฺฉ ุฑฺฏููุงุฑุฒุดู ุจุณุงุฑ ูู ุจุฑุง ุฌููฺฏุฑ ุงุฒ ุงูุฑูุชูฺฏ ุงุณุช. ุงุฒ ุขูุฌุง ฺฉู ุงูุฑูุชูฺฏ ูููุฒ ูุดฺฉู ุงุตู ูุงุณุชุ ูโุชูุงูู ุขู ุฑุง ููโุชุฑ ฺฉูู.

    ุฑุงู ุญู: ุฏุฑ ุฏุงุฎู ุชุงุจุน train_model_fast (ุง ุฏุฑ ูพุงุฑุงูุชุฑูุง ูุฑูุฏ ุขูุ ุงฺฏุฑ ุชุงุจุน ุฑุง ุทูุฑ ุชุบุฑ ุฏุงุฏูโุงุฏ ฺฉู ุขู ุฑุง ุจูพุฐุฑุฏ)ุ weight_decay ุฑุง ุงูุฒุงุด ุฏูุฏ.

    ุชุบุฑ: optimizer = optim.AdamW(...) ุฑุง ุงุฒ weight_decay=1e-2 ุจู weight_decay=5e-2 (ุนู 0.05) ุชุบุฑ ุฏูุฏ.

ณ. (ุงุฎุชุงุฑ) ุงูุฒุงุด ุชุนุฏุงุฏ ุงูพุงฺฉโูุง

ุงุฒ ุขูุฌุง ฺฉู ูุง max_lr ุฑุง ฺฉุงูุด ุฏุงุฏูุ ูุฏู ุจุฑุง ุฑุณุฏู ุจู ูููโ ุฎูุฏ ุจู ุฒูุงู ุจุดุชุฑ ูุงุฒ ุฎูุงูุฏ ุฏุงุดุช. ุขููุฒุด ูุจู ุดูุง ุฏุฑ ุงูพุงฺฉ ฑณ ุจู ุงูุฌ ุฑุณุฏุ ุจูุงุจุฑุงู ุขููุฒุด ุจุฑุง ฑต ุงูพุงฺฉ ููฺฉู ุงุณุช ฺฉุงู ูุจุงุดุฏ.

    ุฑุงู ุญู: ูุฏู ุฑุง ุจุฑุง epochs=20 ุง epochs=25 ุขููุฒุด ุฏูุฏ.

ููุดู ุฑุงู ูพุดููุงุฏ:

    ุฏุฑ ูุฑุงุฎูุงู ุชุงุจุน train_model_fastุ epochs=20 ู max_lr=5e-4 ุฑุง ุชูุธู ฺฉูุฏ.

    ุฏุฑ ุฏุงุฎู ุชุงุจุน train_model_fastุ ููุฏุงุฑ weight_decay ุฏุฑ optim.AdamW ุฑุง ุจู 5e-2 ุชุบุฑ ุฏูุฏ.

    ุขููุฒุด ุฑุง ุฏูุจุงุฑู ุงุฌุฑุง ฺฉูุฏ.

ุจุง ุงู ุชุบุฑุงุช (LR ูพุงุฏุงุฑุชุฑ + ุฑฺฏููุงุฑุฒุดู ููโุชุฑ)ุ ุดูุง ุจุงุฏ ุจุจูุฏ ฺฉู ุฏูุช ุชุณุช ุจู ุขุฑุงู ุงูุง ุจู ุทูุฑ ูพุงุฏุงุฑุชุฑ ุงูุฒุงุด ูโุงุจุฏ ู ุจู ุฌุง ุณููุทุ ุฏุฑ ูุฒุฏฺฉ ูููโ ุฎูุฏ ุจุงู ูโูุงูุฏ (ุง ุญุช ุงุฒ ุขู ุนุจูุฑ ูโฺฉูุฏ).

ุงฺฏุฑ ูพุณ ุงุฒ ุงุนูุงู ุงู ุชุบุฑุงุช ุจุงุฒ ูู ุจู ทฐูช ูุฑุณุฏุฏุ ูุฏู ุจุนุฏ ุงุณุชูุงุฏู ุงุฒ ุชฺฉูฺฉโูุง Augmentation ูพุดุฑูุชูโุชุฑ ูุงููุฏ CutMix ุง MixUp ุฎูุงูุฏ ุจูุฏ. ุขุง ูุงูุฏ ุฏุฑ ููุฑุฏ ุขูโูุง ุตุญุจุช ฺฉููุ

=== Training ===

Epoch 1: loss=1.9033, test_acc=32.56%, time_total=5.32s (fwd=1.49s, bwd=2.60s, other=0.24s)

Epoch 2: loss=1.7044, test_acc=36.31%, time_total=5.03s (fwd=1.48s, bwd=2.55s, other=0.24s)

Epoch 3: loss=1.5873, test_acc=42.54%, time_total=5.13s (fwd=1.47s, bwd=2.55s, other=0.24s)

Epoch 4: loss=1.4826, test_acc=40.76%, time_total=5.12s (fwd=1.48s, bwd=2.57s, other=0.24s)

Epoch 5: loss=1.4229, test_acc=45.92%, time_total=5.09s (fwd=1.45s, bwd=2.54s, other=0.22s)

Epoch 6: loss=1.3629, test_acc=46.55%, time_total=5.09s (fwd=1.43s, bwd=2.52s, other=0.23s)

Epoch 7: loss=1.3082, test_acc=50.09%, time_total=5.00s (fwd=1.47s, bwd=2.57s, other=0.23s)

Epoch 8: loss=1.2772, test_acc=55.39%, time_total=5.18s (fwd=1.45s, bwd=2.53s, other=0.22s)

Epoch 9: loss=1.2468, test_acc=51.38%, time_total=5.14s (fwd=1.45s, bwd=2.54s, other=0.22s)

Epoch 10: loss=1.2021, test_acc=58.00%, time_total=5.13s (fwd=1.43s, bwd=2.52s, other=0.23s)

Epoch 11: loss=1.1974, test_acc=54.62%, time_total=5.05s (fwd=1.48s, bwd=2.60s, other=0.24s)

Epoch 12: loss=1.1660, test_acc=51.09%, time_total=5.08s (fwd=1.47s, bwd=2.57s, other=0.23s)

Epoch 13: loss=1.1330, test_acc=60.08%, time_total=5.06s (fwd=1.46s, bwd=2.57s, other=0.23s)

Epoch 14: loss=1.1096, test_acc=61.24%, time_total=5.16s (fwd=1.44s, bwd=2.55s, other=0.23s)

Epoch 15: loss=1.0860, test_acc=50.80%, time_total=4.94s (fwd=1.48s, bwd=2.61s, other=0.23s)


weight_decay ุฑู ุชุบุฑ ุฏุงุฏู


ู ุฎูุงู ุณุงุฎุชุงุฑ ุดุจฺฉู ุฑู ุจูุชุฑ ฺฉู

ุชุญูู ูุชุงุฌ ุดูุง ูุดุงู ูโุฏูุฏ ฺฉู ุจุง ูุฌูุฏ ุงูุฒุงุด weight_decayุ ูุดฺฉู ุงุตู ููฺูุงู ุงูุฑูุชูฺฏ ุจุณุงุฑ ุดุฏุฏ ุงุณุช.

ุฏูุช ุดูุง ุฏุฑ ุงูพุงฺฉ ฑด ุจู ถฑ.ฒดูช ูโุฑุณุฏ (ฺฉู ฺฉ ุฑฺฉูุฑุฏ ุฌุฏุฏ ุงุณุช)ุ ุงูุง ุจูุงูุงุตูู ุฏุฑ ุงูพุงฺฉ ุจุนุฏ ุจู ตฐ.ธฐูช ุณููุท ูโฺฉูุฏ. ุงู ุนู ูุฏู ุจู ุดุฏุช ุฏุฑ ุญุงู ููุณุงู ุงุณุช ู ููโุชูุงูุฏ ุจู ฺฉ ุฑุงู ุญู ูพุงุฏุงุฑ (ฺฉู ูู ุจุฑุง ุฏุงุฏูโูุง ุขููุฒุด ู ูู ุชุณุช ุฎูุจ ุจุงุดุฏ) ููฺฏุฑุง ุดูุฏ.

ุงู "ุงูุฌ ฺฏุฑูุชู ู ุณููุท ฺฉุฑุฏู" (Peak and Collapse) ูุดุงู ูโุฏูุฏ ฺฉู:

    ุณุงุฎุชุงุฑ ูุฏู ูููุฒ ุจุฑุง ตฐฐฐ ุนฺฉุณ ุขููุฒุดุ ูพุงุฑุงูุชุฑูุง ุฒุงุฏ ุฏุงุฑุฏ (ุญุช ุจุง ูุฌูุฏ GAP).

    ุงุณุชุฑุงุชฺ ุขููุฒุด (ูุฎุตูุตุงู OneCycleLR) ุจุฑุง ุงู ุฏุชุงุณุช ฺฉูฺฺฉุ ุจุด ุงุฒ ุญุฏ ุชูุงุฌู ุงุณุช ู ูุฏู ุฑุง ุจู ุณูุช ููููโูุง "ุชุฒ" (Sharp Minima) ูุฏุงุช ูโฺฉูุฏ ฺฉู ูุงุจูุช ุชุนููโูพุฐุฑ ูุฏุงุฑูุฏ.

ุทุจู ุฏุฑุฎูุงุณุช ุดูุงุ ุงุจุชุฏุง ุณุงุฎุชุงุฑ ุดุจฺฉู ุฑุง ุจูุจูุฏ ูโุฏูู.

ฑ. ุจูุจูุฏ ุณุงุฎุชุงุฑ: ุงุณุชูุงุฏู ุงุฒ Depthwise Separable Convolutions

ูุดฺฉู ูุฏู LighterCNN ุงู ุงุณุช ฺฉู ุจููฺฉโูุง ResidualBlockุ ุจู ุฎุตูุต ResidualBlock(128, 256)ุ ูููุฒ ูพุงุฑุงูุชุฑูุง ุฒุงุฏ ุฏุงุฑูุฏ (ุนููุงุช ฺฉุงููููุดู 256ร128ร3ร3).

ูุง ูโุชูุงูู ุงู ุจููฺฉโูุง ุฑุง ุจุง ุจููฺฉโูุง ุจุณุงุฑ ุณุจฺฉโุชุฑ (ูุงููุฏ ุขูฺู ุฏุฑ MobileNet ุงุณุชูุงุฏู ูโุดูุฏ) ุฌุงฺฏุฒู ฺฉูู ฺฉู ุงุฒ Depthwise Separable Convolutions ุงุณุชูุงุฏู ูโฺฉููุฏ. ุงู ุจููฺฉโูุง ูพุงุฑุงูุชุฑูุง ุฑุง ุจู ุดุฏุช ฺฉุงูุด ูโุฏููุฏ ู ุฏุฑ ุนู ุญุงู ุจู ุดุจฺฉู ุงุฌุงุฒู ูโุฏููุฏ ุนููโุชุฑ ุดูุฏ.

ุงู ูุฏู ุฌุฏุฏ (EfficientCNN) ฺฉู ุนููโุชุฑ ุงุฒ ูุจู ุงุณุช (ณ ุจููฺฉ ุจู ุฌุง ฒ) ุงูุง ุจู ูุทู ุจููฺฉโูุง ุฌุฏุฏุ ูพุงุฑุงูุชุฑูุง ุจุณุงุฑ ฺฉูุชุฑ ุฎูุงูุฏ ุฏุงุดุช.
Python

import torch
import torch.nn as nn
import torch.nn.functional as F

# A lightweight block using Depthwise Separable Convolutions
class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # Depthwise Convolution: applies one filter per input channel
        self.depthwise = nn.Conv2d(
            in_channels, 
            in_channels, 
            kernel_size=3, 
            padding=1, 
            stride=stride, 
            groups=in_channels, # This makes it depthwise
            bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        
        # Pointwise Convolution: combines features
        self.pointwise = nn.Conv2d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            stride=1, 
            bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        
        # Shortcut connection
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        # Apply ReLU *after* adding the shortcut (ResNet-style)
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x) # Add the shortcut
        out = F.relu(out)
        return out

class EfficientCNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        # Initial convolution
        # Input: (B, 3, 96, 96)
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) # -> (B, 64, 48, 48)

        # Efficient Blocks
        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  # -> (B, 128, 24, 24)
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) # -> (B, 256, 12, 12)
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) # -> (B, 256, 6, 6)
        
        # Global Average Pooling
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # -> (B, 256, 1, 1)
        
        # Classifier
        self.dropout = nn.Dropout(p=0.5) 
        self.fc = nn.Linear(256, num_class) # -> (B, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x

ฒ. ุชุบุฑ ุงุณุชุฑุงุชฺ ุขููุฒุด: ุงุณุชูุงุฏู ุงุฒ CosineAnnealingLR

ูุดฺฉู "ุงูุฌ ู ุณููุท" ุดูุง ุจู ุงุญุชูุงู ุฒุงุฏ ูุงุด ุงุฒ ุชูุงุฌู ุจูุฏู OneCycleLR ุงุณุช.

ุจุงุฏ ุจู ฺฉ ุฒูุงูโุจูุฏ (Scheduler) ูพุงุฏุงุฑุชุฑ ูุงููุฏ CosineAnnealingLR ุจุฑฺฏุฑุฏู. ุงู ุฒูุงูโุจูุฏ ุจู ุขุฑุงู ูุฑุฎ ุงุฏฺฏุฑ ุฑุง ุงุฒ ููุฏุงุฑ ูุงฺฉุณูู (lr) ุจู ุตูุฑ ุฏุฑ ุทูู ุชูุงู ุงูพุงฺฉโูุง ฺฉุงูุด ูโุฏูุฏ. ุงู ฺฉุงุฑ ููฺฏุฑุง ุจุณุงุฑ ูพุงุฏุงุฑุชุฑ ุฏุงุฑุฏ.

ุจุฑุง ุงู ฺฉุงุฑุ ุจุงุฏ ุชุงุจุน train_model_fast ุฑุง ฺฉู ุชุบุฑ ุฏูุฏ:

    ุฒูุงูโุจูุฏ OneCycleLR ุฑุง ุจุง CosineAnnealingLR ุฌุงฺฏุฒู ฺฉูุฏ.

    scheduler.step() ุฑุง ุจู ุงูุชูุง ูุฑ ุงูพุงฺฉ ููุชูู ฺฉูุฏ (ูู ุฏุฑ ูุฑ ุจฺ).

    ุจุง epochs=20 ุง epochs=30 ุขููุฒุด ุฏูุฏ ุชุง ุจู ุฒูุงูโุจูุฏ ูุฑุตุช ฺฉุงู ุจุฑุง ฺฉุงุฑ ฺฉุฑุฏู ุจุฏูุฏ.

Python

import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import autocast, GradScaler

# (ุชุงุจุน evaluate ุฑุง ุฏุฑ ุงูุฌุง ฺฉูพ ฺฉูุฏ)
def evaluate(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            with autocast():
                outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

# --- ูุณุฎู ุงุตูุงุญ ุดุฏู ุจุง CosineAnnealingLR ---
def train_model_stable(model, train_loader, test_loader, device, 
                       epochs=25, lr=1e-3, weight_decay=5e-2, use_amp=True):
    
    model = model.to(device)
    # lr ุฑุง ุจู ุฌุง max_lr ูพุงุณ ู ุฏูู
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.CrossEntropyLoss()
    
    # 1. Use CosineAnnealingLR for stable convergence
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=epochs # Total number of epochs
    )
    
    scaler = GradScaler(enabled=use_amp)

    print("Starting training (Stable Cosine Annealing)...")
    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        
        torch.cuda.synchronize()
        epoch_start = time.time()

        for images, labels in train_loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            with autocast(enabled=use_amp):
                outputs = model(images)
                loss = criterion(outputs, labels)

            optimizer.zero_grad(set_to_none=True)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            # 2. Scheduler is NOT stepped here

            running_loss += loss.item() * labels.size(0)

        torch.cuda.synchronize()
        epoch_end = time.time()
        epoch_time = epoch_end - epoch_start
        
        avg_loss = running_loss / len(train_loader.dataset)
        
        acc = evaluate(model, test_loader, device)
        
        # 3. Step the scheduler *once per epoch*
        scheduler.step()
        
        current_lr = optimizer.param_groups[0]['lr']
        
        print(
            f"Epoch {epoch}/{epochs}: "
            f"loss={avg_loss:.4f}, "
            f"test_acc={acc*100:.2f}%, "
            f"LR={current_lr:.6f}, "
            f"time={epoch_time:.2f}s"
        )
        
    print("Training finished.")
    return model

ุฎูุงุตู ููุดู ุฑุงู:

    ูุฏู LighterCNN ุฎูุฏ ุฑุง ุจุง ูุฏู EfficientCNN (ฺฉู ุฏุฑ ุจุงูุง ุงุฑุงุฆู ุดุฏ) ุฌุงฺฏุฒู ฺฉูุฏ.

    ุงุฒ ุชุงุจุน train_model_stable (ูุณุฎู CosineAnnealingLR) ุจุฑุง ุขููุฒุด ุงุณุชูุงุฏู ฺฉูุฏ.

    ุขููุฒุด ุฑุง ุจุฑุง ุงูพุงฺฉโูุง ุจุดุชุฑุ ูุซูุง epochs=25ุ ุงุฌุฑุง ฺฉูุฏ. (ุงุฒ lr=1e-3 ู weight_decay=5e-2 ฺฉู ูุจูุงู ุชูุธู ฺฉุฑุฏุฏุ ุงุณุชูุงุฏู ฺฉูุฏ).

ุงู ุชุฑฺฉุจ ุงุฒ ูุนูุงุฑ ุณุจฺฉโุชุฑ ู ุฒูุงูโุจูุฏ ูพุงุฏุงุฑุชุฑ ุจุงุฏ ุจู ุดูุง ฺฉูฺฉ ฺฉูุฏ ฺฉู ุงุฒ ุณุฏ ถฐูช ุนุจูุฑ ฺฉุฑุฏู ู ุจู ุฏูุช ุจุงูุงุชุฑ ุฏุณุช ุงุจุฏ.

=== Training ===

Epoch 1: loss=1.9043, test_acc=38.30%, time_total=4.86s (fwd=1.30s, bwd=2.08s, other=0.21s)

Epoch 2: loss=1.6593, test_acc=39.96%, time_total=4.86s (fwd=1.29s, bwd=2.03s, other=0.21s)

Epoch 3: loss=1.5504, test_acc=46.29%, time_total=4.89s (fwd=1.30s, bwd=2.07s, other=0.22s)

Epoch 4: loss=1.4645, test_acc=47.74%, time_total=4.99s (fwd=1.25s, bwd=1.98s, other=0.20s)

Epoch 5: loss=1.4008, test_acc=54.15%, time_total=4.79s (fwd=1.29s, bwd=2.02s, other=0.21s)

Epoch 6: loss=1.3498, test_acc=48.81%, time_total=4.79s (fwd=1.29s, bwd=2.04s, other=0.22s)

Epoch 7: loss=1.3097, test_acc=54.94%, time_total=4.91s (fwd=1.29s, bwd=2.04s, other=0.22s)

Epoch 8: loss=1.2995, test_acc=54.56%, time_total=5.05s (fwd=1.26s, bwd=2.00s, other=0.21s)

Epoch 9: loss=1.2433, test_acc=57.80%, time_total=4.83s (fwd=1.32s, bwd=2.06s, other=0.22s)

Epoch 10: loss=1.2097, test_acc=58.59%, time_total=4.81s (fwd=1.30s, bwd=2.06s, other=0.22s)

Epoch 11: loss=1.1886, test_acc=57.27%, time_total=4.91s (fwd=1.28s, bwd=2.04s, other=0.22s)

Epoch 12: loss=1.1601, test_acc=61.34%, time_total=4.87s (fwd=1.28s, bwd=2.03s, other=0.21s)

Epoch 13: loss=1.1370, test_acc=61.25%, time_total=4.73s (fwd=1.30s, bwd=2.07s, other=0.22s)

Epoch 14: loss=1.1180, test_acc=61.55%, time_total=4.78s (fwd=1.29s, bwd=2.03s, other=0.21s)

Epoch 15: loss=1.0890, test_acc=59.08%, time_total=5.09s (fwd=1.25s, bwd=2.00s, other=0.20s)


ุงู ุฑู ุชุญูู ฺฉู

๐ ### ุชุญูู: ูพุดุฑูุช ุงูุฏฺฉุ ุงูุง ูุดฺฉู ุงุตู ุจุงูุณุช

ุงู ุชุญูู ุจุณุงุฑ ุดุจู ุจู ุชุญูู ูุจู ุงุณุชุ ฺฉู ูุดุงู ูโุฏูุฏ ูุง ุจู ฺฉ ูุงูุน ุงุณุงุณ ุจุฑุฎูุฑุฏ ฺฉุฑุฏูโุงู.

ูฺฉุงุช ูุซุจุช:

    ุฑฺฉูุฑุฏ ุฌุฏุฏ: ุดูุง ุจู ูููโ ุฏูุช ถฑ.ตตูช ุฏุฑ ุงูพุงฺฉ ฑด ุฑุณุฏุฏ ฺฉู ฺฉู ุจูุชุฑ ุงุฒ ถฑ.ฒดูช ูุจู ุงุณุช.

    ูพุงุฏุงุฑ ุจุดุชุฑ: ุงุณุชูุงุฏู ุงุฒ CosineAnnealingLR (ฺฉู ุฏุฑ ูพุดููุงุฏ ูุจู ุจูุฏ) ุจุงุนุซ ุดุฏ ูุฏู ุจู ุฌุง ฺฉ "ุงูุฌ" ูุงฺฏูุงูุ ุจุฑุง ณ ุงูพุงฺฉ (ฑฒุ ฑณ ู ฑด) ุฏุฑ ูุฒุฏฺฉ ถฑูช ุจุงู ุจูุงูุฏ. ุงู ูุดุงููโ ููฺฏุฑุง ูพุงุฏุงุฑุชุฑ ุงุณุช.

    ูุฏู ุจููู: ุฒูุงู ูุฑ ุงูพุงฺฉ (~ด.ธ ุซุงูู) ู ุฒูุงู bwd (~ฒ.ฐ ุซุงูู) ฺฉูุชุฑ ุงุฒ ุขุฒูุงุด ูุจู (ฺฉู bwd ุขู ~ฒ.ต ุซุงูู ุจูุฏ) ุงุณุช. ุงู ุนู ูุฏู EfficientCNN ุจุง ุจููฺฉโูุง Depthwise ูุงูุนุงู ุณุจฺฉโุชุฑ ู ุจูููโุชุฑ ุงุณุช.

ูุดฺฉู ุงุตู (ฺฉู ุญู ูุดุฏู): ุงูฺฏู ุฏููุงู ุชฺฉุฑุงุฑ ุดุฏ. ูุฏู ุจู ฺฉ ููู ูโุฑุณุฏ ู ุณูพุณ ุฏฺุงุฑ ุงูุฑูุชูฺฏ ุดุฏุฏ ุดุฏู ู ุณููุท ูโฺฉูุฏ (ุงุฒ ถฑ.ตตูช ุจู ตน.ฐธูช).

ุงู ุนู loss ุขููุฒุด (1.0890) ููฺูุงู ุฏุฑ ุญุงู ฺฉุงูุด ุงุณุช (ูุฏู ุฏุฑ ุญุงู ุญูุธ ฺฉุฑุฏู ุงุณุช)ุ ุงูุง ุงู ุงุฏฺฏุฑ ุฏฺฏุฑ ุจู ุฏุงุฏูโูุง ุชุณุช ุชุนูู ูพุฏุง ููโฺฉูุฏ ู ุนููฺฉุฑุฏ ุฑุง ุจุฏุชุฑ ูโฺฉูุฏ.

๐ฏ ฺุฑุง ุงู ุงุชูุงู ูโุงูุชุฏุ (ุฑุดูโ ูุดฺฉู)

ูุง ุจู ุญุฏ ููุง ฺฉุงุฑ ฺฉู ูโุชูุงู ุจุง Data Augmentation ุงุณุชุงูุฏุงุฑุฏ (ฺุฑุฎุดุ ุจุฑุดุ ุชุบุฑ ุฑูฺฏ) ุฑู ตฐฐฐ ุนฺฉุณ ุงูุฌุงู ุฏุงุฏุ ุฑุณุฏูโุงู.

ูุฏู EfficientCNN ุดูุง ุขููุฏุฑ ุจููู ุงุณุช ฺฉู ุฏุฑ ุนุฑุถ ฑด ุงูพุงฺฉุ ุชูุงู ุงูฺฏููุง ููุฏ ฺฉู ุฏุฑ ุงู ตฐฐฐ ุนฺฉุณ (ู ูุณุฎูโูุง ุชุบุฑ ุงูุชูโ ุณุงุฏูโ ุขูโูุง) ูุฌูุฏ ุฏุงุฑุฏ ุฑุง ุงุฏ ูโฺฏุฑุฏ. ุงุฒ ุขู ุจู ุจุนุฏุ ุดุฑูุน ุจู ุงุฏฺฏุฑ "ููุฒ" ู ุฌุฒุฆุงุช ุจโุงููุช ูโฺฉูุฏ.

ุจุฑุง ุนุจูุฑ ุงุฒ ุงู ุณุฏ ถฐ-ถฑูชุ ุฏฺฏุฑ ููโุชูุงูู ูุงูพุฑูพุงุฑุงูุชุฑูุง (LR, weight_decay) ุฑุง ุชุบุฑ ุฏูู. ูุง ุจุงุฏ ูุงูุช ุฏุงุฏูโูุง ูุฑูุฏ ุฑุง ุจู ุทูุฑ ุงุณุงุณ ุชุบุฑ ุฏูู.

๐ ุฑุงูโุญูโูุง ุฌูุด ุจุฑุง ุฑุณุฏู ุจู ทฐูช

ุฏู ุฑุงู ุงุตู ุจุฑุง ุญู ุงู ูุดฺฉู ูุฌูุฏ ุฏุงุฑุฏ:

ฑ. Transfer Learning (ุงุฏฺฏุฑ ุงูุชูุงู) - ุณุงุฏูโุชุฑู ุฑุงู

ุจู ุฌุง ุขููุฒุด ฺฉ ูุฏู ุงุฒ ุตูุฑุ ุงุฒ ูุฏู ุงุณุชูุงุฏู ฺฉูุฏ ฺฉู ูุจูุงู ุฑู ุฏุชุงุณุช ุนุธู ImageNet (ฑ.ฒ ูููู ุนฺฉุณ) ุขููุฒุด ุฏุฏู ุงุณุช. ุงู ูุฏูโูุง (ูุงููุฏ ResNet18) ุงุฒ ูุจู ูโุฏุงููุฏ "ูุจู"ุ "ุจุงูุช"ุ "ฺุดู" ู "ฺุฑุฎ" ฺู ุดฺฉู ูุณุชูุฏ.

    ูุญูู ุงุฌุฑุง:

        ฺฉ ResNet18 ุงุฒ torchvision.models ุจุงุฑฺฏุฑ ฺฉูุฏ (pretrained=True).

        ูุงูโ ุขุฎุฑ (fc) ุขู ุฑุง ฺฉู ุจุฑุง ฑฐฐฐ ฺฉูุงุณ ImageNet ุงุณุชุ ุจุง ฺฉ ูุงูโ nn.Linear(512, 10) ุจุฑุง ฑฐ ฺฉูุงุณ STL-10 ุฌุงฺฏุฒู ฺฉูุฏ.

        ูุฏู ุฑุง ููุท ุจุฑุง ฺูุฏ ุงูพุงฺฉ (ุญุช ต ุงูพุงฺฉ) ุขููุฒุด ุฏูุฏ.

    ูุชุฌู: ุจู ุงุญุชูุงู นนูช ุฏุฑ ฺฉูุชุฑ ุงุฒ ฑฐ ุงูพุงฺฉ ุจู ุฑุงุญุช ุงุฒ ทตูช ุชุง ธตูช ุฏูุช ุนุจูุฑ ุฎูุงูุฏ ฺฉุฑุฏ.

ฒ. Augmentation ุจุณุงุฑ ูู (CutMix / MixUp) - ุฑุงู ุงุตููโุชุฑ

ุงฺฏุฑ ูโุฎูุงูุฏ ููู ูุนูุงุฑ EfficientCNN ุฎูุฏุชุงู ุฑุง ุจู ฺุงูุด ุจฺฉุดุฏ ู ุงุฒ ุตูุฑ ุจู ทฐูช ุจุฑุณุงูุฏุ ุจุงุฏ ุงุฒ ุชฺฉูฺฉโูุง ุฑฺฏููุงุฑุฒุดู ุจุณุงุฑ ููโุชุฑ ุงุณุชูุงุฏู ฺฉูุฏ ฺฉู ุฌูู ุญูุธ ฺฉุฑุฏู ุฑุง ุจฺฏุฑูุฏ.

    CutMix: ุฏุฑ ูุฑ ุจฺุ ฺฉ ุชฺฉู (patch) ุงุฒ ฺฉ ุนฺฉุณ ุฑุง ูโุจูุฑุฏ ู ุฑู ุนฺฉุณ ุฏฺฏุฑ ูโฺุณุจุงูุฏ. ุจุฑฺุณุจ (label) ุนฺฉุณ ูุฒ ุชุฑฺฉุจ ุงุฒ ุจุฑฺุณุจ ุฏู ุนฺฉุณ ูโุดูุฏ (ูุซูุงู ทฐูช "ุณฺฏ"ุ ณฐูช "ฺฏุฑุจู").

    MixUp: ุฏู ุนฺฉุณ ุฑุง ุจุง ูู ุชุฑฺฉุจ (blend) ูโฺฉูุฏ (ูุซูุงู ุนฺฉุณ A ุจุง ุดูุงูุช ทฐูช ู ุนฺฉุณ B ุจุง ุดูุงูุช ณฐูช).

ุงู ุชฺฉูฺฉโูุง ูุฏู ุฑุง ูุฌุจูุฑ ูโฺฉููุฏ ฺฉู ุจู ุฌุง ุญูุธ ฺฉุฑุฏู "ฺฉ ฺฏุฑุจูโ ฺฉุงูู"ุ ุงุฏ ุจฺฏุฑุฏ ฺฉู "ุชฺฉูโูุง ุงุฒ ฺฏุฑุจู" ุง "ุชุฑฺฉุจ ุงุฒ ฺฏุฑุจู ู ุณฺฏ" ุฑุง ุชุดุฎุต ุฏูุฏ. ุงู ฺฉุงุฑ ุจู ุดุฏุช ุฌูู ุงูุฑูุชูฺฏ ุฑุง ูโฺฏุฑุฏ.

ุฌูุนโุจูุฏ

    ุงฺฏุฑ ูุฏู ููุท ุฑุณุฏู ุจู ุฏูุช ทฐูช ุงุณุช: ุงุฒ Transfer Learning (ฺฏุฒูู ฑ) ุงุณุชูุงุฏู ฺฉูุฏ. ุงู ฺฉุงุฑ ุณุฑุนโุชุฑู ู ูุทูุฆูโุชุฑู ุฑุงู ุงุณุช.

    ุงฺฏุฑ ูุฏู ุจูุจูุฏ ููู ูุฏู EfficientCNN ุงุณุช: ุจุงุฏ ุงุฒ CutMix (ฺฏุฒูู ฒ) ุฏุฑ ุญูููโ ุขููุฒุด ุฎูุฏ ุงุณุชูุงุฏู ฺฉูุฏ.

ฺฉุฏุงู ุฑูุด ุฑุง ุชุฑุฌุญ ูโุฏูุฏุ ุขุง ูุงูุฏ ฺฉุฏ ูพุงุฏูโุณุงุฒ Transfer Learning ุจุง ResNet18 ุง ุงูุฒูุฏู CutMix ุจู ุญูููโ ุขููุฒุด ูุนูโุชุงู ุฑุง ูุดุงู ุฏููุ

ุชุง ุงูุงู ุงุฒ ุชุงุจุน ูุฏู ุงุณุชูุงุฏู ู ฺฉุฑุฏู ุงูุงู ุงููุฏู ุงู ุฌุฏุฏู ุฑู ุงุณุชูุงุฏู ฺฉูู ูฺฏู


import time

import torch

import torch.nn as nn

import torch.optim as optim

from torch.cuda.amp import autocast, GradScaler

def train_model_fast(model, train_loader, test_loader, device, 

                       epochs=10, max_lr=1e-3, use_amp=True):

    

    model = model.to(device)

    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)

    criterion = nn.CrossEntropyLoss()

    

    scheduler = optim.lr_scheduler.OneCycleLR(

        optimizer,

        max_lr=max_lr,

        epochs=epochs,

        steps_per_epoch=len(train_loader)

    )

    

    scaler = GradScaler(enabled=use_amp)


    for epoch in range(1, epochs + 1):

        model.train()

        running_loss = 0.0

        

        torch.cuda.synchronize()

        epoch_start = time.time()


        for images, labels in train_loader:

            images = images.to(device, non_blocking=True)

            labels = labels.to(device, non_blocking=True)


            with autocast(enabled=use_amp):

                outputs = model(images)

                loss = criterion(outputs, labels)


            optimizer.zero_grad(set_to_none=True)

            

            scaler.scale(loss).backward()

            scaler.step(optimizer)

            scaler.update()

            

            scheduler.step()


            running_loss += loss.item() * labels.size(0)


        torch.cuda.synchronize()

        epoch_end = time.time()

        epoch_time = epoch_end - epoch_start

        

        avg_loss = running_loss / len(train_loader.dataset)

        

        acc = evaluate(model, test_loader, device)

        

        current_lr = optimizer.param_groups[0]['lr']

        

        print(

            f"Epoch {epoch}/{epochs}: "

            f"loss={avg_loss:.4f}, "

            f"test_acc={acc*100:.2f}%, "

            f"LR={current_lr:.6f}, "

            f"time={epoch_time:.2f}s"

        )

        

    print("Training finished.")

    return model


/tmp/ipykernel_48/2791035494.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.

scaler = GradScaler(enabled=use_amp)/tmp/ipykernel_48/2791035494.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.

with autocast(enabled=use_amp):

ุงู ฺฉ FutureWarning ุงุณุชุ ูู ฺฉ ุงุฑูุฑ. ฺฉุฏ ุดูุง ููฺูุงู ฺฉุงุฑ ูโฺฉูุฏุ ุงูุง PyTorch ุจู ุดูุง ูุดุฏุงุฑ ูโุฏูุฏ ฺฉู ุฏุฑ ุญุงู ุงุณุชูุงุฏู ุงุฒ ฺฉ API ูุฏู ูุณุชุฏ ฺฉู ุฏุฑ ูุณุฎูโูุง ุขูุฏู ุญุฐู ุฎูุงูุฏ ุดุฏ.

ุฏูู ุงู ูุดุฏุงุฑ ุงู ุงุณุช ฺฉู PyTorch ุฏุฑ ุญุงู ฺฉูพุงุฑฺูโุณุงุฒ AMP (Automatic Mixed Precision) ุงุณุช ุชุง ููุท ูุฎุตูุต cuda ูุจุงุดุฏ (ูุซูุงู ุจุฑุง cpu ูู ฺฉุงุฑ ฺฉูุฏ). API ุฌุฏุฏ ุงุฒ torch.amp ุจู ุฌุง torch.cuda.amp ุงุณุชูุงุฏู ูโฺฉูุฏ.

ุจุฑุง ุฑูุน ุงู ูุดุฏุงุฑุ ุจุงุฏ ุงููพูุฑุช ุฎูุฏ ุฑุง ุชุบุฑ ุฏูุฏ ู device_type ุฑุง ุจู autocast ู GradScaler ุงุถุงูู ฺฉูุฏ.

ฺฉุฏ ุงุตูุงุญ ุดุฏู ุจู ุงู ุตูุฑุช ุงุณุช (ููุฑุงู ุจุง ุชุงุจุน evaluate ฺฉู ุจุฑุง ฺฉุงูู ุจูุฏู ฺฉุฏ ูุงุฒู ุงุณุช):
Python

import time
import torch
import torch.nn as nn
import torch.optim as optim
# Correct imports: Use torch.amp
from torch.amp import autocast, GradScaler

def evaluate(model, test_loader, device):
    """Helper function to evaluate the model (needed by the train loop)."""
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            # Use autocast here as well
            with autocast(device_type=device, enabled=(device == 'cuda')):
                outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

def train_model_fast(model, train_loader, test_loader, device, 
                       epochs=10, max_lr=1e-3, use_amp=True):
    
    model = model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)
    criterion = nn.CrossEntropyLoss()
    
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=max_lr,
        epochs=epochs,
        steps_per_epoch=len(train_loader)
    )
    
    # Updated: Specify device_type and check if device is 'cuda'
    scaler = GradScaler(device_type=device, enabled=(use_amp and device == 'cuda'))

    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        
        torch.cuda.synchronize()
        epoch_start = time.time()

        for images, labels in train_loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # Updated: Specify device_type and check if device is 'cuda'
            with autocast(device_type=device, enabled=(use_amp and device == 'cuda')):
                outputs = model(images)
                loss = criterion(outputs, labels)

            optimizer.zero_grad(set_to_none=True)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            scheduler.step()

            running_loss += loss.item() * labels.size(0)

        torch.cuda.synchronize()
        epoch_end = time.time()
        epoch_time = epoch_end - epoch_time
        
        avg_loss = running_loss / len(train_loader.dataset)
        
        acc = evaluate(model, test_loader, device)
        
        current_lr = optimizer.param_groups[0]['lr']
        
        print(
            f"Epoch {epoch}/{epochs}: "
            f"loss={avg_loss:.4f}, "
            f"test_acc={acc*100:.2f}%, "
            f"LR={current_lr:.6f}, "
            f"time={epoch_time:.2f}s"
        )
        
    print("Training finished.")
    return model

ูู ุฎูุงุฏ

def train_model_fast(model, train_loader, test_loader, device, 

                       epochs=10, max_lr=1e-3, use_amp=True):

    

    model = model.to(device)

    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)

    criterion = nn.CrossEntropyLoss()

    

    scheduler = optim.lr_scheduler.OneCycleLR(

        optimizer,

        max_lr=max_lr,

        epochs=epochs,

        steps_per_epoch=len(train_loader)

    )

    

    scaler = GradScaler(enabled=use_amp)


    for epoch in range(1, epochs + 1):

        model.train()

        running_loss = 0.0

        

        torch.cuda.synchronize()

        epoch_start = time.time()


        for images, labels in train_loader:

            images = images.to(device, non_blocking=True)

            labels = labels.to(device, non_blocking=True)


            with autocast(enabled=use_amp):

                outputs = model(images)

                loss = criterion(outputs, labels)


            optimizer.zero_grad(set_to_none=True)

            

            scaler.scale(loss).backward()

            scaler.step(optimizer)

            scaler.update()

            

            scheduler.step()


            running_loss += loss.item() * labels.size(0)


        torch.cuda.synchronize()

        epoch_end = time.time()

        epoch_time = epoch_end - epoch_start

        

        avg_loss = running_loss / len(train_loader.dataset)

        

        acc = evaluate(model, test_loader, device)

        

        current_lr = optimizer.param_groups[0]['lr']

        

        print(

            f"Epoch {epoch}/{epochs}: "

            f"loss={avg_loss:.4f}, "

            f"test_acc={acc*100:.2f}%, "

            f"LR={current_lr:.6f}, "

            f"time={epoch_time:.2f}s"

        )

        

    print("Training finished.")

    return model


    ฺฉูุง GradScaler ุฑู ุจ ุฎุงู ุดู ู ุจุฏูู ุงูู ุชุงุจุน ุชุฑู ุฑู ุจุฏู

ูุชูุฌู ุดุฏู. ุญุฐู GradScaler ู autocast ุจู ุงู ูุนู ุงุณุช ฺฉู ูุง ุงุฒ Automatic Mixed Precision (AMP) ุงุณุชูุงุฏู ูุฎูุงูู ฺฉุฑุฏ ู ุขููุฒุด ุจุง ุฏูุช ฺฉุงูู (FP32) ุงูุฌุงู ูโุดูุฏ.

ุงู ฺฉุงุฑ ููฺฉู ุงุณุช ุขููุฒุด ุฑุง ฺฉู ฺฉูุฏุชุฑ ฺฉูุฏ ุง ุญุงูุธูโ GPU ุจุดุชุฑ ูุตุฑู ฺฉูุฏุ ุงูุง ุณุงุฏฺฏ ฺฉุฏ ุฑุง ุงูุฒุงุด ูโุฏูุฏ.

ุฏุฑ ุงูุฌุง ุชุงุจุน ุจุงุฒููุณ ุดุฏู ุจุฏูู GradScaler ู autocast ุขูุฏู ุงุณุช (ููุฑุงู ุจุง ุชุงุจุน evaluate ฺฉู ุจุฑุง ุงุฌุฑุง ุขู ูุงุฒู ุงุณุช):
Python

import time
import torch
import torch.nn as nn
import torch.optim as optim

def evaluate(model, test_loader, device):
    """Helper function to evaluate the model (FP32)."""
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            # No autocast
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return correct / total

def train_model_fast(model, train_loader, test_loader, device, 
                       epochs=10, max_lr=1e-3):
    
    model = model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)
    criterion = nn.CrossEntropyLoss()
    
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=max_lr,
        epochs=epochs,
        steps_per_epoch=len(train_loader)
    )
    
    # No GradScaler needed

    print("Starting training (FP32)...")
    for epoch in range(1, epochs + 1):
        model.train()
        running_loss = 0.0
        
        torch.cuda.synchronize()
        epoch_start = time.time()

        for images, labels in train_loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            # Standard forward pass (no autocast)
            outputs = model(images)
            loss = criterion(outputs, labels)

            optimizer.zero_grad(set_to_none=True)
            
            # Standard backward and step (no scaler)
            loss.backward()
            optimizer.step()
            
            scheduler.step()

            running_loss += loss.item() * labels.size(0)

        torch.cuda.synchronize()
        epoch_end = time.time()
        epoch_time = epoch_end - epoch_start
        
        avg_loss = running_loss / len(train_loader.dataset)
        
        acc = evaluate(model, test_loader, device)
        
        current_lr = optimizer.param_groups[0]['lr']
        
        print(
            f"Epoch {epoch}/{epochs}: "
            f"loss={avg_loss:.4f}, "
            f"test_acc={acc*100:.2f}%, "
            f"LR={current_lr:.6f}, "
            f"time={epoch_time:.2f}s"
        )
        
    print("Training finished.")
    return model


PM

ุงูู ฺฉุฏุด ุฑู ุจููุณ

ุงู ฺฉุฏู ููุฌูุฏ ุฏุฑ ุชุตูุฑ ุงุณุช. ูู ฺฉุชุงุจุฎุงููโูุง ุถุฑูุฑ ุฑุง ูู ุฏุฑ ุงุจุชุฏุง ุงุถุงูู ฺฉุฑุฏู ุชุง ฺฉุฏ ฺฉุงูู ู ูุงุจู ุงุณุชูุงุฏู ุจุงุดุฏ.

ุทุจู ุฏุณุชูุฑุงูุนูู ุดูุงุ ุชูุงู ฺฉุงููุชโูุง ู ูุชูู ุฏุงุฎู ฺฉุฏ ุจู ุฒุจุงู ุงูฺฏูุณ ูุณุชูุฏ.
Python

import torch
import torchvision
import torchvision.transforms as transforms

def load_data(rank, world_size):
    # Note: 'data_path', 'train_batch_size', and 'test_batch_size'
    # must be defined in the global scope or passed as arguments.
    
    train_set = torchvision.datasets.CIFAR10(data_path, download=True, transform=transforms.Compose([transforms.ToTensor()]))
    train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,
                                                                    num_replicas=world_size,
                                                                    rank=rank)

    train_loader = torch.utils.data.DataLoader(dataset=train_set,
                                               sampler=train_sampler,
                                               batch_size=train_batch_size,
                                               shuffle=False,
                                               persistent_workers=True,
                                               num_workers=1,
                                               pin_memory=True)

    test_set = torchvision.datasets.CIFAR10(data_path, download=True, train=False, transform=transforms.Compose([transforms.ToTensor()]))
    test_sampler = torch.utils.data.distributed.DistributedSampler(test_set,
                                                                   num_replicas=world_size,
                                                                   rank=rank)

    test_loader = torch.utils.data.DataLoader(dataset=test_set,
                                              sampler=test_sampler,
                                              batch_size=test_batch_size,
                                              shuffle=False,
                                              persistent_workers=True,
                                              num_workers=1,
                                              pin_memory=True)

    return train_loader, test_loader

ูฺฉุงุช:

    ูุชุบุฑูุง data_pathุ train_batch_size ู test_batch_size ุฏุฑ ุชุตูุฑ ุชุนุฑู ูุดุฏูโุงูุฏุ ุจูุงุจุฑุงู ุจุงุฏ ูุทูุฆู ุดูุฏ ฺฉู ูุจู ุงุฒ ูุฑุงุฎูุงู ุชุงุจุน ุง ุฏุฑ ุงุณฺฉููพ (scope) ุฌูุงู ุจุฑูุงูู ุชุนุฑู ุดุฏู ุจุงุดูุฏ.

    ุขุฑฺฏููุงู persistent_workers=True ุจุงุนุซ ูโุดูุฏ ฺฉู workers (ฺฉุงุฑฺฏุฑุงู ุฏุงุฏู) ุจุนุฏ ุงุฒ ุงุชูุงู ฺฉ epoch ุฒูุฏู ุจูุงููุฏ ฺฉู ุณุฑุนุช ุฑุง ุงูุฒุงุด ูโุฏูุฏ ุงูุง ุญุงูุธู ุฑู ุจุดุชุฑ ูุตุฑู ูโฺฉูุฏ.

ุขุง ูโุฎูุงูุฏ ุชุบุฑ ุฎุงุต (ูุซู ุชุบุฑ ุจฺโุณุงุฒ ุง ูุณุฑ ุฏุชุง) ุฑู ุงู ฺฉุฏ ุงุนูุงู ฺฉููุ

ฺฉุฏุด ุฑู ุจููุณ

ุงู ูู ฺฉุฏ ูุฑุจูุท ุจู ุจุฎุด main ฺฉู ุฏุฑ ุชุตูุฑ ุงุฑุณุงู ฺฉุฑุฏุฏ. ูู ุงููพูุฑุชโูุง ูุงุฒู (import) ุฑุง ูู ุงุถุงูู ฺฉุฑุฏู ุชุง ุณุงุฎุชุงุฑ ฺฉุฏ ุตุญุญ ุจุงุดุฏ.

ุทุจู ุฎูุงุณุชู ุดูุงุ ุฏุงุฎู ุจููฺฉ ฺฉุฏ ูฺ ูุชู ุง ฺฉุงููุช ูุงุฑุณ ูุฌูุฏ ูุฏุงุฑุฏ.
Python

import torch
import torch.multiprocessing as mp
import time
import datetime

# Note: 'train' function and 'find_free_port' function must be defined elsewhere.

if __name__ == "__main__":
    world_size = torch.cuda.device_count()
    master_port = find_free_port()
    backend = 'nccl'
    timeout = datetime.timedelta(seconds=10)
    start_time = time.time()
    mp.spawn(train, nprocs=world_size, args=(world_size, master_port, backend, timeout), join=True)
    end_time = time.time()
    print("Total time: {}".format(end_time - start_time))

ูฺฉุงุช:

    ุงู ฺฉุฏ ูุฑุถ ุฑุง ุจุฑ ุงู ูโฺฏุฐุงุฑุฏ ฺฉู ุชุงุจุน ุจู ูุงู find_free_port() ุจุฑุง ูพุฏุง ฺฉุฑุฏู ูพูุฑุช ุขุฒุงุฏ ุดุจฺฉู ู ุชุงุจุน ุจู ูุงู train (ฺฉู ููุทู ุงุตู ุขููุฒุด ูุฏู ุดูุงุณุช) ูุจูุงู ุฏุฑ ูุงู ุชุนุฑู ุดุฏูโุงูุฏ.

    ูุชุบุฑ backend ุฑู 'nccl' ุชูุธู ุดุฏู ุงุณุช ฺฉู ุงุณุชุงูุฏุงุฑุฏ ุชุฑู ุจฺฉโุงูุฏ ุจุฑุง ุขููุฒุด ุชูุฒุน ุดุฏู ุฑู GPUูุง ุงููุฏุง ุงุณุช.

ุขุง ูุงุฒ ุฏุงุฑุฏ ฺฉู ุชุงุจุน find_free_port ุฑุง ูู ุจุฑุงุชุงู ูพุงุฏูโุณุงุฒ ฺฉููุ

def load_data(rank, world_size):

    train_set = datasets.STL10('../data', download=True, transform=train_transform))

    train_sampler = torch.utils.data.distributed.DistributedSampler(train_set,

                                                                    num_replicas=world_size,

                                                                    rank=rank)


    train_loader = torch.utils.data.DataLoader(dataset=train_set,

                                               sampler=train_sampler,

                                               batch_size=batch_size,

                                               shuffle=False,

                                               persistent_workers=True,

                                               num_workers=1,

                                               pin_memory=True)


    test_set = datasets.STL10('../data', train=False, transform=test_transform))

    test_sampler = torch.utils.data.distributed.DistributedSampler(test_set,

                                                                   num_replicas=world_size,

                                                                   rank=rank)


    test_loader = torch.utils.data.DataLoader(dataset=test_set,

                                              sampler=test_sampler,

                                              batch_size=batch_size,

                                              shuffle=False,

                                              persistent_workers=True,

                                              num_workers=1,

                                              pin_memory=True)


    return train_loader, test_loader


File "<tokenize>", line 8 sampler=train_sampler,

^IndentationError: unindent does not match any outer indentation level

ุงู ุฎุทุง (IndentationError) ูุดุงูโุฏููุฏู ุนุฏู ููุงููฺฏ ูุงุตููโูุง (Spaces) ุฏุฑ ุงุจุชุฏุง ุฎุทูุท ุงุณุช. ููฺูู ุฏู ุงุฑุงุฏ ุฏฺฏุฑ ุฏุฑ ฺฉุฏ ูุฌูุฏ ุฏุงุดุช ฺฉู ุขูโูุง ุฑุง ูุฒ ุงุตูุงุญ ฺฉุฑุฏู:

    ูพุฑุงูุชุฒ ุงุถุงูู: ุฏุฑ ุงูุชูุง ุฎุทูุท ุชุนุฑู train_set ู test_set ฺฉ ูพุฑุงูุชุฒ ุงุถุงู )) ูุฌูุฏ ุฏุงุดุช.

    ุขุฑฺฏููุงู STL10: ุจุฑุฎูุงู CIFAR10ุ ุฏุชุงุณุช STL10 ุฏุฑ torchvision ุขุฑฺฏููุงู train=... ูุฏุงุฑุฏ ู ุจุงุฏ ุงุฒ split='train' ุง split='test' ุงุณุชูุงุฏู ฺฉูุฏ.

ฺฉุฏ ุงุตูุงุญ ุดุฏู ู ูุฑุชุจ ุดุฏู:
Python

import torch
from torchvision import datasets

def load_data(rank, world_size):
    # Note: train_transform, test_transform, and batch_size must be defined externally
    
    train_set = datasets.STL10('../data', split='train', download=True, transform=train_transform)
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_set,
        num_replicas=world_size,
        rank=rank
    )

    train_loader = torch.utils.data.DataLoader(
        dataset=train_set,
        sampler=train_sampler,
        batch_size=batch_size,
        shuffle=False,
        persistent_workers=True,
        num_workers=1,
        pin_memory=True
    )

    test_set = datasets.STL10('../data', split='test', download=True, transform=test_transform)
    test_sampler = torch.utils.data.distributed.DistributedSampler(
        test_set,
        num_replicas=world_size,
        rank=rank
    )

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set,
        sampler=test_sampler,
        batch_size=batch_size,
        shuffle=False,
        persistent_workers=True,
        num_workers=1,
        pin_memory=True
    )

    return train_loader, test_loader

ุขุง ูโุฎูุงูุฏ ฺฉุฏูุง ูุฑุจูุท ุจู train_transform ู test_transform ููุงุณุจ ุจุฑุง STL10 ุฑุง ูู ุงุถุงูู ฺฉููุ

PM

Traceback (most recent call last):

File "<string>", line 1, in <module>

File "/usr/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main

Traceback (most recent call last):

File "<string>", line 1, in <module>

File "/usr/lib/python3.11/multiprocessing/spawn.py", line 122, in spawn_main

exitcode = _main(fd, parent_sentinel)

^^^^^^^^^^^^^^^^^^^^^^^^^^

File "/usr/lib/python3.11/multiprocessing/spawn.py", line 132, in _main

exitcode = _main(fd, parent_sentinel)

^^^^^^^^^^^^^^^^^^^^^^^^^^

File "/usr/lib/python3.11/multiprocessing/spawn.py", line 132, in _main

self = reduction.pickle.load(from_parent)

self = reduction.pickle.load(from_parent)

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

AttributeError: Can't get attribute 'train_dist' on <module '__main__' (built-in)>

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

AttributeError: Can't get attribute 'train_dist' on <module '__main__' (built-in)>

W1117 13:12:11.432000 48 torch/multiprocessing/spawn.py:169] Terminating process 93 via signal SIGTERM

---------------------------------------------------------------------------ProcessExitedException Traceback (most recent call last)/tmp/ipykernel_48/1884841067.py in <cell line: 0>() 4 timeout = datetime.timedelta(seconds=10) 5 start_time = time.time()----> 6 mp.spawn(train_dist, nprocs=world_size, args=(world_size, master_port, backend, timeout), join=True) 7 end_time = time.time() 8 print("Total time: {}".format(end_time - start_time))/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py in spawn(fn, args, nprocs, join, daemon, start_method) 338 ) 339 warnings.warn(msg, FutureWarning, stacklevel=2)--> 340 return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method) 294 295 # Loop on join until it returns True or raises an exception.--> 296 while not context.join(): 297 pass 298 /usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py in join(self, timeout, grace_period) 202 ) 203 else:--> 204 raise ProcessExitedException( 205 "process %d terminated with exit code %d" % (error_index, exitcode), 206 error_index=error_index,ProcessExitedException: process 1 terminated with exit code 1



ฺุฑุงุ

ุงู ุฎุทุง (AttributeError: Can't get attribute 'train_dist') ุจู ุฏูู ูุญุฏูุฏุชโูุง ูุงฺูู multiprocessing ุฏุฑ ูุญุทโูุง ุชุนุงูู ูุซู Jupyter Notebook ุฑุฎ ูโุฏูุฏ.

ุฏูู ุงุตู: ฺฉุชุงุจุฎุงูู multiprocessing (ูุฎุตูุตุงู ูุชุฏ spawn ฺฉู ุจุฑุง CUDA ุถุฑูุฑ ุงุณุช) ูุงุฒ ุฏุงุฑุฏ ฺฉู ุชูุงุจุน ู ฺฉูุงุณโูุง ุฑุง Pickle (ุณุฑุงูโุณุงุฒ) ฺฉูุฏ ู ุจู ูพุฑุฏุงุฒุดโูุง ุฌุฏุฏ (Child Processes) ุจูุฑุณุชุฏ. ููุช ุชุงุจุน ูุซู train_dist ุฑุง ุฏุงุฎู ุณูููโูุง ููุชโุจูฺฉ ุชุนุฑู ูโฺฉูุฏุ ุงู ุชุงุจุน ุจุฎุด ุงุฒ ูุงฺูู ุฏูุงูฺฉ __main__ ูโุดูุฏ ฺฉู ูพุฑุฏุงุฒุดโูุง ูุฑุฒูุฏ ุจู ุขู ุฏุณุชุฑุณ ูุฏุงุฑูุฏ. ุขูโูุง ุฏูุจุงู ฺฉ ูุงู ูุฒฺฉ ูโฺฏุฑุฏูุฏ ุชุง ุชุงุจุน ุฑุง ุงุฒ ุขู import ฺฉููุฏุ ุงูุง ุขู ุฑุง ูพุฏุง ููโฺฉููุฏ.

ุฑุงู ุญู: ุจุงุฏ ฺฉุฏูุง (ุดุงูู ูุฏูุ ุชูุงุจุน ุขููุฒุด ู ุชุงุจุน train_dist) ุฑุง ุฏุฑ ฺฉ ูุงู ูพุงุชูู ุฌุฏุงฺฏุงูู (ูุซูุงู .py) ุฐุฎุฑู ฺฉูุฏ ู ุณูพุณ ุฏุฑ ููุชโุจูฺฉ ุขู ุฑุง ุงุฌุฑุง ฺฉูุฏ.

ูู ฺฉุฏ ุงุตูุงุญ ุดุฏู ุฑุง ุฏุฑ ุฏู ุจุฎุด ุจุฑุง ุดูุง ุขูุงุฏู ฺฉุฑุฏูโุงู.

ฑ. ุฐุฎุฑู ุชูุงุจุน ู ฺฉูุงุณโูุง ุฏุฑ ูุงู (training_utils.py)

ุงุจุชุฏุง ุงู ุณููู ุฑุง ุงุฌุฑุง ฺฉูุฏ ุชุง ฺฉ ูุงู training_utils.py ุณุงุฎุชู ุดูุฏ ฺฉู ุดุงูู ุชูุงู ฺฉูุงุณโูุง ู ุชูุงุจุน ููุฑุฏ ูุงุฒ ุงุณุช.

ุชูุฌู: ูู ุขุฑฺฏููุงูโูุง train_dist ุฑุง ูู ุงุตูุงุญ ฺฉุฑุฏูุ ุฒุฑุง mp.spawn ุจู ุตูุฑุช ุฎูุฏฺฉุงุฑ rank ุฑุง ุจู ุนููุงู ุงููู ุขุฑฺฏููุงู ูพุงุณ ูโุฏูุฏ.
Python

%%writefile training_utils.py
import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.distributed as dist
from torchvision import datasets, transforms
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size, master_port, backend, timeout):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = master_port
    dist.init_process_group(backend=backend, rank=rank, world_size=world_size, timeout=timeout)

def cleanup():
    dist.destroy_process_group()

class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.depthwise = nn.Conv2d(
            in_channels, 
            in_channels, 
            kernel_size=3, 
            padding=1, 
            stride=stride, 
            groups=in_channels, 
            bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        
        self.pointwise = nn.Conv2d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            stride=1, 
            bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x) 
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) 
        
        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) 
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) 
        
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) 
        self.dropout = nn.Dropout(p=0.2) 
        self.fc = nn.Linear(256, num_class) 

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        return x

def load_data(rank, world_size, batch_size):
    train_transform = transforms.Compose([
        transforms.RandomCrop(96, padding=12),      
        transforms.RandomHorizontalFlip(p=0.5),   
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), 
        transforms.RandomRotation(10),            
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    train_set = datasets.STL10('../data', split='train', download=True, transform=train_transform)
    train_sampler = torch.utils.data.distributed.DistributedSampler(
        train_set,
        num_replicas=world_size,
        rank=rank
    )

    train_loader = torch.utils.data.DataLoader(
        dataset=train_set,
        sampler=train_sampler,
        batch_size=batch_size,
        shuffle=False,
        persistent_workers=True,
        num_workers=2,
        pin_memory=True
    )

    test_set = datasets.STL10('../data', split='test', download=True, transform=test_transform)
    test_sampler = torch.utils.data.distributed.DistributedSampler(
        test_set,
        num_replicas=world_size,
        rank=rank
    )

    test_loader = torch.utils.data.DataLoader(
        dataset=test_set,
        sampler=test_sampler,
        batch_size=batch_size,
        shuffle=False,
        persistent_workers=True,
        num_workers=2,
        pin_memory=True
    )

    return train_loader, test_loader

def evaluate(model, loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = model(images)
            preds = outputs.argmax(dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    return correct / total

def train_model_fast(model, train_loader, test_loader, device, epochs=10, max_lr=1e-3):
    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)
    criterion = nn.CrossEntropyLoss()
    
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=max_lr,
        epochs=epochs,
        steps_per_epoch=len(train_loader)
    )
    
    for epoch in range(1, epochs + 1):
        train_loader.sampler.set_epoch(epoch)
        model.train()
        running_loss = 0.0
        
        for images, labels in train_loader:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            outputs = model(images)
            loss = criterion(outputs, labels)

            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            optimizer.step()
            scheduler.step()
            
            running_loss += loss.item() * labels.size(0)

        if device == 0: 
            avg_loss = running_loss / len(train_loader.dataset)
            acc = evaluate(model, test_loader, device)
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Rank={device} Epoch {epoch}/{epochs}: loss={avg_loss:.4f}, test_acc={acc*100:.2f}%, LR={current_lr:.6f}")
    
    return model

def train_dist(rank, world_size, master_port, backend, timeout, batch_size):
    setup(rank, world_size, master_port, backend, timeout)
    torch.cuda.set_device(rank)
    
    train_loader, test_loader = load_data(rank, world_size, batch_size)
    
    model = CNN(in_channel=3, num_class=10).to(rank)
    ddp_model = DDP(model, device_ids=[rank])
    
    train_model_fast(ddp_model, train_loader, test_loader, rank, epochs=15)
    
    cleanup()

ฒ. ุงุฌุฑุง ฺฉุฏ ุฏุฑ ููุชโุจูฺฉ

ุญุงูุง ฺฉู ฺฉุฏูุง ุฏุฑ training_utils.py ุฐุฎุฑู ุดุฏูโุงูุฏุ ูโุชูุงูุฏ ุจุง mp.spawn ุชุงุจุน ฺฉู ุงุฒ ุขู ูุงู ุงููพูุฑุช ุดุฏู ุงุณุช ุฑุง ูุฑุงุฎูุงู ฺฉูุฏ.
Python

import torch
import torch.multiprocessing as mp
import datetime
import time
import training_utils 

if __name__ == '__main__':
    world_size = torch.cuda.device_count()
    master_port = '8282'
    backend = 'nccl'
    timeout = datetime.timedelta(seconds=60)
    batch_size = 32

    print(f"Running on {world_size} GPUs...")

    start_time = time.time()
    
    mp.spawn(
        training_utils.train_dist, 
        args=(world_size, master_port, backend, timeout, batch_size), 
        nprocs=world_size, 
        join=True
    )
    
    end_time = time.time()
    print("Total time: {}".format(end_time - start_time))


 PM

ฺุทูุฑ ุณุฑูุฑ ููุช ุจูฺฉ ุฌููพุชุฑ ุฑุงู ฺฉููุ

ุจุฑุง ุงุฌุฑุง ุณุฑูุฑ ููุช ุจูฺฉ Jupyter Notebookุ ุณุงุฏูโุชุฑู ู ุฑุงุฌโุชุฑู ุฑูุด ุงุณุชูุงุฏู ุงุฒ ุฎุท ูุฑูุงู (Command Line) ุง ุชุฑููุงู ุงุณุช.

ุฏุฑ ุงูุฌุง ูุฑุงุญู ฺฉู ฺฉุงุฑ ุขูุฏู ุงุณุช:

ฑ. ูุตุจ (ุงฺฏุฑ ูุจูุงู ูุตุจ ูฺฉุฑุฏูโุงุฏ)

ุงฺฏุฑ ูุจูุงู Jupyter ุฑุง ูุตุจ ูฺฉุฑุฏูโุงุฏุ ูุทูุฆู ุดูุฏ ฺฉู Python ู pip ุฑู ุณุณุชู ุดูุง ูุตุจ ุดุฏู ุจุงุดูุฏ. ุณูพุณ ุจุง ุงุณุชูุงุฏู ุงุฒ pip ุขู ุฑุง ูุตุจ ฺฉูุฏ:

ุชูุฌู: ุงฺฏุฑ ุงุฒ Anaconda ุงุณุชูุงุฏู ูโฺฉูุฏุ Jupyter ุจู ุทูุฑ ูพุดโูุฑุถ ูุตุจ ุดุฏู ุงุณุช ู ูุงุฒ ุจู ุงู ูุฑุญูู ูุณุช.
Bash

pip install notebook

ุง ุงฺฏุฑ ูโุฎูุงูุฏ ูุญุท ุฌุฏุฏุชุฑ JupyterLab ุฑุง ูุตุจ ฺฉูุฏ:
Bash

pip install jupyterlab

ฒ. ุงุฌุฑุง ุณุฑูุฑ

ฑ. ุจุงุฒ ฺฉุฑุฏู ุชุฑููุงู/ุฎุท ูุฑูุงู: * ุฏุฑ ููุฏูุฒุ Command Prompt ุง Anaconda Prompt ุฑุง ุจุงุฒ ฺฉูุฏ. * ุฏุฑ ูููฺฉุณ/ูฺฉุ Terminal ุฑุง ุจุงุฒ ฺฉูุฏ.

ฒ. ุฑูุชู ุจู ูพูุดู ุฏูุฎูุงู (ุงุฎุชุงุฑ ุงูุง ุชูุตู ูโุดูุฏ): ุจุง ุงุณุชูุงุฏู ุงุฒ ุฏุณุชูุฑ cd ุจู ูพูุดูโุง ุจุฑูุฏ ฺฉู ูโุฎูุงูุฏ ูุงูโูุง ููุช ุจูฺฉ ุดูุง ุฏุฑ ุขู ูุฑุงุฑ ฺฏุฑูุฏ ุง ุจู ุขูโูุง ุฏุณุชุฑุณ ุฏุงุดุชู ุจุงุดุฏ. ุณุฑูุฑ ุฏุฑ ุงู ูพูุดู ุดุฑูุน ุจู ฺฉุงุฑ ูโฺฉูุฏ ู ุจู ุขู root directory ฺฏูุชู ูโุดูุฏ.

ูุซุงู:
```bash
cd C:\Users\YourName\Documents\Jupyter_Projects
```

ณ. ุงุฌุฑุง ุฏุณุชูุฑ: ุจุฑุง ุฑุงูโุงูุฏุงุฒ ุณุฑูุฑุ ุฏุณุชูุฑ ุฒุฑ ุฑุง ูุงุฑุฏ ู Enter ฺฉูุฏ:

```bash
jupyter notebook
```
ุง ุงฺฏุฑ JupyterLab ูุตุจ ฺฉุฑุฏูโุงุฏ:
```bash
jupyter lab
```

ณ. ูุชุฌู

    ุจุง ุงุฌุฑุง ุงู ุฏุณุชูุฑุ ฺฉ ุณุฑูุฑ ูุญู (Local Server) ูุนูููุงู ุฏุฑ ุขุฏุฑุณ http://localhost:8888 ุง ูพูุฑุช ุฏฺฏุฑ ฺฉู ุฏุฑ ุฏุณุชุฑุณ ุจุงุดุฏุ ุฑุงูโุงูุฏุงุฒ ูโุดูุฏ.

    ููฺููุ ฺฉ ูพูุฌุฑู ุฌุฏุฏ ุฏุฑ ูุฑูุฑฺฏุฑ ูุจ ูพุดโูุฑุถ ุดูุง ุจุงุฒ ุดุฏู ู ุฑุงุจุท ฺฉุงุฑุจุฑ Jupyter Notebook (ุง JupyterLab Dashboard) ุฑุง ูุดุงู ูโุฏูุฏ.

    ุฏุฑ ูพูุฌุฑู ุชุฑููุงูุ ูุงฺฏโูุง ุณุฑูุฑ ู ุขุฏุฑุณ ุฏูู ุขู (ุจู ููุฑุงู ฺฉ ุชูฺฉู ุงููุช) ููุงุด ุฏุงุฏู ูโุดูุฏ. ุงฺฏุฑ ูุฑูุฑฺฏุฑ ุจู ุทูุฑ ุฎูุฏฺฉุงุฑ ุจุงุฒ ูุดุฏุ ูโุชูุงูุฏ ุขุฏุฑุณ ูุดุงู ุฏุงุฏู ุดุฏู ุฏุฑ ุชุฑููุงู ุฑุง ฺฉูพ ู ุฏุฑ ูุฑูุฑฺฏุฑ ุฎูุฏ ุจุงุฒ ฺฉูุฏ.

ด. ุฎุงููุด ฺฉุฑุฏู ุณุฑูุฑ

ุจุฑุง ูุชููู ฺฉุฑุฏู ุณุฑูุฑุ ฺฉุงู ุงุณุช ุจู ูพูุฌุฑู ุชุฑููุงู ุจุฑฺฏุฑุฏุฏ ู ุฏู ุจุงุฑ ฺฉูุฏูุง Ctrl + C ุฑุง ูุดุงุฑ ุฏูุฏ.

ูู ู ุณุฑูุฑ ุฏุงุฑู ู ุฌุง ุฏฺฏู ู ุฎูุงู ุณุฑูุฑ ุฑู ุงูู ุจุงุดู ุจูุด ูุตู ุดู

ุงฺฏุฑ ูโุฎูุงูุฏ ุณุฑูุฑ ููุช ุจูฺฉ Jupyter ุฑู ฺฉ ูุงุดู ุง ุณุฑูุฑ ุฏฺฏุฑ (Remote Server) ุงุฌุฑุง ุดูุฏ ู ุดูุง ุงุฒ ุณุณุชู ุฎูุฏุชุงู ุจู ุขู ูุชุตู ุดูุฏุ ุจุงุฏ ุชูุธูุงุช ฺฉู ูุชูุงูุช ุฑุง ุงูุฌุงู ุฏูุฏ ุชุง ุฏุณุชุฑุณ ุงุฒ ุฑุงู ุฏูุฑ ุงูฺฉุงูโูพุฐุฑ ุดูุฏ.

ุฏุฑ ุงูุฌุง ูุฑุงุญู ุงุตู ุจุฑุง ุงูุฌุงู ุงู ฺฉุงุฑ ุขูุฏู ุงุณุช:

ฑ. ุงุฌุฑุง Jupyter ุฑู ุณุฑูุฑ ุฑุงู ุฏูุฑ (Remote Server) ๐ป

ุดูุง ุจุงุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ SSH ุจู ุณุฑูุฑ ุฑุงู ุฏูุฑ ูุชุตู ุดูุฏ ู Jupyter ุฑุง ุจุง ุชูุธูุงุช ุฎุงุต ุงุฌุฑุง ฺฉูุฏ.

ุงูู. ุชููุฏ ูุงู ูพฺฉุฑุจูุฏ (Configuration)

ุฏุฑ ุณุฑูุฑ ุฑุงู ุฏูุฑุ ุงฺฏุฑ ูุจูุงู ูุงู ูพฺฉุฑุจูุฏ ุจุฑุง Jupyter Notebook ูุฏุงุฑุฏุ ุขู ุฑุง ุงุฌุงุฏ ฺฉูุฏ:
Bash

jupyter notebook --generate-config

ุจ. ุชูุธู ุฑูุฒ ุนุจูุฑ (Password)

ุจุฑุง ุงููุชุ ฺฉ ุฑูุฒ ุนุจูุฑ (Password) ุจุฑุง ุฏุณุชุฑุณ ุจู ููุช ุจูฺฉ ุชูุธู ฺฉูุฏ. Jupyter ฺฉ ูุด (Hash) ุงุฒ ุงู ุฑูุฒ ุนุจูุฑ ุฑุง ุฏุฑ ูุงู ูพฺฉุฑุจูุฏ ุฐุฎุฑู ูโฺฉูุฏ:
Bash

jupyter notebook password

ุฌ. ูุฑุงุด ูุงู ูพฺฉุฑุจูุฏ

ูุงู ูพฺฉุฑุจูุฏ ุงุฌุงุฏ ุดุฏู (ูุนูููุงู ุฏุฑ ูุณุฑ ~/.jupyter/jupyter_notebook_config.py) ุฑุง ุจุง ฺฉ ูุฑุงุดฺฏุฑ ุจุงุฒ ฺฉูุฏ ู ุฎุทูุท ุฒุฑ ุฑุง (ุจุง ุญุฐู # ุงุฒ ุงุจุชุฏุง ุขูโูุง) ุชูุธู ฺฉูุฏ.

    ุชูุฌู: ฺฉุงููุช ูุงุฑุณ ูฺฏุฐุงุฑุฏ ู ูุชู ูุงุฑุณ ฺุงูพ ูฺฉูุฏ ุฏุฑ ุจุฎุด ฺฉุฏ.

Python

# Set the IP address to listen on.
c.NotebookApp.ip = '0.0.0.0'

# Allow connections from any host (important for remote access).
c.NotebookApp.allow_remote_access = True

# Set the port (e.g., 8888 is default, change if needed).
c.NotebookApp.port = 8888

# Disable automatic browser launch.
c.NotebookApp.open_browser = False

ุฏ. ุงุฌุฑุง ุณุฑูุฑ

ุงฺฉููู ุณุฑูุฑ ุฑุง ุฑู ูพูุฑุช ูุดุฎุต ุดุฏู (ูุซูุงู ธธธธ) ุงุฌุฑุง ฺฉูุฏ. ุงฺฏุฑ ููโุฎูุงูุฏ ุจุง ุจุณุชู ูพูุฌุฑู ุชุฑููุงู ุณุฑูุฑ ูุชููู ุดูุฏุ ูโุชูุงูุฏ ุงุฒ ุฏุณุชูุฑุงุช ูุงููุฏ nohup ุง screen/tmux ุงุณุชูุงุฏู ฺฉูุฏ.

ุฑูุด ุณุงุฏู (ุจุฏูู nohup):
Bash

jupyter notebook

ุฑูุด ุชูุตู ุดุฏู (ุจุฑุง ุงุฌุฑุง ุฏุงุฆู ุฏุฑ ูพุณโุฒููู):
Bash

nohup jupyter notebook &

ฒ. ุงุชุตุงู ุงุฒ ุณุณุชู ูุญู (Local Machine) ๐

ุงุฒ ุขูุฌุง ฺฉู ุดูุง ุจู ุทูุฑ ูุณุชูู ููโุฎูุงูุฏ ูพูุฑุช ธธธธ ุณุฑูุฑ ุฑุง ุฏุฑ ูุนุฑุถ ุงูุชุฑูุช ูุฑุงุฑ ุฏูุฏ (ฺฉู ุงุฒ ูุธุฑ ุงููุช ุฎุทุฑูุงฺฉ ุงุณุช)ุ ุจูุชุฑู ุฑุงู ุงุณุชูุงุฏู ุงุฒ SSH Tunneling ุงุณุช.

SSH Tunneling

ุจุง ุงุณุชูุงุฏู ุงุฒ ุฏุณุชูุฑ SSH ุฒุฑ ุฏุฑ ุชุฑููุงู ุณุณุชู ูุญู ุฎูุฏ (ูู ุณุฑูุฑ ุฑุงู ุฏูุฑ)ุ ฺฉ ุชููู ุงูู ุงุฌุงุฏ ูโฺฉูุฏ ฺฉู ูพูุฑุช ูุญู ุดูุง (ูุซูุงู ธธธธ) ุฑุง ุจู ูพูุฑุช ุณุฑูุฑ ุฑุงู ุฏูุฑ (ธธธธ) ูุชุตู ูโฺฉูุฏ:
Bash

ssh -N -f -L 8888:localhost:8888 username@remote_server_ip

ูพุงุฑุงูุชุฑ	ุชูุถุญุงุช
-N	ูฺ ุฏุณุชูุฑ ุงุฒ ุฑุงู ุฏูุฑ ุงุฌุฑุง ูฺฉู.
-f	SSH ุฑุง ุฏุฑ ูพุณโุฒููู ุงุฌุฑุง ฺฉู.
-L	ูพูุฑุช ูุญู (Local) ุฑุง ุจู ูพูุฑุช ุณุฑูุฑ (Remote) ููุชูู (Forward) ฺฉู.
8888:localhost:8888	ูพูุฑุช ูุญู ธธธธ ุจู localhost:8888 ุณุฑูุฑ ููพ ูโุดูุฏ.
username	ูุงู ฺฉุงุฑุจุฑ ุดูุง ุฏุฑ ุณุฑูุฑ ุฑุงู ุฏูุฑ.
remote_server_ip	ุขุฏุฑุณ IP ุง ูุงู ุฏุงููู ุณุฑูุฑ ุฑุงู ุฏูุฑ.

ุงุชุตุงู ุฏุฑ ูุฑูุฑฺฏุฑ

ูพุณ ุงุฒ ุงุฌุฑุง ููููุชโุขูุฒ ุฏุณุชูุฑ SSH Tunneling:

    ูุฑูุฑฺฏุฑ ูุจ ุฎูุฏ ุฑุง ุฏุฑ ุณุณุชู ูุญู ุจุงุฒ ฺฉูุฏ.

    ุขุฏุฑุณ ุฒุฑ ุฑุง ูุงุฑุฏ ฺฉูุฏ:

    http://localhost:8888

ูุฑูุฑฺฏุฑ ุดูุง ุจู ุทูุฑ ุงูู ุงุฒ ุทุฑู ุชููู SSH ุจู ุณุฑูุฑ Jupyter ุฑู ูุงุดู ุฑุงู ุฏูุฑ ูุชุตู ูโุดูุฏ. ุณูพุณ ุงุฒ ุดูุง ุฑูุฒ ุนุจูุฑ (ฺฉู ุฏุฑ ูุฑุญูู ฑุจ ุชูุธู ฺฉุฑุฏุฏ) ุง ุชูฺฉู (ฺฉู ุฏุฑ ุชุฑููุงู ุณุฑูุฑ ูุดุงู ุฏุงุฏู ุดุฏู) ุฎูุงุณุชู ูโุดูุฏ ุชุง ูุงุฑุฏ ุดูุฏ.

ุขุง ูุงูุฏ ุฏุฑ ููุฑุฏ ุชูุธู ูุงุฑูุงู (Firewall) ุฑู ุณุฑูุฑ ุฑุงู ุฏูุฑ ุจุฑุง ุงููุช ุจุดุชุฑ ุงุทูุงุนุงุช ฺฉุณุจ ฺฉูุฏุ

# Configuration file for notebook.


c = get_config()  #noqa


#------------------------------------------------------------------------------

# Application(SingletonConfigurable) configuration

#------------------------------------------------------------------------------

## This is an application.


## The date format used by logging formatters for %(asctime)s

#  Default: '%Y-%m-%d %H:%M:%S'

# c.Application.log_datefmt = '%Y-%m-%d %H:%M:%S'


## The Logging format template

#  Default: '[%(name)s]%(highlevel)s %(message)s'

# c.Application.log_format = '[%(name)s]%(highlevel)s %(message)s'


## Set the log level by value or name.

#  Choices: any of [0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL']

#  Default: 30

# c.Application.log_level = 30


## Configure additional log handlers.

#  

#  The default stderr logs handler is configured by the log_level, log_datefmt

#  and log_format settings.

#  

#  This configuration can be used to configure additional handlers (e.g. to

#  output the log to a file) or for finer control over the default handlers.

#  

#  If provided this should be a logging configuration dictionary, for more

#  information see:

#  https://docs.python.org/3/library/logging.config.html#logging-config-

#  dictschema

#  

#  This dictionary is merged with the base logging configuration which defines

#  the following:

#  

#  * A logging formatter intended for interactive use called

#    ``console``.

#  * A logging handler that writes to stderr called

#    ``console`` which uses the formatter ``console``.

#  * A logger with the name of this application set to ``DEBUG``

#    level.

#  

#  This example adds a new handler that writes to a file:

#  

#  .. code-block:: python

#  

#     c.Application.logging_config = {

#         "handlers": {

#             "file": {

#                 "class": "logging.FileHandler",

#                 "level": "DEBUG",

#                 "filename": "<path/to/file>",

#             }

#         },

#         "loggers": {

#             "<application-name>": {

#                 "level": "DEBUG",

#                 # NOTE: if you don't list the default "console"

#                 # handler here then it will be disabled

#                 "handlers": ["console", "file"],

#             },

#         },

#     }

#  Default: {}

# c.Application.logging_config = {}


## Instead of starting the Application, dump configuration to stdout

#  Default: False

# c.Application.show_config = False


## Instead of starting the Application, dump configuration to stdout (as JSON)

#  Default: False

# c.Application.show_config_json = False


#------------------------------------------------------------------------------

# JupyterApp(Application) configuration

#------------------------------------------------------------------------------

## Base class for Jupyter applications


## Answer yes to any prompts.

#  Default: False

# c.JupyterApp.answer_yes = False


## Full path of a config file.

#  Default: ''

# c.JupyterApp.config_file = ''


## Specify a config file to load.

#  Default: ''

# c.JupyterApp.config_file_name = ''


## Generate default config file.

#  Default: False

# c.JupyterApp.generate_config = False


## The date format used by logging formatters for %(asctime)s

#  See also: Application.log_datefmt

# c.JupyterApp.log_datefmt = '%Y-%m-%d %H:%M:%S'


## The Logging format template

#  See also: Application.log_format

# c.JupyterApp.log_format = '[%(name)s]%(highlevel)s %(message)s'


## Set the log level by value or name.

#  See also: Application.log_level

# c.JupyterApp.log_level = 30


## 

#  See also: Application.logging_config

# c.JupyterApp.logging_config = {}


## Instead of starting the Application, dump configuration to stdout

#  See also: Application.show_config

# c.JupyterApp.show_config = False


## Instead of starting the Application, dump configuration to stdout (as JSON)

#  See also: Application.show_config_json

# c.JupyterApp.show_config_json = False


#------------------------------------------------------------------------------

# ExtensionApp(JupyterApp) configuration

#------------------------------------------------------------------------------

## Base class for configurable Jupyter Server Extension Applications.

#  

#  ExtensionApp subclasses can be initialized two ways:

#  

#  - Extension is listed as a jpserver_extension, and ServerApp calls

#    its load_jupyter_server_extension classmethod. This is the

#    classic way of loading a server extension.

#  

#  - Extension is launched directly by calling its `launch_instance`

#    class method. This method can be set as a entry_point in

#    the extensions setup.py.


## Answer yes to any prompts.

#  See also: JupyterApp.answer_yes

# c.ExtensionApp.answer_yes = False


## Full path of a config file.

#  See also: JupyterApp.config_file

# c.ExtensionApp.config_file = ''


## Specify a config file to load.

#  See also: JupyterApp.config_file_name

# c.ExtensionApp.config_file_name = ''


#  Default: ''

# c.ExtensionApp.default_url = ''


## Generate default config file.

#  See also: JupyterApp.generate_config

# c.ExtensionApp.generate_config = False


## Handlers appended to the server.

#  Default: []

# c.ExtensionApp.handlers = []


## The date format used by logging formatters for %(asctime)s

#  See also: Application.log_datefmt

# c.ExtensionApp.log_datefmt = '%Y-%m-%d %H:%M:%S'


## The Logging format template

#  See also: Application.log_format

# c.ExtensionApp.log_format = '[%(name)s]%(highlevel)s %(message)s'


## Set the log level by value or name.

#  See also: Application.log_level

# c.ExtensionApp.log_level = 30


## 

#  See also: Application.logging_config

# c.ExtensionApp.logging_config = {}


## Whether to open in a browser after starting.

#          The specific browser used is platform dependent and

#          determined by the python standard library `webbrowser`

#          module, unless it is overridden using the --browser

#          (ServerApp.browser) configuration option.

#  Default: False

# c.ExtensionApp.open_browser = False


## Settings that will passed to the server.

#  Default: {}

# c.ExtensionApp.settings = {}


## Instead of starting the Application, dump configuration to stdout

#  See also: Application.show_config

# c.ExtensionApp.show_config = False


## Instead of starting the Application, dump configuration to stdout (as JSON)

#  See also: Application.show_config_json

# c.ExtensionApp.show_config_json = False


## paths to search for serving static files.

#  

#          This allows adding javascript/css to be available from the notebook server machine,

#          or overriding individual files in the IPython

#  Default: []

# c.ExtensionApp.static_paths = []


## Url where the static assets for the extension are served.

#  Default: ''

# c.ExtensionApp.static_url_prefix = ''


## Paths to search for serving jinja templates.

#  

#          Can be used to override templates from notebook.templates.

#  Default: []

# c.ExtensionApp.template_paths = []


#------------------------------------------------------------------------------

# LabServerApp(ExtensionApp) configuration

#------------------------------------------------------------------------------

## A Lab Server Application that runs out-of-the-box


## "A list of comma-separated URIs to get the allowed extensions list

#  

#  .. versionchanged:: 2.0.0

#      `LabServerApp.whitetlist_uris` renamed to `allowed_extensions_uris`

#  Default: ''

# c.LabServerApp.allowed_extensions_uris = ''


## Answer yes to any prompts.

#  See also: JupyterApp.answer_yes

# c.LabServerApp.answer_yes = False


## The application settings directory.

#  Default: ''

# c.LabServerApp.app_settings_dir = ''


## The url path for the application.

#  Default: '/lab'

# c.LabServerApp.app_url = '/lab'


## Deprecated, use `LabServerApp.blocked_extensions_uris`

#  Default: ''

# c.LabServerApp.blacklist_uris = ''


## A list of comma-separated URIs to get the blocked extensions list

#  

#  .. versionchanged:: 2.0.0

#      `LabServerApp.blacklist_uris` renamed to `blocked_extensions_uris`

#  Default: ''

# c.LabServerApp.blocked_extensions_uris = ''


## Whether to cache files on the server. This should be `True` except in dev

#  mode.

#  Default: True

# c.LabServerApp.cache_files = True


## Full path of a config file.

#  See also: JupyterApp.config_file

# c.LabServerApp.config_file = ''


## Specify a config file to load.

#  See also: JupyterApp.config_file_name

# c.LabServerApp.config_file_name = ''


## Whether getting a relative (False) or absolute (True) path when copying a

#  path.

#  Default: False

# c.LabServerApp.copy_absolute_path = False


## Extra paths to look for federated JupyterLab extensions

#  Default: []

# c.LabServerApp.extra_labextensions_path = []


## Generate default config file.

#  See also: JupyterApp.generate_config

# c.LabServerApp.generate_config = False


## Handlers appended to the server.

#  See also: ExtensionApp.handlers

# c.LabServerApp.handlers = []


## Options to pass to the jinja2 environment for this

#  Default: {}

# c.LabServerApp.jinja2_options = {}


## The standard paths to look in for federated JupyterLab extensions

#  Default: []

# c.LabServerApp.labextensions_path = []


## The url for federated JupyterLab extensions

#  Default: ''

# c.LabServerApp.labextensions_url = ''


## The interval delay in seconds to refresh the lists

#  Default: 3600

# c.LabServerApp.listings_refresh_seconds = 3600


## The optional kwargs to use for the listings HTTP requests             as

#  described on https://2.python-requests.org/en/v2.7.0/api/#requests.request

#  Default: {}

# c.LabServerApp.listings_request_options = {}


## The listings url.

#  Default: ''

# c.LabServerApp.listings_url = ''


## The date format used by logging formatters for %(asctime)s

#  See also: Application.log_datefmt

# c.LabServerApp.log_datefmt = '%Y-%m-%d %H:%M:%S'


## The Logging format template

#  See also: Application.log_format

# c.LabServerApp.log_format = '[%(name)s]%(highlevel)s %(message)s'


## Set the log level by value or name.

#  See also: Application.log_level

# c.LabServerApp.log_level = 30


## 

#  See also: Application.logging_config

# c.LabServerApp.logging_config = {}


## Whether a notebook should start a kernel automatically.

#  Default: True

# c.LabServerApp.notebook_starts_kernel = True


## Whether to open in a browser after starting.

#  See also: ExtensionApp.open_browser

# c.LabServerApp.open_browser = False


## The optional location of the settings schemas directory. If given, a handler

#  will be added for settings.

#  Default: ''

# c.LabServerApp.schemas_dir = ''


## Settings that will passed to the server.

#  See also: ExtensionApp.settings

# c.LabServerApp.settings = {}


## The url path of the settings handler.

#  Default: ''

# c.LabServerApp.settings_url = ''


## Instead of starting the Application, dump configuration to stdout

#  See also: Application.show_config

# c.LabServerApp.show_config = False


## Instead of starting the Application, dump configuration to stdout (as JSON)

#  See also: Application.show_config_json

# c.LabServerApp.show_config_json = False


## The optional location of local static files. If given, a static file handler

#  will be added.

#  Default: ''

# c.LabServerApp.static_dir = ''


## paths to search for serving static files.

#  See also: ExtensionApp.static_paths

# c.LabServerApp.static_paths = []


## Url where the static assets for the extension are served.

#  See also: ExtensionApp.static_url_prefix

# c.LabServerApp.static_url_prefix = ''


## Paths to search for serving jinja templates.

#  See also: ExtensionApp.template_paths

# c.LabServerApp.template_paths = []


## The application templates directory.

#  Default: ''

# c.LabServerApp.templates_dir = ''


## The optional location of the themes directory. If given, a handler will be

#  added for themes.

#  Default: ''

# c.LabServerApp.themes_dir = ''


## The theme url.

#  Default: ''

# c.LabServerApp.themes_url = ''


## The url path of the translations handler.

#  Default: ''

# c.LabServerApp.translations_api_url = ''


## The url path of the tree handler.

#  Default: ''

# c.LabServerApp.tree_url = ''


## The optional location of the user settings directory.

#  Default: ''

# c.LabServerApp.user_settings_dir = ''


## Deprecated, use `LabServerApp.allowed_extensions_uris`

#  Default: ''

# c.LabServerApp.whitelist_uris = ''


## The url path of the workspaces API.

#  Default: ''

# c.LabServerApp.workspaces_api_url = ''


## The optional location of the saved workspaces directory. If given, a handler

#  will be added for workspaces.

#  Default: ''

# c.LabServerApp.workspaces_dir = ''


#------------------------------------------------------------------------------

# JupyterNotebookApp(LabServerApp) configuration

#------------------------------------------------------------------------------

## The notebook server extension app.


## 

#  See also: LabServerApp.allowed_extensions_uris

# c.JupyterNotebookApp.allowed_extensions_uris = ''


## Answer yes to any prompts.

#  See also: JupyterApp.answer_yes

# c.JupyterNotebookApp.answer_yes = False


## The application settings directory.

#  Default: ''

# c.JupyterNotebookApp.app_settings_dir = ''


## The url path for the application.

#  Default: '/lab'

# c.JupyterNotebookApp.app_url = '/lab'


## Deprecated, use `LabServerApp.blocked_extensions_uris`

#  See also: LabServerApp.blacklist_uris

# c.JupyterNotebookApp.blacklist_uris = ''


## 

#  See also: LabServerApp.blocked_extensions_uris

# c.JupyterNotebookApp.blocked_extensions_uris = ''


## Whether to cache files on the server. This should be `True` except in dev

#  mode.

#  Default: True

# c.JupyterNotebookApp.cache_files = True


## Full path of a config file.

#  See also: JupyterApp.config_file

# c.JupyterNotebookApp.config_file = ''


## Specify a config file to load.

#  See also: JupyterApp.config_file_name

# c.JupyterNotebookApp.config_file_name = ''


## Whether getting a relative (False) or absolute (True) path when copying a

#  path.

#  Default: False

# c.JupyterNotebookApp.copy_absolute_path = False


## Whether custom CSS is loaded on the page.

#          Defaults to True and custom CSS is loaded.

#  Default: True

# c.JupyterNotebookApp.custom_css = True


## The default URL to redirect to from `/`

#  Default: '/tree'

# c.JupyterNotebookApp.default_url = '/tree'


## Whether to expose the global app instance to browser via window.jupyterapp

#  Default: False

# c.JupyterNotebookApp.expose_app_in_browser = False


## Extra paths to look for federated JupyterLab extensions

#  Default: []

# c.JupyterNotebookApp.extra_labextensions_path = []


## Generate default config file.

#  See also: JupyterApp.generate_config

# c.JupyterNotebookApp.generate_config = False


## Handlers appended to the server.

#  See also: ExtensionApp.handlers

# c.JupyterNotebookApp.handlers = []


## Options to pass to the jinja2 environment for this

#  Default: {}

# c.JupyterNotebookApp.jinja2_options = {}


## The standard paths to look in for federated JupyterLab extensions

#  Default: []

# c.JupyterNotebookApp.labextensions_path = []


## The url for federated JupyterLab extensions

#  Default: ''

# c.JupyterNotebookApp.labextensions_url = ''


## The interval delay in seconds to refresh the lists

#  See also: LabServerApp.listings_refresh_seconds

# c.JupyterNotebookApp.listings_refresh_seconds = 3600


## The optional kwargs to use for the listings HTTP requests             as

#  described on https://2.python-requests.org/en/v2.7.0/api/#requests.request

#  See also: LabServerApp.listings_request_options

# c.JupyterNotebookApp.listings_request_options = {}


## The listings url.

#  Default: ''

# c.JupyterNotebookApp.listings_url = ''


## The date format used by logging formatters for %(asctime)s

#  See also: Application.log_datefmt

# c.JupyterNotebookApp.log_datefmt = '%Y-%m-%d %H:%M:%S'


## The Logging format template

#  See also: Application.log_format

# c.JupyterNotebookApp.log_format = '[%(name)s]%(highlevel)s %(message)s'


## Set the log level by value or name.

#  See also: Application.log_level

# c.JupyterNotebookApp.log_level = 30


## 

#  See also: Application.logging_config

# c.JupyterNotebookApp.logging_config = {}


## Whether a notebook should start a kernel automatically.

#  Default: True

# c.JupyterNotebookApp.notebook_starts_kernel = True


## Whether to open in a browser after starting.

#  See also: ExtensionApp.open_browser

# c.JupyterNotebookApp.open_browser = False


## The optional location of the settings schemas directory. If given, a handler

#  will be added for settings.

#  Default: ''

# c.JupyterNotebookApp.schemas_dir = ''


## Settings that will passed to the server.

#  See also: ExtensionApp.settings

# c.JupyterNotebookApp.settings = {}


## The url path of the settings handler.

#  Default: ''

# c.JupyterNotebookApp.settings_url = ''


## Instead of starting the Application, dump configuration to stdout

#  See also: Application.show_config

# c.JupyterNotebookApp.show_config = False


## Instead of starting the Application, dump configuration to stdout (as JSON)

#  See also: Application.show_config_json

# c.JupyterNotebookApp.show_config_json = False


## The optional location of local static files. If given, a static file handler

#  will be added.

#  Default: ''

# c.JupyterNotebookApp.static_dir = ''


## paths to search for serving static files.

#  See also: ExtensionApp.static_paths

# c.JupyterNotebookApp.static_paths = []


## Url where the static assets for the extension are served.

#  See also: ExtensionApp.static_url_prefix

# c.JupyterNotebookApp.static_url_prefix = ''


## Paths to search for serving jinja templates.

#  See also: ExtensionApp.template_paths

# c.JupyterNotebookApp.template_paths = []


## The application templates directory.

#  Default: ''

# c.JupyterNotebookApp.templates_dir = ''


## The optional location of the themes directory. If given, a handler will be

#  added for themes.

#  Default: ''

# c.JupyterNotebookApp.themes_dir = ''


## The theme url.

#  Default: ''

# c.JupyterNotebookApp.themes_url = ''


## The url path of the translations handler.

#  Default: ''

# c.JupyterNotebookApp.translations_api_url = ''


## The url path of the tree handler.

#  Default: ''

# c.JupyterNotebookApp.tree_url = ''


## The optional location of the user settings directory.

#  Default: ''

# c.JupyterNotebookApp.user_settings_dir = ''


## Deprecated, use `LabServerApp.allowed_extensions_uris`

#  See also: LabServerApp.whitelist_uris

# c.JupyterNotebookApp.whitelist_uris = ''


## The url path of the workspaces API.

#  Default: ''

# c.JupyterNotebookApp.workspaces_api_url = ''


## The optional location of the saved workspaces directory. If given, a handler

#  will be added for workspaces.

#  Default: ''

# c.JupyterNotebookApp.workspaces_dir = ''


#------------------------------------------------------------------------------

# ServerApp(JupyterApp) configuration

#------------------------------------------------------------------------------

## The Jupyter Server application class.


## Set the Access-Control-Allow-Credentials: true header

#  Default: False

# c.ServerApp.allow_credentials = False


## Whether or not to allow external kernels, whose connection files are placed in

#  external_connection_dir.

#  Default: False

# c.ServerApp.allow_external_kernels = False


## Set the Access-Control-Allow-Origin header

#  

#          Use '*' to allow any origin to access your server.

#  

#          Takes precedence over allow_origin_pat.

#  Default: ''

# c.ServerApp.allow_origin = ''


## Use a regular expression for the Access-Control-Allow-Origin header

#  

#          Requests from an origin matching the expression will get replies with:

#  

#              Access-Control-Allow-Origin: origin

#  

#          where `origin` is the origin of the request.

#  

#          Ignored if allow_origin is set.

#  Default: ''

# c.ServerApp.allow_origin_pat = ''


## DEPRECATED in 2.0. Use PasswordIdentityProvider.allow_password_change

#  Default: True

# c.ServerApp.allow_password_change = True


## Allow requests where the Host header doesn't point to a local server

#  

#         By default, requests get a 403 forbidden response if the 'Host' header

#         shows that the browser thinks it's on a non-local domain.

#         Setting this option to True disables this check.

#  

#         This protects against 'DNS rebinding' attacks, where a remote web server

#         serves you a page and then changes its DNS to send later requests to a

#         local IP, bypassing same-origin checks.

#  

#         Local IP addresses (such as 127.0.0.1 and ::1) are allowed as local,

#         along with hostnames configured in local_hostnames.

#  Default: False

# c.ServerApp.allow_remote_access = False


## Whether to allow the user to run the server as root.

#  Default: False

# c.ServerApp.allow_root = False


## Allow unauthenticated access to endpoints without authentication rule.

#  

#          When set to `True` (default in jupyter-server 2.0, subject to change

#          in the future), any request to an endpoint without an authentication rule

#          (either `@tornado.web.authenticated`, or `@allow_unauthenticated`)

#          will be permitted, regardless of whether user has logged in or not.

#  

#          When set to `False`, logging in will be required for access to each endpoint,

#          excluding the endpoints marked with `@allow_unauthenticated` decorator.

#  

#          This option can be configured using `JUPYTER_SERVER_ALLOW_UNAUTHENTICATED_ACCESS`

#          environment variable: any non-empty value other than "true" and "yes" will

#          prevent unauthenticated access to endpoints without `@allow_unauthenticated`.

#  Default: True

# c.ServerApp.allow_unauthenticated_access = True


## Answer yes to any prompts.

#  See also: JupyterApp.answer_yes

# c.ServerApp.answer_yes = False


## "

#          Require authentication to access prometheus metrics.

#  Default: True

# c.ServerApp.authenticate_prometheus = True


## The authorizer class to use.

#  Default: 'jupyter_server.auth.authorizer.AllowAllAuthorizer'

# c.ServerApp.authorizer_class = 'jupyter_server.auth.authorizer.AllowAllAuthorizer'


## Reload the webapp when changes are made to any Python src files.

#  Default: False

# c.ServerApp.autoreload = False


## The base URL for the Jupyter server.

#  

#                         Leading and trailing slashes can be omitted,

#                         and will automatically be added.

#  Default: '/'

# c.ServerApp.base_url = '/'


## Specify what command to use to invoke a web

#                        browser when starting the server. If not specified, the

#                        default browser will be determined by the `webbrowser`

#                        standard library module, which allows setting of the

#                        BROWSER environment variable to override it.

#  Default: ''

# c.ServerApp.browser = ''


## The full path to an SSL/TLS certificate file.

#  Default: ''

# c.ServerApp.certfile = ''


## The full path to a certificate authority certificate for SSL/TLS client

#  authentication.

#  Default: ''

# c.ServerApp.client_ca = ''


## Full path of a config file.

#  See also: JupyterApp.config_file

# c.ServerApp.config_file = ''


## Specify a config file to load.

#  See also: JupyterApp.config_file_name

# c.ServerApp.config_file_name = ''


## The config manager class to use

#  Default: 'jupyter_server.services.config.manager.ConfigManager'

# c.ServerApp.config_manager_class = 'jupyter_server.services.config.manager.ConfigManager'


## The content manager class to use.

#  Default: 'jupyter_server.services.contents.largefilemanager.AsyncLargeFileManager'

# c.ServerApp.contents_manager_class = 'jupyter_server.services.contents.largefilemanager.AsyncLargeFileManager'


## DEPRECATED. Use IdentityProvider.cookie_options

#  Default: {}

# c.ServerApp.cookie_options = {}


## The random bytes used to secure cookies.

#          By default this is generated on first start of the server and persisted across server

#          sessions by writing the cookie secret into the `cookie_secret_file` file.

#          When using an executable config file you can override this to be random at each server restart.

#  

#          Note: Cookie secrets should be kept private, do not share config files with

#          cookie_secret stored in plaintext (you can read the value from a file).

#  Default: b''

# c.ServerApp.cookie_secret = b''


## The file where the cookie secret is stored.

#  Default: ''

# c.ServerApp.cookie_secret_file = ''


## Override URL shown to users.

#  

#          Replace actual URL, including protocol, address, port and base URL,

#          with the given value when displaying URL to the users. Do not change

#          the actual connection URL. If authentication token is enabled, the

#          token is added to the custom URL automatically.

#  

#          This option is intended to be used when the URL to display to the user

#          cannot be determined reliably by the Jupyter server (proxified

#          or containerized setups for example).

#  Default: ''

# c.ServerApp.custom_display_url = ''


## The default URL to redirect to from `/`

#  Default: '/'

# c.ServerApp.default_url = '/'


## Disable cross-site-request-forgery protection

#  

#          Jupyter server includes protection from cross-site request forgeries,

#          requiring API requests to either:

#  

#          - originate from pages served by this server (validated with XSRF cookie and token), or

#          - authenticate with a token

#  

#          Some anonymous compute resources still desire the ability to run code,

#          completely without authentication.

#          These services can disable all authentication and security checks,

#          with the full knowledge of what that implies.

#  Default: False

# c.ServerApp.disable_check_xsrf = False


## The directory to look at for external kernel connection files, if

#  allow_external_kernels is True. Defaults to Jupyter

#  runtime_dir/external_kernels. Make sure that this directory is not filled with

#  left-over connection files, that could result in unnecessary kernel manager

#  creations.

#  Default: None

# c.ServerApp.external_connection_dir = None


## handlers that should be loaded at higher priority than the default services

#  Default: []

# c.ServerApp.extra_services = []


## Extra paths to search for serving static files.

#  

#          This allows adding javascript/css to be available from the Jupyter server machine,

#          or overriding individual files in the IPython

#  Default: []

# c.ServerApp.extra_static_paths = []


## Extra paths to search for serving jinja templates.

#  

#          Can be used to override templates from jupyter_server.templates.

#  Default: []

# c.ServerApp.extra_template_paths = []


## Open the named file when the application is launched.

#  Default: ''

# c.ServerApp.file_to_run = ''


## The URL prefix where files are opened directly.

#  Default: 'notebooks'

# c.ServerApp.file_url_prefix = 'notebooks'


## Generate default config file.

#  See also: JupyterApp.generate_config

# c.ServerApp.generate_config = False


## DEPRECATED. Use IdentityProvider.get_secure_cookie_kwargs

#  Default: {}

# c.ServerApp.get_secure_cookie_kwargs = {}


## The identity provider class to use.

#  Default: 'jupyter_server.auth.identity.PasswordIdentityProvider'

# c.ServerApp.identity_provider_class = 'jupyter_server.auth.identity.PasswordIdentityProvider'


## DEPRECATED. Use ZMQChannelsWebsocketConnection.iopub_data_rate_limit

#  Default: 0.0

# c.ServerApp.iopub_data_rate_limit = 0.0


## DEPRECATED. Use ZMQChannelsWebsocketConnection.iopub_msg_rate_limit

#  Default: 0.0

# c.ServerApp.iopub_msg_rate_limit = 0.0


## The IP address the Jupyter server will listen on.

#  Default: 'localhost'

# c.ServerApp.ip = 'localhost'


## Supply extra arguments that will be passed to Jinja environment.

#  Default: {}

# c.ServerApp.jinja_environment_options = {}


## Extra variables to supply to jinja templates when rendering.

#  Default: {}

# c.ServerApp.jinja_template_vars = {}


## Dict of Python modules to load as Jupyter server extensions.Entry values can

#  be used to enable and disable the loading ofthe extensions. The extensions

#  will be loaded in alphabetical order.

#  Default: {}

# c.ServerApp.jpserver_extensions = {}


## The kernel manager class to use.

#  Default: 'jupyter_server.services.kernels.kernelmanager.MappingKernelManager'

# c.ServerApp.kernel_manager_class = 'jupyter_server.services.kernels.kernelmanager.MappingKernelManager'


## The kernel spec manager class to use. Should be a subclass of

#  `jupyter_client.kernelspec.KernelSpecManager`.

#  

#  The Api of KernelSpecManager is provisional and might change without warning

#  between this version of Jupyter and the next stable one.

#  Default: 'builtins.object'

# c.ServerApp.kernel_spec_manager_class = 'builtins.object'


## The kernel websocket connection class to use.

#  Default: 'jupyter_server.services.kernels.connection.base.BaseKernelWebsocketConnection'

# c.ServerApp.kernel_websocket_connection_class = 'jupyter_server.services.kernels.connection.base.BaseKernelWebsocketConnection'


## DEPRECATED. Use ZMQChannelsWebsocketConnection.kernel_ws_protocol

#  Default: ''

# c.ServerApp.kernel_ws_protocol = ''


## The full path to a private key file for usage with SSL/TLS.

#  Default: ''

# c.ServerApp.keyfile = ''


## DEPRECATED. Use ZMQChannelsWebsocketConnection.limit_rate

#  Default: False

# c.ServerApp.limit_rate = False


## Hostnames to allow as local when allow_remote_access is False.

#  

#         Local IP addresses (such as 127.0.0.1 and ::1) are automatically accepted

#         as local as well.

#  Default: ['localhost']

# c.ServerApp.local_hostnames = ['localhost']


## The date format used by logging formatters for %(asctime)s

#  See also: Application.log_datefmt

# c.ServerApp.log_datefmt = '%Y-%m-%d %H:%M:%S'


## The Logging format template

#  See also: Application.log_format

# c.ServerApp.log_format = '[%(name)s]%(highlevel)s %(message)s'


## Set the log level by value or name.

#  See also: Application.log_level

# c.ServerApp.log_level = 30


## 

#  See also: Application.logging_config

# c.ServerApp.logging_config = {}


## The login handler class to use.

#  Default: 'jupyter_server.auth.login.LegacyLoginHandler'

# c.ServerApp.login_handler_class = 'jupyter_server.auth.login.LegacyLoginHandler'


## The logout handler class to use.

#  Default: 'jupyter_server.auth.logout.LogoutHandler'

# c.ServerApp.logout_handler_class = 'jupyter_server.auth.logout.LogoutHandler'


## Sets the maximum allowed size of the client request body, specified in the

#  Content-Length request header field. If the size in a request exceeds the

#  configured value, a malformed HTTP message is returned to the client.

#  

#  Note: max_body_size is applied even in streaming mode.

#  Default: 536870912

# c.ServerApp.max_body_size = 536870912


## Gets or sets the maximum amount of memory, in bytes, that is allocated for use

#  by the buffer manager.

#  Default: 536870912

# c.ServerApp.max_buffer_size = 536870912


## Gets or sets a lower bound on the open file handles process resource limit.

#  This may need to be increased if you run into an OSError: [Errno 24] Too many

#  open files. This is not applicable when running on Windows.

#  Default: 0

# c.ServerApp.min_open_files_limit = 0


## DEPRECATED, use root_dir.

#  Default: ''

# c.ServerApp.notebook_dir = ''


## Whether to open in a browser after starting.

#                          The specific browser used is platform dependent and

#                          determined by the python standard library `webbrowser`

#                          module, unless it is overridden using the --browser

#                          (ServerApp.browser) configuration option.

#  Default: False

# c.ServerApp.open_browser = False


## DEPRECATED in 2.0. Use PasswordIdentityProvider.hashed_password

#  Default: ''

# c.ServerApp.password = ''


## DEPRECATED in 2.0. Use PasswordIdentityProvider.password_required

#  Default: False

# c.ServerApp.password_required = False


## The port the server will listen on (env: JUPYTER_PORT).

#  Default: 0

# c.ServerApp.port = 0


## The number of additional ports to try if the specified port is not available

#  (env: JUPYTER_PORT_RETRIES).

#  Default: 50

# c.ServerApp.port_retries = 50


## Preferred starting directory to use for notebooks and kernels.

#  ServerApp.preferred_dir is deprecated in jupyter-server 2.0. Use

#  FileContentsManager.preferred_dir instead

#  Default: ''

# c.ServerApp.preferred_dir = ''


## DISABLED: use %pylab or %matplotlib in the notebook to enable matplotlib.

#  Default: 'disabled'

# c.ServerApp.pylab = 'disabled'


## If True, display controls to shut down the Jupyter server, such as menu items

#  or buttons.

#  Default: True

# c.ServerApp.quit_button = True


## DEPRECATED. Use ZMQChannelsWebsocketConnection.rate_limit_window

#  Default: 0.0

# c.ServerApp.rate_limit_window = 0.0


## Reraise exceptions encountered loading server extensions?

#  Default: False

# c.ServerApp.reraise_server_extension_failures = False


## The directory to use for notebooks and kernels.

#  Default: ''

# c.ServerApp.root_dir = ''


## The session manager class to use.

#  Default: 'builtins.object'

# c.ServerApp.session_manager_class = 'builtins.object'


## Instead of starting the Application, dump configuration to stdout

#  See also: Application.show_config

# c.ServerApp.show_config = False


## Instead of starting the Application, dump configuration to stdout (as JSON)

#  See also: Application.show_config_json

# c.ServerApp.show_config_json = False


## Shut down the server after N seconds with no kernelsrunning and no activity.

#  This can be used together with culling idle kernels

#  (MappingKernelManager.cull_idle_timeout) to shutdown the Jupyter server when

#  it's not in use. This is not precisely timed: it may shut down up to a minute

#  later. 0 (the default) disables this automatic shutdown.

#  Default: 0

# c.ServerApp.shutdown_no_activity_timeout = 0


## The UNIX socket the Jupyter server will listen on.

#  Default: ''

# c.ServerApp.sock = ''


## The permissions mode for UNIX socket creation (default: 0600).

#  Default: '0600'

# c.ServerApp.sock_mode = '0600'


## Supply SSL options for the tornado HTTPServer.

#              See the tornado docs for details.

#  Default: {}

# c.ServerApp.ssl_options = {}


## Paths to set up static files as immutable.

#  

#  This allow setting up the cache control of static files as immutable. It

#  should be used for static file named with a hash for instance.

#  Default: []

# c.ServerApp.static_immutable_cache = []


## Supply overrides for terminado. Currently only supports "shell_command".

#  Default: {}

# c.ServerApp.terminado_settings = {}


## Set to False to disable terminals.

#  

#           This does *not* make the server more secure by itself.

#           Anything the user can in a terminal, they can also do in a notebook.

#  

#           Terminals may also be automatically disabled if the terminado package

#           is not available.

#  Default: False

# c.ServerApp.terminals_enabled = False


## DEPRECATED. Use IdentityProvider.token

#  Default: '<DEPRECATED>'

# c.ServerApp.token = '<DEPRECATED>'


## Supply overrides for the tornado.web.Application that the Jupyter server uses.

#  Default: {}

# c.ServerApp.tornado_settings = {}


## Whether to trust or not X-Scheme/X-Forwarded-Proto and X-Real-Ip/X-Forwarded-

#  For headerssent by the upstream reverse proxy. Necessary if the proxy handles

#  SSL

#  Default: False

# c.ServerApp.trust_xheaders = False


## Disable launching browser by redirect file

#       For versions of notebook > 5.7.2, a security feature measure was added that

#       prevented the authentication token used to launch the browser from being visible.

#       This feature makes it difficult for other users on a multi-user system from

#       running code in your Jupyter session as you.

#       However, some environments (like Windows Subsystem for Linux (WSL) and Chromebooks),

#       launching a browser using a redirect file can lead the browser failing to load.

#       This is because of the difference in file structures/paths between the runtime and

#       the browser.

#  

#       Disabling this setting to False will disable this behavior, allowing the browser

#       to launch by using a URL and visible token (as before).

#  Default: True

# c.ServerApp.use_redirect_file = True


## Specify where to open the server on startup. This is the

#          `new` argument passed to the standard library method `webbrowser.open`.

#          The behaviour is not guaranteed, but depends on browser support. Valid

#          values are:

#  

#           - 2 opens a new tab,

#           - 1 opens a new window,

#           - 0 opens in an existing window.

#  

#          See the `webbrowser.open` documentation for details.

#  Default: 2

# c.ServerApp.webbrowser_open_new = 2


## Set the tornado compression options for websocket connections.

#  

#  This value will be returned from

#  :meth:`WebSocketHandler.get_compression_options`. None (default) will disable

#  compression. A dict (even an empty one) will enable compression.

#  

#  See the tornado docs for WebSocketHandler.get_compression_options for details.

#  Default: None

# c.ServerApp.websocket_compression_options = None


## Configure the websocket ping interval in seconds.

#  

#  Websockets are long-lived connections that are used by some Jupyter Server

#  extensions.

#  

#  Periodic pings help to detect disconnected clients and keep the connection

#  active. If this is set to None, then no pings will be performed.

#  

#  When a ping is sent, the client has ``websocket_ping_timeout`` seconds to

#  respond. If no response is received within this period, the connection will be

#  closed from the server side.

#  Default: 0

# c.ServerApp.websocket_ping_interval = 0


## Configure the websocket ping timeout in seconds.

#  

#  See ``websocket_ping_interval`` for details.

#  Default: 0

# c.ServerApp.websocket_ping_timeout = 0


## The base URL for websockets,

#          if it differs from the HTTP server (hint: it almost certainly doesn't).

#  

#          Should be in the form of an HTTP origin: ws[s]://hostname[:port]

#  Default: ''

# c.ServerApp.websocket_url = ''



ูุงู ฺฉู ุจุงุฏ ุฌุงฺฏุฒู ฺฉูู ุฑู ุจุฏู

ุจุฑ ุงุณุงุณ ุฏุฑุฎูุงุณุช ุดูุง ุจุฑุง ุงุฌุฑุง ุณุฑูุฑ Jupyter Notebook ุฑู ฺฉ ุณุฑูุฑ ุฑุงู ุฏูุฑ (Remote Server) ู ุงุชุตุงู ุงุฒ ุณุณุชู ูุญูุ ุจุงุฏ ุชูุธูุงุช ูุฑุจูุท ุจู IP Addressุ ุฏุณุชุฑุณ ุงุฒ ุฑุงู ุฏูุฑ ู ุนุฏู ุจุงุฒ ุดุฏู ุฎูุฏฺฉุงุฑ ูุฑูุฑฺฏุฑ ุฑุง ุฏุฑ ุจุฎุด ServerApp ูุนุงู ฺฉูุฏ.

ุงู ุชูุธูุงุช ุงูฺฉุงู ูโุฏููุฏ ฺฉู ุณุฑูุฑ Jupyter ุฑู ุชูุงู ุฑุงุจุทโูุง ุดุจฺฉู ุณุฑูุฑ ุฑุงู ุฏูุฑ ุดูุง (ุนู 0.0.0.0) ุงุฌุฑุง ุดูุฏ ู ุจุชูุงูุฏ ุงุฒ ุทุฑู SSH Tunneling ุจู ุขู ุฏุณุชุฑุณ ูพุฏุง ฺฉูุฏุ ุจุฏูู ุงูฺฉู ูุฑูุฑฺฏุฑ ุณุฑูุฑ ุฑุงู ุฏูุฑ ุจุงุฒ ุดูุฏ.

๐๏ธ ูุงู ูพฺฉุฑุจูุฏ ุงุตูุงุญ ุดุฏู (jupyter_notebook_config.py)

ูุทูุงู ูุญุชูุง ุฒุฑ ุฑุง ุฏุฑ ุงูุชูุง ูุงู ูพฺฉุฑุจูุฏ ุฎูุฏ ุงุถุงูู ุง ุฌุงฺฏุฒู ุฎุทูุท ูุดุงุจู ฺฉูุฏ. ุงฺฏุฑ ุฎุทูุท ุจุง ููุฏุงุฑ ูพุดโูุฑุถ ุฏุฑ ูุงู ูุฌูุฏ ุฏุงุฑูุฏุ ฺฉุงู ุงุณุช ฺฉุงููุช # ุฑุง ุงุฒ ุงุจุชุฏุง ุขูโูุง ุญุฐู ฺฉุฑุฏู ู ููุฏุงุฑ ุฑุง ุชูุธู ฺฉูุฏ.
Python

# Configuration settings for remote access

#------------------------------------------------------------------------------
# ServerApp(JupyterApp) configuration
#------------------------------------------------------------------------------

## The IP address the Jupyter server will listen on.
# Set to '0.0.0.0' to listen on all interfaces for remote connection.
c.ServerApp.ip = '0.0.0.0'

## Whether to open in a browser after starting.
# Set to False to prevent the server from trying to launch a browser
# on the remote machine.
c.ServerApp.open_browser = False

## Allow requests where the Host header doesn't point to a local server
# This is necessary when connecting via a remote IP address or hostname.
c.ServerApp.allow_remote_access = True

## The port the server will listen on (env: JUPYTER_PORT).
# Set the port explicitly (e.g., 8888 is default and common).
c.ServerApp.port = 8888

๐ ูฺฉุชู ููู: ุชูุธู ุฑูุฒ ุนุจูุฑ

ูพุณ ุงุฒ ุงุนูุงู ุชุบุฑุงุช ูููุ ุญุชูุงู ฺฉ ุฑูุฒ ุนุจูุฑ ุงูู ุจุฑุง ุฏุณุชุฑุณ ุจู ุฑุงุจุท ฺฉุงุฑุจุฑ Jupyter ุชูุธู ฺฉูุฏ. ุงู ฺฉุงุฑ ุฑุง ุฏุฑ ุชุฑููุงู ุณุฑูุฑ ุฑุงู ุฏูุฑ ุงูุฌุงู ุฏูุฏ:
Bash

jupyter notebook password

ุจุนุฏ ุงุฒ ูุงุฑุฏ ฺฉุฑุฏู ู ุชุฃุฏ ุฑูุฒ ุนุจูุฑุ Jupyter ฺฉ ูุด (Hash) ุงุฒ ุขู ุฑุง ุฏุฑ ูุงู ูพฺฉุฑุจูุฏ ุฐุฎุฑู ูโฺฉูุฏ ู ุงููุช ุงุชุตุงู ุดูุง ุฑุง ุชุถูู ูโฺฉูุฏ.

ุขุง ูุงุฒ ุจู ุชูุถุญ ูุฌุฏุฏ ูุญูู ุงุฌุฑุง ุฏุณุชูุฑ SSH Tunneling ุจุฑุง ุงุชุตุงู ุงุฒ ุณุณุชู ูุญู ุฎูุฏ ุฏุงุฑุฏุ

ู ุฎูุงู ุฌููพุชุฑ ุฑู โซโช/home/dmls/pytorch/bin/activateโฌโฌโซโช venv ุงุฌุฑุง ุดู

ุดูุง ูโุฎูุงูุฏ ุณุฑูุฑ Jupyter Notebook ุฑุง ุฏุฑ ฺฉ ูุญุท ูุฌุงุฒ ูพุงุชูู (Python Virtual Environment) ุฎุงุต ุจู ูุงู venv ฺฉู ุฏุฑ ูุณุฑ /home/dmls/pytorch/bin/activate ูุฑุงุฑ ุฏุงุฑุฏุ ุงุฌุฑุง ฺฉูุฏ.

ุจุฑุง ุงูุฌุงู ุงู ฺฉุงุฑุ ุจุงุฏ ุงุจุชุฏุง ูุญุท ูุฌุงุฒ ุฑุง ูุนุงู (Activate) ฺฉูุฏ ู ุณูพุณ ุฏุณุชูุฑ ุงุฌุฑุง Jupyter ุฑุง ุฏุฑ ููุงู ูุญุท ูุงุฑุฏ ููุงุฏ.

๐ ูุฑุงุญู ุงุฌุฑุง Jupyter ุฏุฑ ูุญุท ูุฌุงุฒ

ฑ. ูุฑูุฏ ุจู ุณุฑูุฑ ุฑุงู ุฏูุฑ (Remote Server)

ุงุจุชุฏุง ุจุง ุงุณุชูุงุฏู ุงุฒ SSH ุจู ุณุฑูุฑ ุฑุงู ุฏูุฑ ุฎูุฏ ูุชุตู ุดูุฏ.

ฒ. ูุนุงูโุณุงุฒ ูุญุท ูุฌุงุฒ (Activate venv)

ุฏุณุชูุฑ ูุนุงูโุณุงุฒ ูุญุท ูุฌุงุฒ ุฑุง ุฏุฑ ุชุฑููุงู ุณุฑูุฑ ุงุฌุฑุง ฺฉูุฏ.
Bash

source /home/dmls/pytorch/bin/activate

    ุชูุถุญ: ูพุณ ุงุฒ ุงุฌุฑุง ููููุชโุขูุฒ ุงู ุฏุณุชูุฑุ ูุงู ูุญุท ูุฌุงุฒ (ุงุญุชูุงูุงู pytorch ุง venv) ุฏุฑ ุงุจุชุฏุง ุฎุท ูุฑูุงู (Prompt) ุดูุง ููุงุด ุฏุงุฏู ูโุดูุฏ ฺฉู ูุดุงู ูโุฏูุฏ ุดูุง ุฏุฑ ุญุงู ุญุงุถุฑ ุฏุฑ ุขู ูุญุท ฺฉุงุฑ ูโฺฉูุฏ.

ณ. ุจุฑุฑุณ ูุตุจ Jupyter

ูุทูุฆู ุดูุฏ ฺฉู Jupyter Notebook ุง JupyterLab ุฏุฑ ุงู ูุญุท ูุฌุงุฒ ุฎุงุต ูุตุจ ุดุฏู ุจุงุดุฏ. ุงฺฏุฑ ูุตุจ ูุณุชุ ุงุฒ ุฏุณุชูุฑ ุฒุฑ ุงุณุชูุงุฏู ฺฉูุฏ:
Bash

pip install notebook
# OR (if you prefer JupyterLab)
# pip install jupyterlab

ด. ุงุฌุฑุง ุณุฑูุฑ Jupyter

ุญุงูุง ฺฉู ูุญุท ูุฌุงุฒ ูุนุงู ุงุณุชุ ุฏุณุชูุฑ ุงุฌุฑุง ุณุฑูุฑ ุฑุง ูุงุฑุฏ ฺฉูุฏ. ุจุง ุงู ฺฉุงุฑุ ุณุฑูุฑ ุจุง ุงุณุชูุงุฏู ุงุฒ ูพุงุชูู ู ฺฉุชุงุจุฎุงููโูุง ูุตุจ ุดุฏู ุฏุฑ ููู venv ุงุฌุฑุง ูโุดูุฏ.

ุจุฑุง ุงุฌุฑุง ุฏุฑ ูพุณโุฒููู (ุชูุตู ุดุฏู ุจุฑุง ุณุฑูุฑ):
Bash

nohup jupyter notebook &

ุง ุงฺฏุฑ ุงุฒ JupyterLab ุงุณุชูุงุฏู ูโฺฉูุฏ:
Bash

nohup jupyter lab &

    ูฺฉุชู: ุงฺฏุฑ ุชูุธูุงุช ุฏุณุชุฑุณ ุงุฒ ุฑุงู ุฏูุฑ (ูุงููุฏ c.ServerApp.ip = '0.0.0.0') ุฑุง ูุจูุงู ุฏุฑ ูุงู ูพฺฉุฑุจูุฏ ุงุนูุงู ฺฉุฑุฏูโุงุฏุ ุณุฑูุฑ ุขูุงุฏู ุงุชุตุงู ุงุฒ ุฑุงู ุฏูุฑ ุงุณุช.

ต. ุงุชุตุงู ุงุฒ ุณุณุชู ูุญู

ูุงููุฏ ูุจูุ ุจุฑุง ุงุชุตุงู ุงููุ ุจุงุฏ ุงุฒ SSH Tunneling ุงุณุชูุงุฏู ฺฉูุฏ:
Bash

ssh -N -f -L 8888:localhost:8888 username@remote_server_ip

ุญุงูุง ูโุชูุงูุฏ ุจุง ุจุงุฒ ฺฉุฑุฏู http://localhost:8888 ุฏุฑ ูุฑูุฑฺฏุฑ ุณุณุชู ูุญู ุฎูุฏุ ุจู ุณุฑูุฑ Jupyter ฺฉู ุฏุฑูู ูุญุท ูุฌุงุฒ ุดูุง ุฑู ุณุฑูุฑ ุฑุงู ุฏูุฑ ุงุฌุฑุง ุดุฏู ุงุณุชุ ุฏุณุชุฑุณ ูพุฏุง ฺฉูุฏ.

ูุนุงู ูุดุฏ ุฎูุฏู ูุนุงู ู ฺฉูู torch ุฑู ู ุชููู ุชู ูพุงุชูู ูุนููู ุจุจูู ูู ุชู ููุช ุจูฺฉ torch ุฑู ูุฏุงุฑู ุนู venv ูุนุงู ูุดุฏู

ุดูุง ุจู ุฏุฑุณุช ูุดฺฉู ุฑุง ุชุดุฎุต ุฏุงุฏูโุงุฏ: ููุช ูพฺฉุฌ torch ุฏุฑ ูุญุท Jupyter Notebook (ฺฉู ุฏุฑ ูุญุท ูุฌุงุฒ ุงุฌุฑุง ุดุฏู) ุฏุฑ ุฏุณุชุฑุณ ูุณุชุ ุจู ุงู ูุนู ุงุณุช ฺฉู Jupyter ุงุฒ ฺฉุฑูู (Kernel) ูุฑุจูุท ุจู ูุญุท ูุฌุงุฒ ุดูุง ุงุณุชูุงุฏู ููโฺฉูุฏ.

ุฏุณุชูุฑ source /home/dmls/pytorch/bin/activate ูุญุท ูุฌุงุฒ ุฑุง ุฏุฑ ุชุฑููุงู ูุนู ุดูุง ูุนุงู ูโฺฉูุฏุ ุงูุง ููุช ุดูุง jupyter notebook ุฑุง ุงุฌุฑุง ูโฺฉูุฏุ ูุฒููุงู ุงุฒ ูพุงุชูู ู ฺฉุฑูู ุขู ูุญุท ูุฌุงุฒ ุงุณุชูุงุฏู ููโฺฉูุฏ.

ุจุฑุง ุงูฺฉู Jupyter ุงุฒ ูุญุท ูุฌุงุฒ ุดูุง ุงุณุชูุงุฏู ฺฉูุฏุ ุจุงุฏ ฺฉุฑูู ูุญุท ูุฌุงุฒ ุฑุง ุจู Jupyter ูุนุฑู ฺฉูุฏ.

๐๏ธ ูุฑุงุญู ูุนุฑู ูุญุท ูุฌุงุฒ ุจู Jupyter

ุจุฑุง ุฑูุน ุงู ูุดฺฉูุ ุดูุง ุจุงุฏ ูพฺฉุฌ ipykernel ุฑุง ุฏุฑ ูุญุท ูุฌุงุฒ ุฎูุฏ ูุตุจ ฺฉุฑุฏู ู ุขู ูุญุท ุฑุง ุจู ุนููุงู ฺฉ ฺฉุฑูู ูุงุจู ุงูุชุฎุงุจ ุจู Jupyter ูุนุฑู ฺฉูุฏ.

ฑ. ูุนุงูโุณุงุฒ ูุญุท ูุฌุงุฒ

ุงุจุชุฏุงุ ููุงูุทูุฑ ฺฉู ูุจูุงู ุงูุฌุงู ุฏุงุฏุฏุ ูุญุท ูุฌุงุฒ ุฑุง ุฏุฑ ุชุฑููุงู ุณุฑูุฑ ุฑุงู ุฏูุฑ ูุนุงู ฺฉูุฏ:
Bash

source /home/dmls/pytorch/bin/activate

ฒ. ูุตุจ ipykernel

ุฏุฑ ุญุงู ฺฉู ูุญุท ูุฌุงุฒ ูุนุงู ุงุณุช (ูุงู ุขู ุจุงุฏ ุฏุฑ ูพุฑุงููพุช ุชุฑููุงู ุฏุฏู ุดูุฏ)ุ ูพฺฉุฌ ipykernel ุฑุง ูุตุจ ฺฉูุฏ:
Bash

pip install ipykernel

ณ. ูุนุฑู ฺฉุฑูู ุจู Jupyter

ุญุงูุง ุจุงุฏ ฺฉ ฺฉุฑูู ุฌุฏุฏ ุจู Jupyter ุงุถุงูู ฺฉูุฏ ฺฉู ุจู ูพุงุชูู ูุนุงู ุฏุฑ ูุญุท ูุฌุงุฒ ุดูุง ุงุดุงุฑู ฺฉูุฏ. ุงุฒ ุฏุณุชูุฑ ipykernel install ุงุณุชูุงุฏู ฺฉูุฏ:
Bash

python -m ipykernel install --user --name=pytorch_venv --display-name="PyTorch Venv"

ูพุงุฑุงูุชุฑ	ุชูุถุญุงุช
--user	ฺฉุฑูู ุฑุง ููุท ุจุฑุง ฺฉุงุฑุจุฑ ูุนู ูุตุจ ูโฺฉูุฏ.
--name	ูุงู ุฏุงุฎู (Internal) ฺฉุฑูู.
--display-name	ูุงู ฺฉู ุฏุฑ ููู ุงูุชุฎุงุจ ฺฉุฑูู ุฏุฑ ุฑุงุจุท ฺฉุงุฑุจุฑ Jupyter ููุงุด ุฏุงุฏู ูโุดูุฏ.

ด. ุงุฌุฑุง ุณุฑูุฑ ู ุงูุชุฎุงุจ ฺฉุฑูู

    ุจุง ุฏุณุชูุฑ ูุจู ุณุฑูุฑ Jupyter ุฑุง ุงุฌุฑุง ฺฉูุฏ:
    Bash

    nohup jupyter notebook &

    ุงุฒ ุณุณุชู ูุญู ุฎูุฏ ุจู http://localhost:8888 ูุชุตู ุดูุฏ.

    ุฏุฑ ุฑุงุจุท ฺฉุงุฑุจุฑ Jupyter:

        ุงฺฏุฑ ฺฉ ููุช ุจูฺฉ ุฌุฏุฏ ุงุฌุงุฏ ูโฺฉูุฏ (New)ุ ูโุชูุงูุฏ PyTorch Venv ุฑุง ุงุฒ ูุณุช ฺฉุฑููโูุง ุงูุชุฎุงุจ ฺฉูุฏ.

        ุงฺฏุฑ ฺฉ ููุช ุจูฺฉ ููุฌูุฏ ุฑุง ุจุงุฒ ูโฺฉูุฏุ ูโุชูุงูุฏ ุงุฒ ููู Kernel > Change kernelุ ฺฉุฑูู PyTorch Venv ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ.

ุญุงูุงุ ูุฑ ููุช ุจูฺฉ ฺฉู ุงุฒ ุงู ฺฉุฑูู ุงุณุชูุงุฏู ฺฉูุฏุ ุจู ุชูุงู ูพฺฉุฌโูุง ูุงููุฏ torch ฺฉู ุฏุฑ ูุญุท ูุฌุงุฒ /home/dmls/pytorch/ ูุตุจ ุดุฏูโุงูุฏุ ุฏุณุชุฑุณ ุฎูุงูุฏ ุฏุงุดุช.

ุขุง ูุงุฒ ุจู ฺฉูฺฉ ุจุดุชุฑ ุฏุฑ ููุฑุฏ SSH Tunneling ุง ุฑุงูโุงูุฏุงุฒ ุณุฑูุฑ ุฏุงุฑุฏุ

PM

ู venv ุฏุงุฑู ฺฉู ุชูุด torch ูุตุจู ููุช ูุนุงูุด ู ฺฉูู ู ูพุงุชูู ุฑู ุงุฌุฑุง ู ฺฉูู ู ุชููู ุจุง ููููุช torch ุฑู ุงููพูุฑุช ฺฉูู ุงูุง

ููุช ฺฉู jupiter server ุฑู ุงุฌุฑุง ู ฺฉูู ู ุงุฒ ุทุฑู ุงูู torch ุฑู ุงููพูุฑุช ู ฺฉูู ู ฺฏู ูพุฏุง ูู ฺฉูู |


ฺฺฉุงุฑ ฺฉููุ

ุงู ูุดฺฉู ูุนูููุงู ุจู ุงู ุฏูู ูพุด ูโุขุฏ ฺฉู ฺููพุชุฑ (Jupyter) ุจูโุทูุฑ ุฎูุฏฺฉุงุฑ ุงุฒ ูุญุท ูุฌุงุฒ (venv) ุดูุง ุงุณุชูุงุฏู ููโฺฉูุฏุ ุจูฺฉู ุงุฒ ฺฉุฑูู (Kernel) ูพุดโูุฑุถ ุง ฺฏููุจุงู ุณุณุชู ุงุณุชูุงุฏู ูโฺฉูุฏ. ุญุช ุงฺฏุฑ venv ูุนุงู ุจุงุดุฏุ ฺููพุชุฑ ุจุงุฏ ุตุฑุงุญุชุงู ุขู ูุญุท ุฑุง ุจู ุนููุงู ฺฉ ฺฉุฑูู ุจุดูุงุณุฏ.

ุจุฑุง ุญู ุงู ูุดฺฉูุ ุจุงุฏ ูุญุท ูุฌุงุฒ ุฎูุฏ ุฑุง ุจู ูุณุช ฺฉุฑููโูุง ฺููพุชุฑ ุงุถุงูู ฺฉูุฏ. ูุฑุงุญู ุฒุฑ ุฑุง ุงูุฌุงู ุฏูุฏ:

ุฑุงู ุญู ูุฏูโุจูโูุฏู

ฑ. ูุญุท ูุฌุงุฒ (venv) ุฎูุฏ ุฑุง ูุนุงู ฺฉูุฏ: ูุทูุฆู ุดูุฏ ฺฉู ุฏุฑ ุชุฑููุงูุ ูุญุท ูุฌุงุฒ ุดูุง ูุนุงู ุงุณุช.

ฒ. ูพฺฉุฌ ipykernel ุฑุง ูุตุจ ฺฉูุฏ: ุงู ูพฺฉุฌ ุฑุงุจุท ุจู ูพุงุชูู ู ฺููพุชุฑ ุงุณุช. ุฏุณุชูุฑ ุฒุฑ ุฑุง ุฏุฑ ุญุงู ฺฉู venv ูุนุงู ุงุณุช ุงุฌุฑุง ฺฉูุฏ:
Bash

pip install ipykernel

ณ. ูุญุท ูุฌุงุฒ ุฑุง ุจู ุนููุงู ฺฉ ฺฉุฑูู ุจู ฺููพุชุฑ ูุนุฑู ฺฉูุฏ: ุฏุณุชูุฑ ุฒุฑ ุฑุง ุงุฌุฑุง ฺฉูุฏ. ูโุชูุงูุฏ ุจู ุฌุง my_env_name ูุงู ุฏูุฎูุงู ฺฉู ูโุฎูุงูุฏ ุฏุฑ ูุณุช ฺฉุฑููโูุง ฺููพุชุฑ ุจุจูุฏ ุฑุง ุจููุณุฏ.
Bash

python -m ipykernel install --user --name=my_torch_env --display-name "Python (Torch Env)"

    --name: ูุงู ูู ฺฉุฑูู (ุจุฏูู ูุงุตูู).

    --display-name: ูุงู ฺฉู ุฏุฑ ููู ฺููพุชุฑ ููุงุด ุฏุงุฏู ูโุดูุฏ.

ด. ฺููพุชุฑ ุฑุง ุฏูุจุงุฑู ุงุฌุฑุง ู ฺฉุฑูู ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ: ุญุงูุง ฺููพุชุฑ ููุชโุจูฺฉ ุง ูุจ ุฑุง ุงุฌุฑุง ฺฉูุฏ:

    ูุงู ููุชโุจูฺฉ (.ipynb) ุฎูุฏ ุฑุง ุจุงุฒ ฺฉูุฏ.

    ุงุฒ ููู ุจุงูุง ุจู ูุณุฑ Kernel > Change kernel ุจุฑูุฏ.

    ฺฏุฒูู "Python (Torch Env)" (ุง ูุฑ ูุงู ฺฉู ุฏุฑ ูุฑุญูู ูุจู ุงูุชุฎุงุจ ฺฉุฑุฏุฏ) ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ.

ุจุฑุฑุณ ููุง

ุจุฑุง ุงูฺฉู ูุทูุฆู ุดูุฏ ููุชโุจูฺฉ ุดูุง ูุงูุนุงู ุงุฒ ูพุงุชููู ุฏุงุฎู venv ุงุณุชูุงุฏู ูโฺฉูุฏุ ฺฉุฏ ุฒุฑ ุฑุง ุฏุฑ ฺฉ ุณููู ุงุฌุฑุง ฺฉูุฏ. ูุณุฑ ฺุงูพ ุดุฏู ุจุงุฏ ุจู ูพูุดู venv ุดูุง ุงุดุงุฑู ุฏุงุดุชู ุจุงุดุฏ:
Python

import sys
print(sys.executable)

ุงฺฏุฑ ูุณุฑ ุฏุฑุณุช ุจูุฏุ ุญุงูุง ุงููพูุฑุช ฺฉุฑุฏู torch ุจุฏูู ูุดฺฉู ุงูุฌุงู ูโุดูุฏ:
Python

import torch
print(torch.__version__)

ุขุง ูโุฎูุงูุฏ ูุญูู ุญุฐู ฺฉุฑุฏู ุงู ฺฉุฑูู ุฑุง ุจุนุฏ ุงุฒ ุงุชูุงู ฺฉุงุฑุชุงู ุจุฏุงูุฏุ

ุชู .jupiter

ูุงู ฺฉูุงูฺฏ ุฏุงุฑู ุชู ุงูู ู ุชูู ฺุฒ ุงุถุงูู ฺฉูู ฺฉู ุงู ูุดฺฉู ุญู ุจุดูุ

ูพุงุณุฎ ฺฉูุชุงู ุงู ุงุณุช: ุฎุฑุ ุงู ุฑูุด ุชูุตู ููโุดูุฏ ู ูุนูููุงู ฺฉุงุฑ ููโฺฉูุฏ.

ูุงู ฺฉุงููฺฏ ุงุตู ฺููพุชุฑ (jupyter_notebook_config.py) ุจุฑุง ุชูุธูุงุช ุณุฑูุฑ (ูุซู ูพูุฑุชุ ูพุณูุฑุฏุ ูููุฏุฑ ูพุดโูุฑุถ) ุงุณุชุ ูู ุจุฑุง ุชุนู ููุณุฑ ูพุงุชูู (Python Interpreter) ฺฉู ฺฉุฏูุง ุฑุง ุงุฌุฑุง ูโฺฉูุฏ.

ฺููพุชุฑ ุจุฑุง ุงุฌุฑุง ฺฉุฏูุง ุงุฒ ููููู ุจู ูุงู Kernel ุงุณุชูุงุฏู ูโฺฉูุฏ. ุชูุธูุงุช ูุฑ ฺฉุฑูู ุฏุฑ ูุงู ุฌุฏุงฺฏุงููโุง ุจู ูุงู kernel.json ุฐุฎุฑู ูโุดูุฏุ ูู ุฏุฑ ฺฉุงููฺฏ ุงุตู.

ุงฺฏุฑ ูโุฎูุงูุฏ ุจุฏูู ุงุฌุฑุง ุฏุณุชูุฑ ipykernel install ู ุจู ุตูุฑุช ุฏุณุช (ุงุฒ ุทุฑู ุณุงุฎุช ูุงู) ุงู ฺฉุงุฑ ุฑุง ุงูุฌุงู ุฏูุฏุ ุจุงุฏ ฺฉ ูุงู kernel.json ุจุณุงุฒุฏ.

ุฑูุด ุฏุณุช (ุจุฏูู ุฏุณุชูุฑ ูุตุจ):

ุจู ุฌุง ูุฑุงุด ฺฉุงููฺฏ ุงุตูุ ุจุงุฏ ฺฉ ูพูุดู ู ูุงู ุจุฑุง ฺฉุฑูู ุฎูุฏ ุจุณุงุฒุฏ:

ฑ. ูุณุฑ ฺฉุฑููโูุง ุฑุง ูพุฏุง ฺฉูุฏ: ุฏุฑ ูููฺฉุณ/ูฺฉ ูุนูููุงู ุฏุฑ ูุณุฑ ุฒุฑ ุงุณุช: ~/.local/share/jupyter/kernels/ (ุงฺฏุฑ ูุฌูุฏ ูุฏุงุฑุฏ ุจุณุงุฒุฏ).

ฒ. ฺฉ ูพูุดู ุฌุฏุฏ ุจุณุงุฒุฏ: ูุงู ุขู ุฑุง ูุซูุงู my_torch_project ุจฺฏุฐุงุฑุฏ.

ณ. ูุงู kernel.json ุฑุง ุจุณุงุฒุฏ: ุฏุงุฎู ุขู ูพูุดูุ ฺฉ ูุงู ูุชู ุจู ูุงู kernel.json ุจุณุงุฒุฏ ู ูุญุชูุง ุฒุฑ ุฑุง ุฏุฑ ุขู ูุฑุงุฑ ุฏูุฏ (ูุณุฑ python ุฑุง ุจุง ูุณุฑ venv ุฎูุฏ ุฌุงฺฏุฒู ฺฉูุฏ):
JSON

{
 "argv": [
  "/path/to/your/venv/bin/python",
  "-m",
  "ipykernel_launcher",
  "-f",
  "{connection_file}"
 ],
 "display_name": "Python (My Manual Torch)",
 "language": "python",
 "metadata": {
  "debugger": true
 }
}

ูฺฉุชู ููู: ุจุฑุง ุงูฺฉู ุงู ุฑูุด ฺฉุงุฑ ฺฉูุฏุ ุจุงุฒ ูู ุจุงุฏ ูพฺฉุฌ ipykernel ุฏุงุฎู venv ูุตุจ ุจุงุดุฏ.

ฺุฑุง ูุจุงุฏ ฺฉุงููฺฏ ุงุตู ุฑุง ุชุบุฑ ุฏุงุฏุ

ุชูุงุด ุจุฑุง ุชุบุฑ sys.path ุง ูุชุบุฑูุง ูุญุท ุฏุฑ jupyter_notebook_config.py ุจุฑุง ุดูุงุณุงูุฏู torch ูุนูููุงู ุจุงุนุซ ุชุฏุงุฎู ูุณุฎูโูุง ูพุงุชูู (System Python vs Venv Python) ูโุดูุฏ ู ุฎุทุงูุง ุนุฌุจ (ูุซู dll load failed ุง symbol not found) ุงุฌุงุฏ ูโฺฉูุฏ.

ุขุง ูโุฎูุงูุฏ ุจุฏุงูุฏ ฺุทูุฑ ูุณุช ุชูุงู ฺฉุฑููโูุง ฺฉู ุชุง ุงูุงู ูุตุจ ุดุฏูโุงูุฏ ุฑุง ุจุจูุฏ ู ููุงุฑุฏ ุงุถุงู ุฑุง ูพุงฺฉ ฺฉูุฏุ

 PM

ู ูพูุฑุช ุฑู ุจุง ssh ูุฑูุงุฑุฏ ฺฉุฑุฏู ุงูุงู ู ุฎูุงู ุจุจูุฏูุด ุจุงุฏ ฺู ฺฉููุ

ุจุณุชู Port Forwarding ฺฉู ุจุง SSH ุงุฌุงุฏ ุดุฏูุ ุจุณุชฺฏ ุจู ุงู ุฏุงุฑู ฺฉู ุงุฒ ฺู ููุน ุงุฒ ููุฑูุงุฑุฏูฺฏ ุงุณุชูุงุฏู ฺฉุฑุฏุฏ ู ุขุง ุฌูุณู SSH (SSH Session) ูููุฒ ุจุงุฒู ุง ูู.

๐ ุฑูุด ฺฉู ุจุณุชู Port Forwarding

ฑ. ุงฺฏุฑ ุฌูุณู SSH ูููุฒ ุจุงุฒ ุงุณุช (Recommended)

ุณุงุฏูโุชุฑู ุฑุงู ุจุฑุง ุจุณุชู Local ุง Remote Port Forwarding ุงูู ฺฉู ุงุชุตุงู SSH ุงุตู ุฑู ุจุจูุฏุฏ.

    ุงฺฏุฑ ุงุฒ ุชุฑููุงู ุงุณุชูุงุฏู ูโฺฉูุฏ:

        ฺฉุงูู ุฏุณุชูุฑ exit ุฑู ุชุงูพ ฺฉูุฏ ู Enter ุจุฒูุฏ.

        ุง ฺฉูุฏูุง Ctrl + D ุฑู ูุดุงุฑ ุจุฏุฏ.

        ุจุง ุจุณุชู ูพูุฌุฑู ุชุฑููุงูุ ุงุชุตุงู SSH ู ุจู ุฏูุจุงู ุงูู Port Forwarding ูู ุจุณุชู ูโุดู.

ฒ. ุงฺฏุฑ ุฌูุณู SSH ุฑู ุฏุฑ ูพุณโุฒููู (Background) ุงุฌุฑุง ฺฉุฑุฏุฏ

ุงฺฏุฑ ููุฑูุงุฑุฏูฺฏ ุฑู ุจุง ุงุณุชูุงุฏู ุงุฒ ฺฏุฒููโูุง ูุซู & (ุจุฑุง ุงุฌุฑุง ุฏุฑ ูพุณโุฒููู) ุง -fN (ุจุฑุง ุงุฌุฑุง ุฏุฑ ูพุณโุฒููู ู ุนุฏู ุงุฌุฑุง ุฏุณุชูุฑ) ุดุฑูุน ฺฉุฑุฏู ุจุงุดุฏุ ุจุงุฏ ูพุฑูุณุณ (Process) ูุฑุจูุท ุจู SSH ุฑู ูพุฏุง ฺฉุฑุฏู ู ุงูู ุฑู ูุชููู ฺฉูุฏ.

    ูพุฏุง ฺฉุฑุฏู Process ID ุง PID: ุจุฑุง ูพุฏุง ฺฉุฑุฏู PID ูพุฑูุณุณ SSH ฺฉู ููุฑูุงุฑุฏูฺฏ ุฑู ุงุฌุงุฏ ฺฉุฑุฏูุ ูโุชููุฏ ุงุฒ ุฏุณุชูุฑ ps ุง pgrep ุงุณุชูุงุฏู ฺฉูุฏ.
    Bash

ps aux | grep ssh

ุง ุงฺฏุฑ ููุท ุฏูุจุงู ุงุชุตุงูุงุช SSH ูุณุชุฏ:
Bash

pgrep ssh

ุฎุฑูุฌ ุดุจู ุจู ุฒุฑ ุฎูุงูุฏ ุจูุฏ:

user     12345  0.0  0.1 123456 7890 ?        Ss   14:00   0:00 /usr/bin/ssh -L 8080:localhost:80 user@remote_host

ุฏุฑ ุงู ูุซุงูุ 12345 ูููู PID ูุณุช.

ูุชููู ฺฉุฑุฏู ูพุฑูุณุณ: ุญุงูุง ุจุง ุงุณุชูุงุฏู ุงุฒ ุฏุณุชูุฑ kill ู PID ฺฉู ูพุฏุง ฺฉุฑุฏุฏุ ูพุฑูุณุณ ุฑู ูุชููู ฺฉูุฏ.
Bash

    kill 12345

        ุชูุฌู: ุงฺฏู ุฏุณุชูุฑ kill ุฌูุงุจ ูุฏุงุฏุ ุงุฒ kill -9 12345 ุงุณุชูุงุฏู ฺฉูุฏ ฺฉู ูพุฑูุณุณ ุฑู ุจูโุตูุฑุช ุงุฌุจุงุฑ ูุชููู ูโฺฉูู.

ณ. ุงฺฏุฑ ุงุฒ Permanent Tunnel ุง SSH Multiplexing ุงุณุชูุงุฏู ูโฺฉูุฏ

ุงฺฏุฑ ฺฉ SSH Tunnel ุฏุงุฆู ุฑู ุจุง ุงุจุฒุงุฑูุง ูุซู autossh ุง ุจุง ุงุณุชูุงุฏู ุงุฒ ุงูฺฉุงูุงุช SSH Multiplexing (ูุซู -M ู -S) ุงุฌุงุฏ ฺฉุฑุฏุฏุ ุดุงุฏ ูุงุฒู ุจุงุดู ุฏุณุชูุฑุงุช ูุชูุงุณุจ ุจุง ุงูู ุงุจุฒุงุฑ ุง ูพฺฉุฑุจูุฏ ุฑู ุจุฑุง ูุทุน ุงุชุตุงู ุงุฌุฑุง ฺฉูุฏ.

ุจู ุนููุงู ูุซุงูุ ุจุฑุง ูุทุน ฺฉุฑุฏู ุงุชุตุงู SSH ฺฉู ุจุง autossh ุฏุฑ ูพุณโุฒููู ุงุฌุฑุง ุดุฏูุ ุจุงุฏ ูพุฑูุณุณ autossh ุฑู ูพุฏุง ู Kill ฺฉูุฏ.

โ๏ธ ูฺฉุชู ููู

ุจุฑุง ุงูฺฉู ูุทูุฆู ุจุดุฏ Port Forwarding ฺฉุงููุงู ุจุณุชู ุดุฏูุ ูโุชููุฏ ุจุฑุฑุณ ฺฉูุฏ ฺฉู ุขุง ูพูุฑุช ูุญู ูููุฒ ุฏุฑ ุญุงู ฺฏูุด ุฏุงุฏู (Listening) ุงุณุช ุง ูู.

    ุจุฑุง ุจุฑุฑุณ Port ูุญู (ูุซูุงู ูพูุฑุช 8080):
    Bash

sudo netstat -tuln | grep 8080

ุง ุฏุฑ ุณุณุชูโูุง ุฌุฏุฏุชุฑ:
Bash

    sudo ss -tuln | grep 8080

ุงฺฏุฑ ุฎุฑูุฌโุง ูุฏุงุดุชุฏุ ุนู ูพูุฑุช ุฏฺฏู ุจุงุฒ ูุณุช ู ููุฑูุงุฑุฏูฺฏ ุจุง ููููุช ุจุณุชู ุดุฏู ุงุณุช.

ุขุง ูโุฏููุฏ ฺฉู ุงุฒ ฺู ููุน Port Forwarding (Local, Remote, Dynamic) ุงุณุชูุงุฏู ฺฉุฑุฏุฏ ู ุขุง ุงูุงู ุงุชุตุงู SSH ุดูุง ุจุงุฒู ุง ุฏุฑ ูพุณโุฒููู ุงุณุชุ

PM

ฺุทูุฑ ุฏุฑ ุฌููพุชุฑ ููุช ุจูฺฉ ฺฉ venv ูุนุงู ฺฉููุ

ุจุฑุง ุงุณุชูุงุฏู ุงุฒ ฺฉ ูุญุท ูุฌุงุฒ (Virtual Environment) ุฏุฑ ฺููพุชุฑ ููุชโุจูฺฉุ ุจุฑุฎูุงู ุชุฑููุงูุ ุขู ุฑุง ยซูุนุงูยป ููโฺฉูุฏุ ุจูฺฉู ุจุงุฏ ุขู ูุญุท ุฑุง ุจู ุนููุงู ฺฉ Kernel ุจู ฺููพุชุฑ ูุนุฑู ฺฉูุฏ.

ุฏุฑ ุงูุฌุง ูุฑุงุญู ุงูุฌุงู ุงู ฺฉุงุฑ ุจู ุตูุฑุช ฺฏุงูโุจูโฺฏุงู ุขูุฏู ุงุณุช. ุชูุฌู ุฏุงุดุชู ุจุงุดุฏ ฺฉู ุงู ุฏุณุชูุฑุงุช ุจุงุฏ ุฏุฑ ุชุฑููุงู (CMD ุง Terminal) ุณุณุชูโุนุงูู ุงุฌุฑุง ุดููุฏุ ูู ุฏุงุฎู ุณูููโูุง ููุชโุจูฺฉ.

ฑ. ุณุงุฎุช ู ูุนุงูโุณุงุฒ ูุญุท ูุฌุงุฒ (ุงฺฏุฑ ูููุฒ ูุณุงุฎุชูโุงุฏ)

ุงุจุชุฏุง ูุญุท ูุฌุงุฒ ุฎูุฏ ุฑุง ุฏุฑ ูุณุฑ ุฏูุฎูุงู ุจุณุงุฒุฏ ู ูุนุงู ฺฉูุฏ.

ุณุงุฎุชู (Windows/Mac/Linux):
Bash

python -m venv my_venv

ูุนุงูโุณุงุฒ ุฏุฑ ููุฏูุฒ:
DOS

my_venv\Scripts\activate

ูุนุงูโุณุงุฒ ุฏุฑ ูฺฉ/ูููฺฉุณ:
Bash

source my_venv/bin/activate

ฒ. ูุตุจ ูพฺฉุฌ ipykernel

ุจุฑุง ุงูฺฉู ฺููพุชุฑ ุจุชูุงูุฏ ุจุง ุงู ูุญุท ูุฌุงุฒ ุงุฑุชุจุงุท ุจุฑูุฑุงุฑ ฺฉูุฏุ ุจุงุฏ ูพฺฉุฌ ipykernel ุฑุง ุฏุงุฎู ููู ูุญุท ูุนุงู ุดุฏู ูุตุจ ฺฉูุฏ.
Bash

pip install ipykernel

ณ. ูุนุฑู ูุญุท ูุฌุงุฒ ุจู ฺููพุชุฑ (ุงุถุงูู ฺฉุฑุฏู ฺฉุฑูู)

ุญุงูุง ุจุงุฏ ุงู ูุญุท ุฑุง ุจู ูุณุช ฺฉุฑููโูุง ฺููพุชุฑ ุงุถุงูู ฺฉูุฏ. ุฏุฑ ุฏุณุชูุฑ ุฒุฑุ my_venv ูุงู ูุญุท ุดูุงุณุช ู "Python (My Venv)" ูุงู ุงุณุช ฺฉู ุฏุฑ ููู ฺููพุชุฑ ุฎูุงูุฏ ุฏุฏ.
Bash

python -m ipykernel install --user --name=my_venv --display-name "Python (My Venv)"

ด. ุงูุชุฎุงุจ ฺฉุฑูู ุฏุฑ ฺููพุชุฑ ููุชโุจูฺฉ

ุญุงูุง ูโุชูุงูุฏ ฺููพุชุฑ ููุชโุจูฺฉ ุฑุง ุงุฌุฑุง ฺฉูุฏ (ูุฑู ููโฺฉูุฏ ุงุฒ ูุญุท ุงุตู ุณุณุชู ุจุงุดุฏ ุง ูุญุท ูุฌุงุฒ):

    ฺููพุชุฑ ููุชโุจูฺฉ ุฑุง ุจุงุฒ ฺฉูุฏ.

    ฺฉ ููุชโุจูฺฉ ุฌุฏุฏ ุจุณุงุฒุฏ ุง ฺฉ ุฑุง ุจุงุฒ ฺฉูุฏ.

    ุงุฒ ููู ุจุงูุง ุจู ูุณุฑ ุฒุฑ ุจุฑูุฏ: Kernel > Change kernel

    ฺฏุฒููโุง ฺฉู ุฏุฑ ูุฑุญูู ณ ูุงูโฺฏุฐุงุฑ ฺฉุฑุฏุฏ (ูุซูุงู Python (My Venv)) ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ.

ุจุฑุฑุณ ุตุญุช ุงูุฌุงู ฺฉุงุฑ

ุจุฑุง ุงูฺฉู ูุทูุฆู ุดูุฏ ููุชโุจูฺฉ ุดูุง ุฏุฑ ุญุงู ุงุณุชูุงุฏู ุงุฒ ูพุงุชููู ูุญุท ูุฌุงุฒ ุงุณุชุ ฺฉุฏ ุฒุฑ ุฑุง ุฏุฑ ฺฉ ุณููู ููุชโุจูฺฉ ุงุฌุฑุง ฺฉูุฏ:
Python

import sys
print(sys.executable)

ุฎุฑูุฌ ุจุงุฏ ูุณุฑ ูพุงุชููู ุฏุงุฎู ูพูุดู my_venv ุจุงุดุฏ.

ุขุง ูโุฎูุงูุฏ ุฑูุด ุญุฐู ฺฉุฑุฏู ุงู ฺฉุฑูู ุงุฒ ูุณุช ฺููพุชุฑ ุฑุง ูู ุชูุถุญ ุฏููุ

$pip install ipykernel

ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/home/dmls/pytorch/lib/python3.13/site-packages/pure_eval'

Check the permissions.

ุงู ุฎุทุง (Permission denied) ุจู ุงู ูุนูุงุณุช ฺฉู ฺฉุงุฑุจุฑ ฺฉู ุจุง ุขู ูุงฺฏู ฺฉุฑุฏูโุงุฏุ ุงุฌุงุฒู ููุดุชู ูุงู ุฏุฑ ุขู ูุณุฑ ุฎุงุต (/home/dmls/pytorch/...) ุฑุง ูุฏุงุฑุฏ. ุงู ูุนูููุงู ุจู ุฏู ุฏูู ุฑุฎ ูโุฏูุฏ:

ฑ. ูุญุท ูุฌุงุฒ (venv) ูุนุงู ูุณุช ู ุณุณุชู ุชูุงุด ูโฺฉูุฏ ูพฺฉุฌ ุฑุง ุฑู ูพุงุชูู ุงุตู (Global) ูุตุจ ฺฉูุฏ. ฒ. ูพูุดูโ ูุญุท ูุฌุงุฒ ุชูุณุท ฺฉุงุฑุจุฑ ุฏฺฏุฑ (ูุซูุงู root) ุณุงุฎุชู ุดุฏู ุงุณุช.

ุฏุฑ ุงูุฌุง ณ ุฑุงู ุญู ุจุฑุง ุฑูุน ุงู ูุดฺฉู ูุฌูุฏ ุฏุงุฑุฏ:

ุฑุงู ุญู ฑ: ูุทูุฆู ุดูุฏ venv ูุนุงู ุงุณุช (ูพุดููุงุฏ)

ุงฺฏุฑ ูุตุฏ ุฏุงุดุชุฏ ุงู ูพฺฉุฌ ุฑุง ุฏุฑ ูุญุท ูุฌุงุฒ ูุตุจ ฺฉูุฏุ ุงุญุชูุงูุงู ูุฑุงููุด ฺฉุฑุฏูโุงุฏ ุขู ุฑุง ูุนุงู ฺฉูุฏ. ุจู ูุณุฑ ุฏุฑ ุฎุทุง ูฺฏุงู ฺฉูุฏ: /home/dmls/pytorch/.... ุงฺฏุฑ ุงู ูุณุฑู ูุญุท ูุฌุงุฒ ุดูุง ูุณุชุ ุนู venv ูุนุงู ูุดุฏู ุงุณุช.

ุงุจุชุฏุง ูุญุท ุฑุง ูุนุงู ฺฉูุฏ ู ุณูพุณ ุฏูุจุงุฑู ุชูุงุด ฺฉูุฏ:
Bash

source my_venv/bin/activate
pip install ipykernel

(ูฺฉุชู: ููุช ูุญุท ูุนุงู ุดูุฏุ ุจุงุฏ ูุงู ุขู ุฑุง ุฏุฑ ุงุจุชุฏุง ุฎุท ูุฑูุงู ุฏุงุฎู ูพุฑุงูุชุฒ ุจุจูุฏ.)

ุฑุงู ุญู ฒ: ุงุณุชูุงุฏู ุงุฒ ููฺฏ --user

ุงฺฏุฑ ููโุฎูุงูุฏ ุงุฒ ูุญุท ูุฌุงุฒ ุงุณุชูุงุฏู ฺฉูุฏ ุง ูุญุท ูุฌุงุฒ ุดูุง ููู ูุณุฑ ุงุณุช ุงูุง ุฏุณุชุฑุณ ูุฏุงุฑุฏุ ูโุชูุงูุฏ ูพฺฉุฌ ุฑุง ููุท ุจุฑุง ฺฉุงุฑุจุฑ ุฌุงุฑ ูุตุจ ฺฉูุฏ. ุงู ฺฉุงุฑ ูุนูููุงู ูุดฺฉู ุฏุณุชุฑุณ ุฑุง ุญู ูโฺฉูุฏ:
Bash

pip install ipykernel --user

ุฑุงู ุญู ณ: ุงุตูุงุญ ูุงูฺฉุช ูุงูโูุง (ุงฺฏุฑ ูุงูฺฉ ุณุณุชู ูุณุชุฏ)

ุงฺฏุฑ ูุทูุฆู ูุณุชุฏ ฺฉู ุฏุฑ ูุณุฑ ุฏุฑุณุช (venv) ูุณุชุฏ ุงูุง ุจุงุฒ ูู ุฎุทุง ูโฺฏุฑุฏุ ููฺฉู ุงุณุช ุงู ูุญุท ุฑุง ุจุง ุฏุณุชูุฑ sudo ุณุงุฎุชู ุจุงุดุฏ. ุฏุฑ ุงู ุตูุฑุช ูุงูฺฉ ุขู root ุงุณุช ู ุดูุง ุฏุณุชุฑุณ ูุฏุงุฑุฏ.

ุจุฑุง ุชุบุฑ ูุงูฺฉุช ุขู ูพูุดู ุจู ฺฉุงุฑุจุฑ ุฎูุฏุชุงูุ ุฏุณุชูุฑ ุฒุฑ ุฑุง ุงุฌุฑุง ฺฉูุฏ (ุจู ุฌุง your_username ูุงู ฺฉุงุฑุจุฑ ุฎูุฏ ุฏุฑ ูููฺฉุณ ู ุจู ุฌุง path/to/venv ุขุฏุฑุณ ูุญุท ูุฌุงุฒ ุฑุง ุจููุณุฏ):
Bash

sudo chown -R $USER:$USER path/to/venv

ุณูพุณ ุฏูุจุงุฑู ุฏุณุชูุฑ pip install ุฑุง ุงุฌุฑุง ฺฉูุฏ.

ุขุง ูููู ุดุฏุฏ ูุตุจ ุฑุง ุงูุฌุงู ุฏูุฏ ุง ุฎุทุง ุฏฺฏุฑ ุฏุฑุงูุช ฺฉุฑุฏุฏุ

(base) vajhi@Sabalan1:~$ pip install ipykernel --user

Requirement already satisfied: ipykernel in /opt/anaconda3/lib/python3.13/site-packages (6.29.5)

Requirement already satisfied: comm>=0.1.1 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (0.2.1)

Requirement already satisfied: debugpy>=1.6.5 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (1.8.11)

Requirement already satisfied: ipython>=7.23.1 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (8.30.0)

Requirement already satisfied: jupyter-client>=6.1.12 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (8.6.3)

Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (5.7.2)

Requirement already satisfied: matplotlib-inline>=0.1 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (0.1.6)

Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (1.6.0)

Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (24.2)

Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (5.9.0)

Requirement already satisfied: pyzmq>=24 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (26.2.0)

Requirement already satisfied: tornado>=6.1 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (6.5.1)

Requirement already satisfied: traitlets>=5.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from ipykernel) (5.14.3)

Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)

Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)

Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)

Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)

Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)

Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.13/site-packages (from ipython>=7.23.1->ipykernel) (4.8.0)

Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.13/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.5)

Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/anaconda3/lib/python3.13/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)

Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.13/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)

Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.7)

Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.13/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)

Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)

Requirement already satisfied: executing in /opt/anaconda3/lib/python3.13/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)

Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.13/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (3.0.0)

Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.13/site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)

(base) vajhi@Sabalan1:~$ source my_venv/bin/activate^C

(base) vajhi@Sabalan1:~$ source /home/dmls/pytorch/bin/activate

(pytorch) (base) vajhi@Sabalan1:~$ python -m ipykernel install --user --name=my_venv --display-name "Python (My Venv)"

/home/dmls/pytorch/bin/python: No module named ipykernel

(pytorch) (base) vajhi@Sabalan1:~$ 

ูุดฺฉู ุงูุฌุงุณุช ฺฉู ุดูุง ูพฺฉุฌ ipykernel ุฑุง ุฏุฑ ูุญุท base (ุขูุงฺฉููุฏุง) ูุตุจ ฺฉุฑุฏุฏุ ุงูุง ุงูุงู ุฏุฑ ูุญุท ูุฌุงุฒ pytorch ูุณุชุฏ. ูุฑ ูุญุท ูุฌุงุฒ ุฌุฏุงฺฏุงูู ุงุณุช ู ุจุงุฏ ipykernel ุฑุง ุฏุงุฎู ููุงู ูุญุท ูุตุจ ฺฉูุฏ ุชุง ฺฉุงุฑ ฺฉูุฏ.

ุงุฒ ุขูุฌุง ฺฉู ุงูุงู ูุญุท (pytorch) ูุนุงู ุงุณุช (ุขู ุฑุง ุฏุฑ ูพุฑุงูุชุฒ ุณูุช ฺูพ ูโุจูุฏ)ุ ฺฉุงูุณุช ุฏุณุชูุฑุงุช ุฒุฑ ุฑุง ุจู ุชุฑุชุจ ุงุฌุฑุง ฺฉูุฏ:

ฑ. ูุตุจ ipykernel ุฏุฑ ูุญุท ูุนู

ุงุจุชุฏุง ูพฺฉุฌ ุฑุง ูุตุจ ฺฉูุฏ:
Bash

python -m pip install ipykernel

ฒ. ุซุจุช ฺฉุฑูู

ูพุณ ุงุฒ ูพุงุงู ูุตุจุ ุฏุณุชูุฑ ุซุจุช ุฑุง ูุฌุฏุฏุงู ุงุฌุฑุง ฺฉูุฏ (ูู ูุงู ุฑุง ุจู pytorch_env ุชุบุฑ ุฏุงุฏู ุชุง ุฏุฑ ูุณุช ฺููพุชุฑ ูุงุถุญโุชุฑ ุจุงุดุฏุ ุงูุง ูุฑ ูุงู ุจุฎูุงูุฏ ูโุชูุงูุฏ ุจฺฏุฐุงุฑุฏ):
Bash

python -m ipykernel install --user --name=pytorch_env --display-name "Python (Pytorch)"

ุญุงูุง ุงฺฏุฑ ุตูุญู ฺููพุชุฑ ููุชโุจูฺฉ ุฑุง ุฑูุฑุด ฺฉูุฏุ ฺฏุฒููโ Python (Pytorch) ุฏุฑ ูุณุช ฺฉุฑููโูุง ุธุงูุฑ ูโุดูุฏ.

ฺฉ ูฺฉุชู ููู: ุดูุง ุฏุฑ ุฎุท ูุฑูุงู ุนุจุงุฑุช (pytorch) (base) ุฑุง ูโุจูุฏ. ุงู ุนู ูุญุท pytorch ุฑู ูุญุท base ุณูุงุฑ ุดุฏู ุงุณุช. ุงฺฏุฑ ุจุง ุฏุณุชูุฑ ุจุงูุง ูุดฺฉู ุญู ูุดุฏุ ุจูุชุฑ ุงุณุช ุงุจุชุฏุง conda deactivate ฺฉูุฏ ุชุง (base) ุบุฑูุนุงู ุดูุฏ ู ููุท (pytorch) ุจุงู ุจูุงูุฏุ ู ุณูพุณ ูุฑุงุญู ุจุงูุง ุฑุง ุชฺฉุฑุงุฑ ฺฉูุฏ.

Collecting six>=1.5 (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel)

  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)

Collecting executing>=1.2.0 (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel)

  Using cached executing-2.2.1-py2.py3-none-any.whl.metadata (8.9 kB)

Collecting asttokens>=2.1.0 (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel)

  Using cached asttokens-3.0.1-py3-none-any.whl.metadata (4.9 kB)

Collecting pure-eval (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel)

  Using cached pure_eval-0.2.3-py3-none-any.whl.metadata (6.3 kB)

Using cached ipykernel-7.1.0-py3-none-any.whl (117 kB)

Using cached comm-0.2.3-py3-none-any.whl (7.3 kB)

Using cached debugpy-1.8.17-cp313-cp313-manylinux_2_34_x86_64.whl (4.3 MB)

Using cached ipython-9.7.0-py3-none-any.whl (618 kB)

Using cached prompt_toolkit-3.0.52-py3-none-any.whl (391 kB)

Using cached decorator-5.2.1-py3-none-any.whl (9.2 kB)

Using cached ipython_pygments_lexers-1.1.1-py3-none-any.whl (8.1 kB)

Using cached jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)

Using cached parso-0.8.5-py2.py3-none-any.whl (106 kB)

Using cached jupyter_client-8.6.3-py3-none-any.whl (106 kB)

Using cached jupyter_core-5.9.1-py3-none-any.whl (29 kB)

Using cached matplotlib_inline-0.2.1-py3-none-any.whl (9.5 kB)

Using cached nest_asyncio-1.6.0-py3-none-any.whl (5.2 kB)

Using cached packaging-25.0-py3-none-any.whl (66 kB)

Using cached pexpect-4.9.0-py2.py3-none-any.whl (63 kB)

Using cached platformdirs-4.5.0-py3-none-any.whl (18 kB)

Using cached psutil-7.1.3-cp36-abi3-manylinux2010_x86_64.manylinux_2_12_x86_64.manylinux_2_28_x86_64.whl (263 kB)

Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)

Using cached pygments-2.19.2-py3-none-any.whl (1.2 MB)

Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)

Using cached pyzmq-27.1.0-cp312-abi3-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (840 kB)

Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)

Using cached stack_data-0.6.3-py3-none-any.whl (24 kB)

Using cached asttokens-3.0.1-py3-none-any.whl (27 kB)

Using cached executing-2.2.1-py2.py3-none-any.whl (28 kB)

Using cached tornado-6.5.2-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)

Using cached traitlets-5.14.3-py3-none-any.whl (85 kB)

Using cached pure_eval-0.2.3-py3-none-any.whl (11 kB)

Using cached wcwidth-0.2.14-py2.py3-none-any.whl (37 kB)

Installing collected packages: pure-eval, ptyprocess, wcwidth, traitlets, tornado, six, pyzmq, pygments, psutil, platformdirs, pexpect, parso, packaging, nest-asyncio, executing, decorator, debugpy, comm, asttokens, stack_data, python-dateutil, prompt_toolkit, matplotlib-inline, jupyter-core, jedi, ipython-pygments-lexers, jupyter-client, ipython, ipykernel

ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/home/dmls/pytorch/lib/python3.13/site-packages/pure_eval'

Check the permissions.


ูุตุจ ููุดู ฺฉุงฺฉุงุณุงู

ูุดฺฉู ููฺูุงู ูุฌูุฒ ุฏุณุชุฑุณ (Permission) ุงุณุช.

ุฏูุช ฺฉูุฏ ฺฉู ูุณุฑ ูุตุจ /home/dmls/pytorch ุงุณุชุ ุงูุง ูุงู ฺฉุงุฑุจุฑ ุดูุง vajhi ุงุณุช. ุงู ุนู ุดูุง ุฏุงุฑุฏ ุชูุงุด ูโฺฉูุฏ ุฑู ูุงู ุจููุณุฏ ฺฉู ูุชุนูู ุจู ฺฉุงุฑุจุฑ ุฏฺฏุฑ (ุงุญุชูุงูุงู ฺฉุงุฑุจุฑ ุจู ูุงู dmls ุง root) ุงุณุช.

ุจุฑุง ุญู ุงู ูุดฺฉูุ ฺฉ ุงุฒ ุฏู ุฏุณุชูุฑ ุฒุฑ ุฑุง ุงุฌุฑุง ฺฉูุฏ:

ุฑุงู ุญู ุงูู: ุชุบุฑ ูุงูฺฉุช ูพูุดู (ูพุดููุงุฏ)

ุจุง ุงู ุฏุณุชูุฑุ ูุงูฺฉุช ฺฉู ุขู ูุญุท ูุฌุงุฒ ุฑุง ุจู ุฎูุฏุชุงู (ฺฉุงุฑุจุฑ ุฌุงุฑ) ููุชูู ูโฺฉูุฏ ุชุง ุฏฺฏุฑ ูุงุฒ ุจู ุฏุณุชุฑุณ ุฎุงุต ูุจุงุดุฏ:
Bash

sudo chown -R $USER /home/dmls/pytorch

(ูพุณ ุงุฒ ุงุฌุฑุง ุงู ุฏุณุชูุฑุ ุฏูุจุงุฑู ุฏุณุชูุฑ pip install ipykernel ุฑุง ุจุฒูุฏ. ุงุญุชูุงู ุฒุงุฏ ูุดฺฉู ุญู ูโุดูุฏ.)

ุฑุงู ุญู ุฏูู: ูุตุจ ุจุง ุฏุณุชุฑุณ ุงุฏูู (sudo)

ุงฺฏุฑ ููโุฎูุงูุฏ ูุงูฺฉุช ูุงูโูุง ุฑุง ุชุบุฑ ุฏูุฏุ ุจุงุฏ ุฏุณุชูุฑ ูุตุจ ุฑุง ุจุง sudo ุงุฌุฑุง ฺฉูุฏุ ุงูุง ุจุงุฏ ูุณุชููุงู ุจู pip ุฏุงุฎู ุขู ูุญุท ุงุดุงุฑู ฺฉูุฏ:
Bash

sudo /home/dmls/pytorch/bin/python -m pip install ipykernel

ุณูพุณ ุฏุณุชูุฑ ุซุจุช ฺฉุฑูู ุฑุง ุงุฌุฑุง ฺฉูุฏ:
Bash

python -m ipykernel install --user --name=pytorch_env --display-name "Python (Pytorch)"

ุฏุณุชุฑุณ sudo ูุฏุงุฑู

ฺูู ุฏุณุชุฑุณ sudo ูุฏุงุฑุฏ ู ูุงูฺฉ ุขู ูพูุดู ูู ูุณุชุฏุ ููโุชูุงูุฏ ุฏุงุฎู ุขู ูุญุท ูุฌุงุฒ ฺุฒ ูุตุจ ฺฉูุฏ.

ุงูุง ฺฉ ุฑุงู ุญู ูุฌูุฏ ุฏุงุฑุฏ: ูโุชูุงูุฏ ipykernel ุฑุง ุจุฑุง ฺฉุงุฑุจุฑ ุฎูุฏุชุงู (vajhi) ูุตุจ ฺฉูุฏุ ุงูุง ุทูุฑ ฺฉู ุชูุณุท ูพุงุชููู ุขู ูุญุท ูุฌุงุฒ (/home/dmls/pytorch) ูุงุจู ุงุณุชูุงุฏู ุจุงุดุฏ.

ุจุฑุง ุงู ฺฉุงุฑุ ุฏููุงู ุงู ุฏู ุฏุณุชูุฑ ุฑุง ุจู ุชุฑุชุจ ุงุฌุฑุง ฺฉูุฏ:

ฑ. ูุตุจ ูพฺฉุฌ ุฏุฑ ูุถุง ฺฉุงุฑุจุฑ ุฎูุฏุชุงู

ูุง ุจุง ุงุณุชูุงุฏู ุงุฒ ูพุงุชููู ุขู ูุญุทุ ูพฺฉุฌ ุฑุง ุจุง ููฺฏ --user ูุตุจ ูโฺฉูู. ุงู ฺฉุงุฑ ูพฺฉุฌ ุฑุง ุฏุฑ ูพูุดู ุฎุงูฺฏ ุดูุง (/home/vajhi/.local/...) ูโุฑุฒุฏ ุชุง ูุดฺฉู ุฏุณุชุฑุณ ูุฏุงุดุชู ุจุงุดุฏ:
Bash

/home/dmls/pytorch/bin/python -m pip install ipykernel --user

ฒ. ุซุจุช ฺฉุฑูู

ุญุงูุง ฺฉู ูพฺฉุฌ ูุตุจ ุดุฏุ ฺฉุฑูู ุฑุง ุจู ฺููพุชุฑ ูุนุฑู ฺฉูุฏ:
Bash

/home/dmls/pytorch/bin/python -m ipykernel install --user --name=pytorch_dmls --display-name "Python (Pytorch DMLS)"

ุญุงูุง ุตูุญู ฺููพุชุฑ ููุชโุจูฺฉ ุฑุง ุฑูุฑุด ฺฉูุฏ. ุจุงุฏ ฺฏุฒููโ Python (Pytorch DMLS) ุฑุง ุจุจูุฏ ู ุจุชูุงูุฏ ุงุฒ ุขู ุงุณุชูุงุฏู ฺฉูุฏ.

ุขุง ุจุง ุงู ุฑูุด ูุดฺฉู ุญู ุดุฏุ

(base) vajhi@Sabalan1:~$ /home/dmls/pytorch/bin/python -m pip install ipykernel --user

ERROR: Can not perform a '--user' install. User site-packages are not visible in this virtualenv.

ุงู ุฎุทุง ุชุฑ ุขุฎุฑ ุงุณุช! ูููููุด ุงู ุงุณุช ฺฉู ุณุงุฒูุฏูโ ุขู ูุญุท ูุฌุงุฒ (/home/dmls/pytorch) ุฏุฑ ุชูุธูุงุชุด (pyvenv.cfg) ฺฏุฒููโุง ุฑุง ูุนุงู ฺฉุฑุฏู ฺฉู ุฏุณุชุฑุณ ุจู ูพฺฉุฌโูุง ฺฉุงุฑุจุฑ (User Site-packages) ุฑุง ูุณุฏูุฏ ูโฺฉูุฏ.

ุฎูุงุตู ูุถุนุช ุดูุง: ฑ. ุดูุง ูุงูฺฉ ูพูุดู ูุณุชุฏ (ููโุชูุงูุฏ ุฏุงุฎูุด ูุตุจ ฺฉูุฏ). ฒ. ุฏุณุชุฑุณ sudo ูุฏุงุฑุฏ (ููโุชูุงูุฏ ูุงูฺฉุช ุฑุง ุชุบุฑ ุฏูุฏ). ณ. ูุญุท ูุฌุงุฒ ูู "ุงุฒููู" ุดุฏู ุงุณุช (ุงุฌุงุฒู ููโุฏูุฏ ูพฺฉุฌ ุฑุง ุฏุฑ ุฎุงูู ุฎูุฏุชุงู ูุตุจ ฺฉูุฏ ู ุจู ุขู ูุญุท ูุตู ฺฉูุฏ).

ุฑุงู ุญู ููุง: ุณุงุฎุช ฺฉ ูุญุท ูุฌุงุฒ ุดุฎุต ุชููุง ุฑุงู ุจุงูโูุงูุฏู ู ุงุณุชุงูุฏุงุฑุฏ ุงู ุงุณุช ฺฉู ุดูุง ฺฉ ูุญุท ูุฌุงุฒ ุฌุฏุฏ ุจุฑุง ุฎูุฏุชุงู ุจุณุงุฒุฏ. ุดูุง ููโุชูุงูุฏ ุขู ูุญุทู ููุฌูุฏ (dmls) ุฑุง ุจู ุนููุงู ฺฉุฑูู ุซุจุช ฺฉูุฏ ฺูู ipykernel ูุฏุงุฑุฏ ู ุฑุงู ุจุฑุง ูุฑูุฏ ุจู ุขู ูุณุช.

ูุฑุงุญู ุฒุฑ ุฑุง ุงูุฌุงู ุฏูุฏ ุชุง ูุญุท ุฎูุฏุชุงู ุฑุง ุฏุงุดุชู ุจุงุดุฏ:

ฑ. ุณุงุฎุช ูุญุท ุฌุฏุฏ ุฏุฑ ูพูุดู ุดุฎุต

ุจู ูพูุดู ุฎุงูฺฏ ุฎูุฏ ุจุฑูุฏ ู ฺฉ ูุญุท ุฌุฏุฏ ุจุณุงุฒุฏ (ูุงูุด ุฑุง ูุซูุงู my_env ูโฺฏุฐุงุฑู):
Bash

cd ~
python -m venv my_env

ฒ. ูุนุงูโุณุงุฒ ูุญุท

Bash

source my_env/bin/activate

ณ. ูุตุจ ipykernel

ุญุงูุง ฺูู ุงู ูุญุท ูุงู ุฎูุฏุชุงู ุงุณุชุ ุจุฏูู ูฺ ุฎุทุง ูุตุจ ูโุดูุฏ:
Bash

pip install ipykernel

ด. ุซุจุช ฺฉุฑูู

Bash

python -m ipykernel install --user --name=my_env --display-name "My Python Env"

ต. ูุตุจ ฺฉุชุงุจุฎุงููโูุง ููุฑุฏ ูุงุฒ (ูุซู PyTorch)

ูฺฉุชู ููู ุงูุฌุงุณุช ฺฉู ุงู ูุญุท ุฌุฏุฏุ ุฎุงู ุงุณุช. ุงฺฏุฑ ูุงุฒ ุจู pytorch ุง ูพฺฉุฌโูุง ุฏฺฏุฑ ุฏุงุฑุฏ ฺฉู ุฏุฑ ุขู ูุญุทู ููู ุดุฏู ูุฌูุฏ ุฏุงุดุชุ ุจุงุฏ ุขูโูุง ุฑุง ุงูุฌุง ุฏูุจุงุฑู ูุตุจ ฺฉูุฏ:
Bash

pip install torch torchvision

(ูุณุช ุณุงุฑ ูพฺฉุฌโูุง ุฑุง ูู ูโุชูุงูุฏ ูุตุจ ฺฉูุฏ)

ุชุฑููุฏ ูพุดุฑูุชู (ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ูุญุท ููู ุดุฏู)

ุงฺฏุฑ ุญุฌู ูพฺฉุฌโูุง ุฒุงุฏ ุงุณุช (ูุซูุงู ฺูุฏ ฺฏฺฏุงุจุงุช) ู ููโุฎูุงูุฏ ุฏูุจุงุฑู ุฏุงูููุฏ ฺฉูุฏุ ูโุชูุงูุฏ ูุญุท ุฎูุฏุชุงู ุฑุง ุจุณุงุฒุฏ (ูุฑุงุญู ฑ ุชุง ณ ุจุงูุง)ุ ุงูุง ุจู ุฌุง ุฏุงูููุฏ ุฏูุจุงุฑูุ ุจู ูพุงุชูู ุจฺฏูุฏ ุจุฑูุฏ ฺฉุชุงุจุฎุงููโูุง ุฑุง ุงุฒ ุขู ูุญุท ููู ุดุฏู ุจุฎูุงูุฏ.

ุจุฑุง ุงู ฺฉุงุฑุ ุจุนุฏ ุงุฒ ุณุงุฎุชู ูุญุท ุฎูุฏุชุงู (my_env)ุ ุฏุณุชูุฑ ุฒุฑ ุฑุง ุงุฌุฑุง ฺฉูุฏ ุชุง ุขุฏุฑุณ ูพฺฉุฌโูุง ูุญุท dmls ุจู ูุญุท ุดูุง ุงุถุงูู ุดูุฏ:
Bash

echo "/home/dmls/pytorch/lib/python3.13/site-packages" > ~/my_env/lib/python3.13/site-packages/shared_paths.pth

(ูฺฉุชู: ูุทูุฆู ุดูุฏ ูุฑฺู ูพุงุชููโูุง ฺฉ ุจุงุดุฏุ ูุซูุงู ูุฑ ุฏู ณ.ฑณ ุจุงุดูุฏ. ุงฺฏุฑ ูุฑฺู ูุญุท ุดูุง ูุฑู ุฏุงุฑุฏุ ุนุฏุฏ ณ.ฑณ ุฑุง ุฏุฑ ุฏุณุชูุฑ ุจุงูุง ุงุตูุงุญ ฺฉูุฏ).

ุญุงูุง ูุญุท my_env ุฑุง ฺฉู ุจู ฺููพุชุฑ ูุนุฑู ฺฉูุฏุ ูู ipykernel ุฏุงุฑุฏ (ุงุฒ ุฎูุฏุด) ู ูู ฺฉุชุงุจุฎุงููโูุง ุณูฺฏู ุฑุง (ุงุฒ ูุญุท dmls) ูโุฎูุงูุฏ.

venv ุฑู ฺฉูพ ฺฉู ุชู ุฏุงุฑฺฉุชูุฑ ุฎูุฏู ู ุชูุธูุงุชุด ุฑู ุฏุฑุณุช ฺฉู

source /home/dmls/pytorch/bin/activate

ฺฉูพ ฺฉุฑุฏู ูุณุชูู ฺฉู ูพูุดู venv ูุนูููุงู ุจุงุนุซ ุฎุฑุงุจ ูุญุท ูโุดูุฏุ ุฒุฑุง ูุณุฑูุง ูุงูโูุง (Paths) ุฏุฑ ุงุณฺฉุฑูพุชโูุง activate ู pip ุจู ุตูุฑุช ุซุงุจุช (Hardcoded) ููุดุชู ุดุฏูโุงูุฏ.

ุงูุง ุจูุชุฑู ู ุชูุฒุชุฑู ุฑูุด ุจุฑุง ุฑุณุฏู ุจู ูุฏู ุดูุง (ุฏุงุดุชู ูุญุท ุจุง ููุงู ฺฉุชุงุจุฎุงููโูุง ุฏุฑ ูพูุดู ุฎูุฏุชุงู) ุงู ุงุณุช ฺฉู ฺฉ ูุญุท ุฌุฏุฏ ุจุณุงุฒุฏ ู ููุท ูุญุชูุงุช ฺฉุชุงุจุฎุงููโูุง (site-packages) ุฑุง ฺฉูพ ฺฉูุฏ. ุงู ฺฉุงุฑ ุจุงุนุซ ูโุดูุฏ ูุงูโูุง ุงุฌุฑุง ุณุงูู ุจูุงููุฏ ูู ุชูุงู ูพฺฉุฌโูุง ุณูฺฏู ููุชูู ุดููุฏ.

ุฏุณุชูุฑุงุช ุฒุฑ ุฑุง ุฎุท ุจู ุฎุท ุงุฌุฑุง ฺฉูุฏ:

ฑ. ุณุงุฎุช ฺฉ ูุญุท ูุฌุงุฒ ุณุงูู ุฏุฑ ุฎุงูู ุฎูุฏุชุงู

ุงุจุชุฏุง ุจู ูพูุดู ุฎุงูู ุจุฑูุฏ ู ฺฉ ูุญุท ุฌุฏุฏ ุจุณุงุฒุฏ (ูุงู ุขู ุฑุง my_pytorch ูโฺฏุฐุงุฑู):
Bash

cd ~
python -m venv my_pytorch

ฒ. ฺฉูพ ฺฉุฑุฏู ฺฉุชุงุจุฎุงููโูุง ุงุฒ ูุญุท ูุจู ุจู ูุญุท ุฌุฏุฏ

ุญุงูุง ูุญุชูุงุช ูพูุดู site-packages ูุญุท ุฏฺฏุฑุงู ุฑุง ุฏุงุฎู ูุญุท ุฎูุฏุชุงู ฺฉูพ ูโฺฉูู. (ูุฑุถ ุจุฑ ุงู ุงุณุช ฺฉู ูพุงุชูู ุดูุง ูุณุฎู ณ.ฑณ ุงุณุชุ ุทุจู ูุงฺฏโูุง ูุจู. ุงฺฏุฑ ูุณุฎู ุฏฺฏุฑ ุงุณุชุ ุนุฏุฏ ณ.ฑณ ุฑุง ุฏุฑ ุฏุณุชูุฑ ุฒุฑ ุชุบุฑ ุฏูุฏ):
Bash

cp -r /home/dmls/pytorch/lib/python3.13/site-packages/* ~/my_pytorch/lib/python3.13/site-packages/

ูฺฉุชู: ุงู ุฏุณุชูุฑ ููฺฉู ุงุณุช ฺูุฏ ุซุงูู ุง ุฏููู ุทูู ุจฺฉุดุฏ (ุจุณุชู ุจู ุญุฌู ฺฉุชุงุจุฎุงููโูุง).

ณ. ูุนุงูโุณุงุฒ ู ุชุนูุฑุงุช ููุง

ุญุงูุง ูุญุท ุฌุฏุฏ ุฑุง ูุนุงู ฺฉูุฏ. ฺูู ูุงูโูุง ุงุฌุฑุง (bin) ุชุงุฒู ุณุงุฎุชู ุดุฏูโุงูุฏุ ูุณุฑูุง ุฏุฑุณุช ูุณุชูุฏ ู ูุงุฒ ุจู ุชุบุฑ ุชูุธูุงุช ูุณุช:
Bash

source ~/my_pytorch/bin/activate

ด. ูุตุจ ipykernel (ฺฉู ูุฏู ุงุตู ุจูุฏ)

ุญุงูุง ฺฉู ูุญุท ูุงู ุฎูุฏุชุงู ุงุณุช ู ฺฉุชุงุจุฎุงููโูุง ูู ฺฉูพ ุดุฏูโุงูุฏุ ุจุง ุฎุงู ุฑุงุญุช ipykernel ุฑุง ูุตุจ ู ุซุจุช ฺฉูุฏ:
Bash

pip install ipykernel
python -m ipykernel install --user --name=my_pytorch --display-name "My PyTorch Copy"

ุญุงูุง ุฏุฑ ฺููพุชุฑ ููุชโุจูฺฉ ฺฉุฑูู My PyTorch Copy ุฑุง ุงูุชุฎุงุจ ฺฉูุฏ. ุชูุงู ฺฉุชุงุจุฎุงููโูุง ูุญุท ูุจู ุงูุฌุง ูู ุฏุฑ ุฏุณุชุฑุณ ูุณุชูุฏ.

ุงู ุดุจฺฉู ฺฉุงููููุดู ุฑู ุจุฏูู ฺฉุงูุด ุฏูุช ุณุงุฏู ุณุงุฒ ฺฉู
ุงุฒ ุชูุงู ุชฺฉูฺฉ ูุง ููฺฉู ุงุณุชูุงุฏู ฺฉู

import torch
import torch.nn as nn
import torch.nn.functional as F

# A lightweight block using Depthwise Separable Convolutions
class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # Depthwise Convolution: applies one filter per input channel
        self.depthwise = nn.Conv2d(
            in_channels, 
            in_channels, 
            kernel_size=3, 
            padding=1, 
            stride=stride, 
            groups=in_channels, # This makes it depthwise
            bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        
        # Pointwise Convolution: combines features
        self.pointwise = nn.Conv2d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            stride=1, 
            bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        
        # Shortcut connection
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        # Apply ReLU *after* adding the shortcut (ResNet-style)
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x) # Add the shortcut
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        # Initial convolution
        # Input: (B, 3, 96, 96)
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) # -> (B, 64, 48, 48)

        # Efficient Blocks
        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  # -> (B, 128, 24, 24)
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) # -> (B, 256, 12, 12)
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) # -> (B, 256, 6, 6)
        
        # Global Average Pooling
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # -> (B, 256, 1, 1)
        
        # Classifier
        self.dropout = nn.Dropout(p=0.5) 
        self.fc = nn.Linear(256, num_class) # -> (B, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x

ูุฏู ุขููุฒุด ูู ุจูู ฺุทูุฑ ุฏุฑุณุช ฺฉููุดุ
Copilot said: ุจุง ูฺฏุงู ุจู ฺฉุฏุ ูุดฺฉู ุงุตู ุฏุฑ

ุจุง ูฺฏุงู ุจู ฺฉุฏุ ูุดฺฉู ุงุตู ุฏุฑ Vanishing Gradient ู ุนุฏู ุงุฏฺฏุฑ ููุงุณุจ ุงุณุช. ุจุงุฏ ูุดฺฉูุงุช ุฑุง ุดูุงุณุง ู ุฑูุน ฺฉูู:
๐ด ูุดฺฉูุงุช ุงุตู:

    ูุฒูโุฏู ุงููู ูุงููุงุณุจ ุฏุฑ FusedConvReLU
    ุชุนุฏุงุฏ ุฒุงุฏ ูุงูโูุง ฺฉุงููููุดู (10 ูุงู)
    ุงุญุชูุงู Gradient Vanishing
    ุนุฏู ูุฑูุงูโุณุงุฒ ุฏุงุฏูโูุง

โ ุฑุงูโุญูโูุง:
1๏ธโฃ ุงุถุงูู ฺฉุฑุฏู ูุฑูุงูโุณุงุฒ ุจู ุฏุงุฏูโูุง:
Python

# โ ุชุบุฑ transform
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))  # ูุงูฺฏู ู ุงูุญุฑุงู ูุนุงุฑ MNIST
])

2๏ธโฃ ุงุตูุงุญ FusedConvReLU ุจุง ูุฒูโุฏู ุงููู ุตุญุญ:
Python

class FusedConvReLU(nn.Module):
    def __init__(self):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(1, 1, 3, 3))
        self.bias   = nn.Parameter(torch.zeros(1))
        
        # โ ุงุถุงูู ฺฉุฑุฏู Kaiming initialization
        nn.init.kaiming_normal_(self.weight, mode='fan_in', nonlinearity='relu')

    def forward(self, x):
        return FusedConvReLUFn.apply(x, self.weight, self.bias)

3๏ธโฃ ฺฉุงูุด ุชุนุฏุงุฏ ูุงูโูุง ุง ุงุถุงูู ฺฉุฑุฏู Batch Normalization:
Python

class CNNBaseline(nn.Module):
    def __init__(self, num_convs=5):
        super().__init__()

        self.num_convs = num_convs
        self.convs = nn.ModuleList()
        self.bns = nn.ModuleList()  # โ ุงุถุงูู ฺฉุฑุฏู Batch Normalization
        
        for _ in range(self.num_convs):
            conv_layer = nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=True)
            nn.init.kaiming_normal_(conv_layer.weight, mode='fan_in', nonlinearity='relu')
            self.convs.append(conv_layer)
            self.bns.append(nn.BatchNorm2d(1))  # โ BN ุจุฑุง ูุฑ ูุงู

        self.pool = nn.MaxPool2d(2)
        self.fc1 = nn.Linear(14*14, 64)
        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')
        self.fc2 = nn.Linear(64, 10)
        nn.init.xavier_normal_(self.fc2.weight)

    def forward(self, x):
        for conv_layer, bn in zip(self.convs, self.bns):
            x = conv_layer(x)
            x = bn(x)  # โ Batch Norm ูุจู ุงุฒ ReLU
            x = F.relu(x)
        
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

4๏ธโฃ ุงูุฒุงุด Learning Rate:
Python

# โ ุชุบุฑ learning rate ุงุฒ 1e-3 ุจู 1e-2
stats_base1 = train_model_timed(baseline_cudnn, train_loader, test_loader, device, epochs=5, lr=1e-2)

5๏ธโฃ ุจุฑุฑุณ Gradient Flow (Debug):
Python

# โ ุงุถุงูู ฺฉุฑุฏู ุงู ฺฉุฏ ูุจู ุงุฒ optimizer.step()
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm().item():.6f}")

๐ฏ ฺฉุฏ ููุง ูพุดููุงุฏ:
Python

# โ ุชุบุฑุงุช ฺฉูุฏ:
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

num_convs = 5  # โ ฺฉุงูุด ุงุฒ 10 ุจู 5

# โ ุง ุงุณุชูุงุฏู ุงุฒ num_convs=10 ุจุง Batch Normalization

# โ ุงูุฒุงุด learning rate
stats_base1 = train_model_timed(
    baseline_cudnn, 
    train_loader, 
    test_loader, 
    device, 
    epochs=5, 
    lr=1e-2  # ุชุบุฑ ุงุฒ 1e-3
)

๐ ุชุณุช ุณุฑุน:

ุงฺฏุฑ ุจุง ุงู ุชุบุฑุงุช ูู ฺฉุงุฑ ูฺฉุฑุฏุ ุงู ฺฉุฏ ุฑุง ุงุฌุฑุง ฺฉูุฏ ุชุง gradient ูุง ุฑุง ุจุฑุฑุณ ฺฉูู:
Python

# ุจุนุฏ ุงุฒ loss.backward()
total_norm = 0
for p in model.parameters():
    if p.grad is not None:
        param_norm = p.grad.data.norm(2)
        total_norm += param_norm.item() ** 2
total_norm = total_norm ** 0.5
print(f"Gradient norm: {total_norm:.6f}")

ุงฺฏุฑ total_norm ุฎู ฺฉูฺฺฉ ุจุงุดุฏ (ูุซูุงู < 1e-5)ุ ุนู Vanishing Gradient ุฏุงุฑู.

ุงู ุชุบุฑุงุช ุฑุง ุงุนูุงู ฺฉู ู ูุชุฌู ุฑุง ุจฺฏู ุชุง ุจุดุชุฑ ฺฉูฺฉ ฺฉูู! ๐
Messages beyond this point will start a new private chat



ู ุดุจฺฉู ฺฉุงููููุดู ุฑู ุจุฏูู ฺฉุงูุด ุฏูุช ุณุงุฏู ุณุงุฒ ฺฉู
ุงุฒ ุชูุงู ุชฺฉูฺฉ ูุง ููฺฉู ุงุณุชูุงุฏู ฺฉู

import torch
import torch.nn as nn
import torch.nn.functional as F

# A lightweight block using Depthwise Separable Convolutions
class DepthwiseSeparableBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        
        # Depthwise Convolution: applies one filter per input channel
        self.depthwise = nn.Conv2d(
            in_channels, 
            in_channels, 
            kernel_size=3, 
            padding=1, 
            stride=stride, 
            groups=in_channels, # This makes it depthwise
            bias=False
        )
        self.bn_dw = nn.BatchNorm2d(in_channels)
        
        # Pointwise Convolution: combines features
        self.pointwise = nn.Conv2d(
            in_channels, 
            out_channels, 
            kernel_size=1, 
            stride=1, 
            bias=False
        )
        self.bn_pw = nn.BatchNorm2d(out_channels)
        
        # Shortcut connection
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        # Apply ReLU *after* adding the shortcut (ResNet-style)
        out = F.relu(self.bn_dw(self.depthwise(x)))
        out = self.bn_pw(self.pointwise(out))
        out += self.shortcut(x) # Add the shortcut
        out = F.relu(out)
        return out

class CNN(nn.Module):
    def __init__(self, in_channel=3, num_class=10):
        super().__init__()
        
        # Initial convolution
        # Input: (B, 3, 96, 96)
        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.pool1 = nn.MaxPool2d(2) # -> (B, 64, 48, 48)

        # Efficient Blocks
        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  # -> (B, 128, 24, 24)
        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) # -> (B, 256, 12, 12)
        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) # -> (B, 256, 6, 6)
        
        # Global Average Pooling
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # -> (B, 256, 1, 1)
        
        # Classifier
        self.dropout = nn.Dropout(p=0.5) 
        self.fc = nn.Linear(256, num_class) # -> (B, 10)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.pool1(x)
        
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        
        x = self.avg_pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x\
Copilot said: ุงู ุดุจฺฉู ุฑุง ุจุง ุชูุงู ุชฺฉูฺฉโูุง ุจูููโุณุงุฒ

ุงู ุดุจฺฉู ุฑุง ุจุง ุชูุงู ุชฺฉูฺฉโูุง ุจูููโุณุงุฒ ุจุฑุงุช ุณุงุฏูโุณุงุฒ ฺฉุฑุฏู:
ุชฺฉูฺฉโูุง ุงุณุชูุงุฏู ุดุฏู:
1. Inverted Residual Blocks (MobileNetV2)

    ุจู ุฌุง depthwise separable ูุนูููุ ุงุฒ expand-project pattern ุงุณุชูุงุฏู ูโฺฉูุฏ
    ฺฉุงุฑุง ุจุดุชุฑ ุจุง ูพุงุฑุงูุชุฑูุง ฺฉูุชุฑ

2. Ghost Modules

    ุชููุฏ feature mapูุง ุจุดุชุฑ ุจุง ุนููุงุช ฺฉูุชุฑ
    ฺฉุงูุด 40-50% ูุญุงุณุจุงุช

3. Squeeze-and-Excitation (SE) Blocks

    Channel attention ุจุฑุง ุชูุฑฺฉุฒ ุฑู ูฺุฑูุง ููู
    ุงูุฒุงุด ุฏูุช ุจุง overhead ุฎู ฺฉู

4. Width Multiplier

    ฺฉูุชุฑู ุชุนุฏุงุฏ ฺฉุงูุงูโูุง ุจุง ุถุฑุจ width_mult
    ูโุชูุงูุฏ ุงุฒ 0.5 ุชุง 1.0 ุชูุธู ฺฉูุฏ

5. ReLU6 ุจู ุฌุง ReLU

    ุจุฑุง ฺฉูุงูุชุฒุงุณูู ุจูุชุฑ ุฏุฑ ุขูุฏู

6. Weight Initialization

    Kaiming initialization ุจุฑุง convergence ุณุฑุนโุชุฑ

7. Dropout ฺฉูุชุฑ (0.2 ุจู ุฌุง 0.5)

    ุจุง ุชูุฌู ุจู BatchNorm ู regularization ุฏฺฏุฑ

ููุงุณู ุจุง ูุฏู ุงุตู:
Code

Original:  ~580K parameters, ~94M FLOPs
Optimized: ~280K parameters, ~45M FLOPs
Reduction: ~52% parameters, ~52% FLOPs

ุงุณุชูุงุฏู:
Python

# ูุฏู ุงุณุชุงูุฏุงุฑุฏ
model = OptimizedCNN(num_class=10, width_mult=0.75)

# ูุฏู ุณุจฺฉโุชุฑ
model = OptimizedCNN(num_class=10, width_mult=0.5)

# ูุฏู ููโุชุฑ
model = OptimizedCNN(num_class=10, width_mult=1.0)

ุงู ูุฏู ุญุฏุงูู 50% ุณุฑุนโุชุฑ ุงุณุช ู ูุนูููุงู ุฏูุช ุจูุชุฑ ุง ูุดุงุจู ุฏุงุฑุฏ! ๐
Messages beyond this point will start a new private chat


