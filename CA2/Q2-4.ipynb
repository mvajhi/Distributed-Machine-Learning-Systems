{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8e7dbe9910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(96, padding=12),      \n",
    "    transforms.RandomHorizontalFlip(p=0.5),   \n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), \n",
    "    transforms.RandomRotation(10),            \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset1 = datasets.STL10('/storage/dmls/stl10_data', split='train',\n",
    "                         transform=train_transform)\n",
    "dataset2 = datasets.STL10('/storage/dmls/stl10_data', split='test',\n",
    "                         transform=test_transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model_fast(model, train_loader, test_loader, device, \n",
    "                       epochs=10, max_lr=1e-3):\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=max_lr,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    cuda_mem = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            cuda_mem = torch.cuda.max_memory_allocated(device=device)\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(\n",
    "            f\"Rank={device} \"\n",
    "            f\"Epoch {epoch}/{epochs}: \"\n",
    "            f\"loss={avg_loss:.4f}, \"\n",
    "            f\"test_acc={acc*100:.2f}%, \"\n",
    "            f\"LR={current_lr:.6f}, \"\n",
    "        )\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Rank={device} Total Time={end-start}\")\n",
    "    print(f\"Rank={device} Mem Usage={cuda_mem / (1024**2)} MB\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DepthwiseSeparableBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, \n",
    "            in_channels, \n",
    "            kernel_size=3, \n",
    "            padding=1, \n",
    "            stride=stride, \n",
    "            groups=in_channels, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn_dw = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        \n",
    "        self.pointwise = nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=1, \n",
    "            stride=1, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn_pw = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu(self.bn_dw(self.depthwise(x)))\n",
    "        out = self.bn_pw(self.pointwise(out))\n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channel=3, num_class=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2) \n",
    "\n",
    "        \n",
    "        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  \n",
    "        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) \n",
    "        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) \n",
    "        \n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "        self.fc = nn.Linear(256, num_class) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = CNN(in_channel=3, num_class=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training ===\n",
      "Rank=cuda Epoch 1/15: loss=2.1743, test_acc=26.38%, LR=0.000153, \n",
      "Rank=cuda Epoch 2/15: loss=1.8903, test_acc=35.16%, LR=0.000438, \n",
      "Rank=cuda Epoch 3/15: loss=1.6732, test_acc=41.05%, LR=0.000761, \n",
      "Rank=cuda Epoch 4/15: loss=1.5576, test_acc=42.56%, LR=0.000972, \n",
      "Rank=cuda Epoch 5/15: loss=1.4481, test_acc=46.24%, LR=0.000994, \n",
      "Rank=cuda Epoch 6/15: loss=1.3617, test_acc=50.89%, LR=0.000950, \n",
      "Rank=cuda Epoch 7/15: loss=1.2771, test_acc=55.83%, LR=0.000866, \n",
      "Rank=cuda Epoch 8/15: loss=1.2190, test_acc=54.45%, LR=0.000749, \n",
      "Rank=cuda Epoch 9/15: loss=1.1686, test_acc=58.01%, LR=0.000610, \n",
      "Rank=cuda Epoch 10/15: loss=1.1122, test_acc=60.34%, LR=0.000462, \n",
      "Rank=cuda Epoch 11/15: loss=1.0793, test_acc=62.71%, LR=0.000316, \n",
      "Rank=cuda Epoch 12/15: loss=1.0468, test_acc=63.64%, LR=0.000188, \n",
      "Rank=cuda Epoch 13/15: loss=1.0010, test_acc=64.22%, LR=0.000086, \n",
      "Rank=cuda Epoch 14/15: loss=0.9759, test_acc=64.54%, LR=0.000022, \n",
      "Rank=cuda Epoch 15/15: loss=0.9704, test_acc=64.84%, LR=0.000000, \n",
      "Rank=cuda Total Time=107.86553740501404\n",
      "Rank=cuda Mem Usage=312.080078125 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Training ===\")\n",
    "device = \"cuda\"\n",
    "\n",
    "stats = train_model_fast(model, train_loader, test_loader, device, epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_ddp.py\n",
    "import os, time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.cpp_extension import load_inline\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "import argparse\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "def train_model_fast(model, train_loader, test_loader, device, \n",
    "                       epochs=10, max_lr=1e-3):\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=max_lr, weight_decay=5e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=max_lr,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    cuda_mem = 0\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            cuda_mem = torch.cuda.max_memory_allocated(device=device)\n",
    "\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        print(\n",
    "            f\"Rank={device} \"\n",
    "            f\"Epoch {epoch}/{epochs}: \"\n",
    "            f\"loss={avg_loss:.4f}, \"\n",
    "            f\"test_acc={acc*100:.2f}%, \"\n",
    "            f\"LR={current_lr:.6f}, \"\n",
    "        )\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Rank={device} Total Time={end-start}\")\n",
    "    print(f\"Rank={device} Mem Usage={cuda_mem / (1024**2)} MB\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "class DepthwiseSeparableBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, \n",
    "            in_channels, \n",
    "            kernel_size=3, \n",
    "            padding=1, \n",
    "            stride=stride, \n",
    "            groups=in_channels, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn_dw = nn.BatchNorm2d(in_channels)\n",
    "        \n",
    "        \n",
    "        self.pointwise = nn.Conv2d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size=1, \n",
    "            stride=1, \n",
    "            bias=False\n",
    "        )\n",
    "        self.bn_pw = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = F.relu(self.bn_dw(self.depthwise(x)))\n",
    "        out = self.bn_pw(self.pointwise(out))\n",
    "        out += self.shortcut(x) \n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channel=3, num_class=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=3, padding=1, stride=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2) \n",
    "\n",
    "        \n",
    "        self.block1 = DepthwiseSeparableBlock(64, 128, stride=2)  \n",
    "        self.block2 = DepthwiseSeparableBlock(128, 256, stride=2) \n",
    "        self.block3 = DepthwiseSeparableBlock(256, 256, stride=2) \n",
    "        \n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) \n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2) \n",
    "        self.fc = nn.Linear(256, num_class) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def setup(rank, world_size, master_port, backend, timeout):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = master_port\n",
    "    dist.init_process_group(backend=backend, rank=rank, world_size=world_size, timeout=timeout)\n",
    "    \n",
    "def load_data(rank, world_size, batch_size):\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(96, padding=12),      \n",
    "        transforms.RandomHorizontalFlip(p=0.5),   \n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2), \n",
    "        transforms.RandomRotation(10),            \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    train_set = datasets.STL10('/storage/dmls/stl10_data', split='train', transform=train_transform)\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_set,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        persistent_workers=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_set = datasets.STL10('/storage/dmls/stl10_data', split='test', transform=test_transform)\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        test_set,\n",
    "        num_replicas=world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set,\n",
    "        sampler=test_sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        persistent_workers=True,\n",
    "        num_workers=1,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def train_dist(rank, world_size, master_port, backend, timeout, batch_size):\n",
    "    setup(rank, world_size, master_port, backend, timeout)\n",
    "    torch.cuda.set_device(rank)\n",
    "    train_loader, test_loader = load_data(rank, world_size, batch_size)\n",
    "    model = CNN(in_channel=3, num_class=10).to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    train_model_fast(ddp_model, train_loader, test_loader, rank, epochs=EPOCHS)\n",
    "    dist.destroy_process_group()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--backend', type=str, default='nccl')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    world_size = torch.cuda.device_count()\n",
    "    master_port = '8282'\n",
    "    timeout = datetime.timedelta(seconds=10)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    mp.spawn(\n",
    "        train_dist, \n",
    "        nprocs=world_size, \n",
    "        args=(world_size, master_port, args.backend, timeout, args.batch_size), \n",
    "        join=True\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(\"Total time: {}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank=0 Epoch 1/15: loss=1.1100, test_acc=27.85%, LR=0.000153, \n",
      "Rank=1 Epoch 1/15: loss=1.1080, test_acc=28.50%, LR=0.000153, \n",
      "Rank=0 Epoch 2/15: loss=0.9687, test_acc=36.33%, LR=0.000439, \n",
      "Rank=1 Epoch 2/15: loss=0.9663, test_acc=34.38%, LR=0.000439, \n",
      "Rank=0 Epoch 3/15: loss=0.8702, test_acc=41.45%, LR=0.000762, \n",
      "Rank=1 Epoch 3/15: loss=0.8575, test_acc=40.90%, LR=0.000762, \n",
      "Rank=0 Epoch 4/15: loss=0.8058, test_acc=43.38%, LR=0.000972, \n",
      "Rank=1 Epoch 4/15: loss=0.7981, test_acc=44.07%, LR=0.000972, \n",
      "Rank=0 Epoch 5/15: loss=0.7552, test_acc=46.12%, LR=0.000994, \n",
      "Rank=1 Epoch 5/15: loss=0.7482, test_acc=45.77%, LR=0.000994, \n",
      "Rank=0 Epoch 6/15: loss=0.7056, test_acc=51.65%, LR=0.000950, \n",
      "Rank=1 Epoch 6/15: loss=0.7100, test_acc=50.72%, LR=0.000950, \n",
      "Rank=0 Epoch 7/15: loss=0.6757, test_acc=54.07%, LR=0.000865, \n",
      "Rank=1 Epoch 7/15: loss=0.6744, test_acc=54.02%, LR=0.000865, \n",
      "Rank=0 Epoch 8/15: loss=0.6396, test_acc=55.30%, LR=0.000748, \n",
      "Rank=1 Epoch 8/15: loss=0.6267, test_acc=53.85%, LR=0.000748, \n",
      "Rank=0 Epoch 9/15: loss=0.6168, test_acc=57.17%, LR=0.000609, \n",
      "Rank=1 Epoch 9/15: loss=0.6081, test_acc=55.67%, LR=0.000609, \n",
      "Rank=0 Epoch 10/15: loss=0.5879, test_acc=58.65%, LR=0.000461, \n",
      "Rank=1 Epoch 10/15: loss=0.5870, test_acc=57.77%, LR=0.000461, \n",
      "Rank=0 Epoch 11/15: loss=0.5807, test_acc=59.70%, LR=0.000316, \n",
      "Rank=1 Epoch 11/15: loss=0.5654, test_acc=59.17%, LR=0.000316, \n",
      "Rank=0 Epoch 12/15: loss=0.5640, test_acc=60.40%, LR=0.000187, \n",
      "Rank=1 Epoch 12/15: loss=0.5531, test_acc=59.72%, LR=0.000187, \n",
      "Rank=0 Epoch 13/15: loss=0.5449, test_acc=61.82%, LR=0.000086, \n",
      "Rank=1 Epoch 13/15: loss=0.5397, test_acc=60.62%, LR=0.000086, \n",
      "Rank=0 Epoch 14/15: loss=0.5391, test_acc=61.92%, LR=0.000022, \n",
      "Rank=1 Epoch 14/15: loss=0.5262, test_acc=61.22%, LR=0.000022, \n",
      "Rank=0 Epoch 15/15: loss=0.5312, test_acc=61.85%, LR=0.000000, \n",
      "Rank=0 Total Time=67.18971014022827\n",
      "Rank=0 Mem Usage=313.796875 MB\n",
      "Rank=1 Epoch 15/15: loss=0.5253, test_acc=61.22%, LR=0.000000, \n",
      "Rank=1 Total Time=67.21239113807678\n",
      "Rank=1 Mem Usage=313.796875 MB\n",
      "Total time: 79.35693264007568\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank=0 Epoch 1/15: loss=1.1018, test_acc=30.15%, LR=0.000153, \n",
      "Rank=1 Epoch 1/15: loss=1.0995, test_acc=30.03%, LR=0.000153, \n",
      "Rank=0 Epoch 2/15: loss=0.9478, test_acc=38.88%, LR=0.000438, \n",
      "Rank=1 Epoch 2/15: loss=0.9442, test_acc=38.65%, LR=0.000438, \n",
      "Rank=0 Epoch 3/15: loss=0.8511, test_acc=41.75%, LR=0.000761, \n",
      "Rank=1 Epoch 3/15: loss=0.8475, test_acc=40.60%, LR=0.000761, \n",
      "Rank=0 Epoch 4/15: loss=0.8059, test_acc=45.10%, LR=0.000972, \n",
      "Rank=1 Epoch 4/15: loss=0.7898, test_acc=43.70%, LR=0.000972, \n",
      "Rank=0 Epoch 5/15: loss=0.7525, test_acc=46.62%, LR=0.000994, \n",
      "Rank=1 Epoch 5/15: loss=0.7455, test_acc=48.20%, LR=0.000994, \n",
      "Rank=0 Epoch 6/15: loss=0.7138, test_acc=50.48%, LR=0.000950, \n",
      "Rank=1 Epoch 6/15: loss=0.7010, test_acc=51.00%, LR=0.000950, \n",
      "Rank=1 Epoch 7/15: loss=0.6630, test_acc=53.17%, LR=0.000866, \n",
      "Rank=0 Epoch 7/15: loss=0.6728, test_acc=54.45%, LR=0.000866, \n",
      "Rank=0 Epoch 8/15: loss=0.6375, test_acc=56.83%, LR=0.000749, \n",
      "Rank=1 Epoch 8/15: loss=0.6382, test_acc=56.65%, LR=0.000749, \n",
      "Rank=0 Epoch 9/15: loss=0.6150, test_acc=58.77%, LR=0.000610, \n",
      "Rank=1 Epoch 9/15: loss=0.6128, test_acc=57.55%, LR=0.000610, \n",
      "Rank=1 Epoch 10/15: loss=0.5930, test_acc=60.42%, LR=0.000462, \n",
      "Rank=0 Epoch 10/15: loss=0.5939, test_acc=60.88%, LR=0.000462, \n",
      "Rank=0 Epoch 11/15: loss=0.5715, test_acc=61.02%, LR=0.000316, \n",
      "Rank=1 Epoch 11/15: loss=0.5723, test_acc=59.15%, LR=0.000316, \n",
      "Rank=0 Epoch 12/15: loss=0.5618, test_acc=61.98%, LR=0.000188, \n",
      "Rank=1 Epoch 12/15: loss=0.5510, test_acc=61.30%, LR=0.000188, \n",
      "Rank=0 Epoch 13/15: loss=0.5327, test_acc=62.42%, LR=0.000086, \n",
      "Rank=1 Epoch 13/15: loss=0.5401, test_acc=61.48%, LR=0.000086, \n",
      "Rank=0 Epoch 14/15: loss=0.5285, test_acc=62.92%, LR=0.000022, \n",
      "Rank=1 Epoch 14/15: loss=0.5202, test_acc=62.40%, LR=0.000022, \n",
      "Rank=1 Epoch 15/15: loss=0.5156, test_acc=62.30%, LR=0.000000, \n",
      "Rank=1 Total Time=78.6634259223938\n",
      "Rank=1 Mem Usage=167.25048828125 MB\n",
      "Rank=0 Epoch 15/15: loss=0.5274, test_acc=63.08%, LR=0.000000, \n",
      "Rank=0 Total Time=78.67163705825806\n",
      "Rank=0 Mem Usage=167.25048828125 MB\n",
      "Total time: 92.8861346244812\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py --batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank=0 Epoch 1/15: loss=1.1159, test_acc=27.12%, LR=0.000153, \n",
      "Rank=1 Epoch 1/15: loss=1.1214, test_acc=26.47%, LR=0.000153, \n",
      "Rank=1 Epoch 2/15: loss=0.9696, test_acc=33.23%, LR=0.000439, \n",
      "Rank=0 Epoch 2/15: loss=0.9769, test_acc=35.08%, LR=0.000439, \n",
      "Rank=1 Epoch 3/15: loss=0.8654, test_acc=39.25%, LR=0.000762, \n",
      "Rank=0 Epoch 3/15: loss=0.8743, test_acc=39.62%, LR=0.000762, \n",
      "Rank=0 Epoch 4/15: loss=0.8081, test_acc=45.48%, LR=0.000972, \n",
      "Rank=1 Epoch 4/15: loss=0.7997, test_acc=44.45%, LR=0.000972, \n",
      "Rank=1 Epoch 5/15: loss=0.7522, test_acc=45.98%, LR=0.000994, \n",
      "Rank=0 Epoch 5/15: loss=0.7530, test_acc=45.70%, LR=0.000994, \n",
      "Rank=1 Epoch 6/15: loss=0.7045, test_acc=45.40%, LR=0.000950, \n",
      "Rank=0 Epoch 6/15: loss=0.7225, test_acc=45.60%, LR=0.000950, \n",
      "Rank=1 Epoch 7/15: loss=0.6675, test_acc=53.33%, LR=0.000865, \n",
      "Rank=0 Epoch 7/15: loss=0.6762, test_acc=53.42%, LR=0.000865, \n",
      "Rank=0 Epoch 8/15: loss=0.6462, test_acc=53.73%, LR=0.000748, \n",
      "Rank=1 Epoch 8/15: loss=0.6399, test_acc=52.80%, LR=0.000748, \n",
      "Rank=0 Epoch 9/15: loss=0.6180, test_acc=54.60%, LR=0.000609, \n",
      "Rank=1 Epoch 9/15: loss=0.6195, test_acc=54.57%, LR=0.000609, \n",
      "Rank=1 Epoch 10/15: loss=0.5972, test_acc=57.35%, LR=0.000461, \n",
      "Rank=0 Epoch 10/15: loss=0.5927, test_acc=57.25%, LR=0.000461, \n",
      "Rank=1 Epoch 11/15: loss=0.5780, test_acc=58.63%, LR=0.000316, \n",
      "Rank=0 Epoch 11/15: loss=0.5757, test_acc=59.82%, LR=0.000316, \n",
      "Rank=1 Epoch 12/15: loss=0.5608, test_acc=60.48%, LR=0.000187, \n",
      "Rank=0 Epoch 12/15: loss=0.5643, test_acc=60.40%, LR=0.000187, \n",
      "Rank=1 Epoch 13/15: loss=0.5430, test_acc=61.25%, LR=0.000086, \n",
      "Rank=0 Epoch 13/15: loss=0.5505, test_acc=61.00%, LR=0.000086, \n",
      "Rank=1 Epoch 14/15: loss=0.5349, test_acc=61.15%, LR=0.000022, \n",
      "Rank=0 Epoch 14/15: loss=0.5333, test_acc=61.02%, LR=0.000022, \n",
      "Rank=1 Epoch 15/15: loss=0.5342, test_acc=61.38%, LR=0.000000, \n",
      "Rank=1 Total Time=80.25996947288513\n",
      "Rank=1 Mem Usage=312.9384765625 MB\n",
      "Rank=0 Epoch 15/15: loss=0.5352, test_acc=61.10%, LR=0.000000, \n",
      "Rank=0 Total Time=80.27124166488647\n",
      "Rank=0 Mem Usage=313.796875 MB\n",
      "Total time: 94.2029390335083\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py --batch_size 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank=1 Epoch 1/15: loss=1.1376, test_acc=23.72%, LR=0.000154, \n",
      "Rank=0 Epoch 1/15: loss=1.1384, test_acc=24.18%, LR=0.000154, \n",
      "Rank=1 Epoch 2/15: loss=1.0158, test_acc=31.45%, LR=0.000440, \n",
      "Rank=0 Epoch 2/15: loss=1.0125, test_acc=32.05%, LR=0.000440, \n",
      "Rank=0 Epoch 3/15: loss=0.9112, test_acc=38.98%, LR=0.000765, \n",
      "Rank=1 Epoch 3/15: loss=0.9132, test_acc=37.20%, LR=0.000765, \n",
      "Rank=1 Epoch 4/15: loss=0.8288, test_acc=39.32%, LR=0.000974, \n",
      "Rank=0 Epoch 4/15: loss=0.8417, test_acc=40.05%, LR=0.000974, \n",
      "Rank=0 Epoch 5/15: loss=0.7852, test_acc=45.52%, LR=0.000994, \n",
      "Rank=1 Epoch 5/15: loss=0.7785, test_acc=43.80%, LR=0.000994, \n",
      "Rank=0 Epoch 6/15: loss=0.7570, test_acc=47.58%, LR=0.000949, \n",
      "Rank=1 Epoch 6/15: loss=0.7403, test_acc=45.45%, LR=0.000949, \n",
      "Rank=0 Epoch 7/15: loss=0.7184, test_acc=48.27%, LR=0.000864, \n",
      "Rank=1 Epoch 7/15: loss=0.7081, test_acc=46.90%, LR=0.000864, \n",
      "Rank=1 Epoch 8/15: loss=0.6865, test_acc=51.85%, LR=0.000747, \n",
      "Rank=0 Epoch 8/15: loss=0.6824, test_acc=52.85%, LR=0.000747, \n",
      "Rank=0 Epoch 9/15: loss=0.6619, test_acc=53.40%, LR=0.000608, \n",
      "Rank=1 Epoch 9/15: loss=0.6625, test_acc=52.85%, LR=0.000608, \n",
      "Rank=1 Epoch 10/15: loss=0.6339, test_acc=53.75%, LR=0.000459, \n",
      "Rank=0 Epoch 10/15: loss=0.6344, test_acc=53.70%, LR=0.000459, \n",
      "Rank=1 Epoch 11/15: loss=0.6211, test_acc=55.77%, LR=0.000314, \n",
      "Rank=0 Epoch 11/15: loss=0.6204, test_acc=55.65%, LR=0.000314, \n",
      "Rank=1 Epoch 12/15: loss=0.6030, test_acc=58.03%, LR=0.000185, \n",
      "Rank=0 Epoch 12/15: loss=0.6086, test_acc=57.23%, LR=0.000185, \n",
      "Rank=1 Epoch 13/15: loss=0.5899, test_acc=57.77%, LR=0.000085, \n",
      "Rank=0 Epoch 13/15: loss=0.5930, test_acc=58.17%, LR=0.000085, \n",
      "Rank=1 Epoch 14/15: loss=0.5803, test_acc=58.33%, LR=0.000021, \n",
      "Rank=0 Epoch 14/15: loss=0.5852, test_acc=58.50%, LR=0.000021, \n",
      "Rank=1 Epoch 15/15: loss=0.5738, test_acc=58.50%, LR=0.000000, \n",
      "Rank=1 Total Time=80.28480839729309\n",
      "Rank=1 Mem Usage=605.1728515625 MB\n",
      "Rank=0 Epoch 15/15: loss=0.5902, test_acc=58.65%, LR=0.000000, \n",
      "Rank=0 Total Time=80.30587029457092\n",
      "Rank=0 Mem Usage=605.1728515625 MB\n",
      "Total time: 93.023508310318\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py --batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank=1 Epoch 1/15: loss=1.1601, test_acc=15.07%, LR=0.000155, \n",
      "Rank=0 Epoch 1/15: loss=1.1614, test_acc=14.57%, LR=0.000155, \n",
      "Rank=0 Epoch 2/15: loss=1.0527, test_acc=25.90%, LR=0.000444, \n",
      "Rank=1 Epoch 2/15: loss=1.0567, test_acc=27.47%, LR=0.000444, \n",
      "Rank=0 Epoch 3/15: loss=0.9528, test_acc=35.10%, LR=0.000770, \n",
      "Rank=1 Epoch 3/15: loss=0.9507, test_acc=33.88%, LR=0.000770, \n",
      "Rank=0 Epoch 4/15: loss=0.8695, test_acc=40.58%, LR=0.000976, \n",
      "Rank=1 Epoch 4/15: loss=0.8713, test_acc=40.58%, LR=0.000976, \n",
      "Rank=0 Epoch 5/15: loss=0.8110, test_acc=42.27%, LR=0.000993, \n",
      "Rank=1 Epoch 5/15: loss=0.7963, test_acc=40.62%, LR=0.000993, \n",
      "Rank=1 Epoch 6/15: loss=0.7606, test_acc=44.60%, LR=0.000947, \n",
      "Rank=0 Epoch 6/15: loss=0.7657, test_acc=45.05%, LR=0.000947, \n",
      "Rank=1 Epoch 7/15: loss=0.7238, test_acc=48.27%, LR=0.000861, \n",
      "Rank=0 Epoch 7/15: loss=0.7337, test_acc=47.70%, LR=0.000861, \n",
      "Rank=1 Epoch 8/15: loss=0.7011, test_acc=49.18%, LR=0.000743, \n",
      "Rank=0 Epoch 8/15: loss=0.7051, test_acc=49.05%, LR=0.000743, \n",
      "Rank=0 Epoch 9/15: loss=0.6812, test_acc=51.08%, LR=0.000604, \n",
      "Rank=1 Epoch 9/15: loss=0.6749, test_acc=50.98%, LR=0.000604, \n",
      "Rank=1 Epoch 10/15: loss=0.6513, test_acc=52.08%, LR=0.000455, \n",
      "Rank=0 Epoch 10/15: loss=0.6588, test_acc=52.55%, LR=0.000455, \n",
      "Rank=0 Epoch 11/15: loss=0.6394, test_acc=54.57%, LR=0.000310, \n",
      "Rank=1 Epoch 11/15: loss=0.6311, test_acc=54.40%, LR=0.000310, \n",
      "Rank=1 Epoch 12/15: loss=0.6250, test_acc=56.03%, LR=0.000182, \n",
      "Rank=0 Epoch 12/15: loss=0.6286, test_acc=56.47%, LR=0.000182, \n",
      "Rank=1 Epoch 13/15: loss=0.6170, test_acc=55.73%, LR=0.000083, \n",
      "Rank=0 Epoch 13/15: loss=0.6190, test_acc=56.50%, LR=0.000083, \n",
      "Rank=1 Epoch 14/15: loss=0.6011, test_acc=55.93%, LR=0.000020, \n",
      "Rank=0 Epoch 14/15: loss=0.6105, test_acc=56.77%, LR=0.000020, \n",
      "Rank=1 Epoch 15/15: loss=0.6083, test_acc=55.90%, LR=0.000000, \n",
      "Rank=1 Total Time=80.54266476631165\n",
      "Rank=1 Mem Usage=1189.30078125 MB\n",
      "Rank=0 Epoch 15/15: loss=0.6102, test_acc=56.83%, LR=0.000000, \n",
      "Rank=0 Total Time=80.59433770179749\n",
      "Rank=0 Mem Usage=1189.30078125 MB\n",
      "Total time: 93.4434585571289\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py --batch_size 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank=0 Epoch 1/15: loss=1.1172, test_acc=24.77%, LR=0.000153, \n",
      "Rank=1 Epoch 1/15: loss=1.1139, test_acc=25.85%, LR=0.000153, \n",
      "Rank=0 Epoch 2/15: loss=0.9897, test_acc=34.15%, LR=0.000439, \n",
      "Rank=1 Epoch 2/15: loss=0.9817, test_acc=32.95%, LR=0.000439, \n",
      "Rank=0 Epoch 3/15: loss=0.8722, test_acc=38.73%, LR=0.000762, \n",
      "Rank=1 Epoch 3/15: loss=0.8648, test_acc=38.10%, LR=0.000762, \n",
      "Rank=1 Epoch 4/15: loss=0.7914, test_acc=42.98%, LR=0.000972, \n",
      "Rank=0 Epoch 4/15: loss=0.8037, test_acc=42.50%, LR=0.000972, \n",
      "Rank=0 Epoch 5/15: loss=0.7632, test_acc=48.70%, LR=0.000994, \n",
      "Rank=1 Epoch 5/15: loss=0.7518, test_acc=47.00%, LR=0.000994, \n",
      "Rank=0 Epoch 6/15: loss=0.7124, test_acc=50.65%, LR=0.000950, \n",
      "Rank=1 Epoch 6/15: loss=0.7039, test_acc=48.65%, LR=0.000950, \n",
      "Rank=0 Epoch 7/15: loss=0.6764, test_acc=53.40%, LR=0.000865, \n",
      "Rank=1 Epoch 7/15: loss=0.6819, test_acc=52.88%, LR=0.000865, \n",
      "Rank=1 Epoch 8/15: loss=0.6465, test_acc=52.80%, LR=0.000748, \n",
      "Rank=0 Epoch 8/15: loss=0.6528, test_acc=53.47%, LR=0.000748, \n",
      "Rank=0 Epoch 9/15: loss=0.6262, test_acc=56.20%, LR=0.000609, \n",
      "Rank=1 Epoch 9/15: loss=0.6217, test_acc=56.05%, LR=0.000609, \n",
      "Rank=0 Epoch 10/15: loss=0.6039, test_acc=57.20%, LR=0.000461, \n",
      "Rank=1 Epoch 10/15: loss=0.5950, test_acc=56.45%, LR=0.000461, \n",
      "Rank=0 Epoch 11/15: loss=0.5812, test_acc=59.13%, LR=0.000316, \n",
      "Rank=1 Epoch 11/15: loss=0.5720, test_acc=58.63%, LR=0.000316, \n",
      "Rank=0 Epoch 12/15: loss=0.5691, test_acc=60.75%, LR=0.000187, \n",
      "Rank=1 Epoch 12/15: loss=0.5603, test_acc=60.00%, LR=0.000187, \n",
      "Rank=0 Epoch 13/15: loss=0.5598, test_acc=61.12%, LR=0.000086, \n",
      "Rank=1 Epoch 13/15: loss=0.5487, test_acc=61.20%, LR=0.000086, \n",
      "Rank=0 Epoch 14/15: loss=0.5460, test_acc=61.32%, LR=0.000022, \n",
      "Rank=1 Epoch 14/15: loss=0.5374, test_acc=62.05%, LR=0.000022, \n",
      "Rank=0 Epoch 15/15: loss=0.5411, test_acc=61.72%, LR=0.000000, \n",
      "Rank=0 Total Time=67.25555896759033\n",
      "Rank=0 Mem Usage=312.9384765625 MB\n",
      "Rank=1 Epoch 15/15: loss=0.5352, test_acc=61.75%, LR=0.000000, \n",
      "Rank=1 Total Time=67.26204061508179\n",
      "Rank=1 Mem Usage=312.9384765625 MB\n",
      "Total time: 78.58185935020447\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py --batch_size 32 --backend nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank=1 Epoch 1/15: loss=1.1561, test_acc=16.90%, LR=0.000155, \n",
      "Rank=0 Epoch 1/15: loss=1.1577, test_acc=16.38%, LR=0.000155, \n",
      "Rank=0 Epoch 2/15: loss=1.0503, test_acc=25.82%, LR=0.000444, \n",
      "Rank=1 Epoch 2/15: loss=1.0589, test_acc=26.67%, LR=0.000444, \n",
      "Rank=0 Epoch 3/15: loss=0.9510, test_acc=33.67%, LR=0.000770, \n",
      "Rank=1 Epoch 3/15: loss=0.9476, test_acc=34.10%, LR=0.000770, \n",
      "Rank=1 Epoch 4/15: loss=0.8671, test_acc=37.20%, LR=0.000976, \n",
      "Rank=0 Epoch 4/15: loss=0.8718, test_acc=37.60%, LR=0.000976, \n",
      "Rank=0 Epoch 5/15: loss=0.8089, test_acc=42.40%, LR=0.000993, \n",
      "Rank=1 Epoch 5/15: loss=0.7947, test_acc=41.68%, LR=0.000993, \n",
      "Rank=0 Epoch 6/15: loss=0.7695, test_acc=45.85%, LR=0.000947, \n",
      "Rank=1 Epoch 6/15: loss=0.7625, test_acc=44.35%, LR=0.000947, \n",
      "Rank=0 Epoch 7/15: loss=0.7409, test_acc=47.27%, LR=0.000861, \n",
      "Rank=1 Epoch 7/15: loss=0.7343, test_acc=47.08%, LR=0.000861, \n",
      "Rank=0 Epoch 8/15: loss=0.7157, test_acc=48.70%, LR=0.000743, \n",
      "Rank=1 Epoch 8/15: loss=0.6989, test_acc=48.80%, LR=0.000743, \n",
      "Rank=0 Epoch 9/15: loss=0.6977, test_acc=51.52%, LR=0.000604, \n",
      "Rank=1 Epoch 9/15: loss=0.6830, test_acc=50.80%, LR=0.000604, \n",
      "Rank=1 Epoch 10/15: loss=0.6533, test_acc=51.78%, LR=0.000455, \n",
      "Rank=0 Epoch 10/15: loss=0.6747, test_acc=52.65%, LR=0.000455, \n",
      "Rank=0 Epoch 11/15: loss=0.6533, test_acc=53.95%, LR=0.000310, \n",
      "Rank=1 Epoch 11/15: loss=0.6384, test_acc=53.17%, LR=0.000310, \n",
      "Rank=0 Epoch 12/15: loss=0.6434, test_acc=54.00%, LR=0.000182, \n",
      "Rank=1 Epoch 12/15: loss=0.6305, test_acc=52.45%, LR=0.000182, \n",
      "Rank=1 Epoch 13/15: loss=0.6187, test_acc=53.85%, LR=0.000083, \n",
      "Rank=0 Epoch 13/15: loss=0.6282, test_acc=54.47%, LR=0.000083, \n",
      "Rank=0 Epoch 14/15: loss=0.6244, test_acc=55.45%, LR=0.000020, \n",
      "Rank=1 Epoch 14/15: loss=0.6160, test_acc=54.12%, LR=0.000020, \n",
      "Rank=0 Epoch 15/15: loss=0.6206, test_acc=55.15%, LR=0.000000, \n",
      "Rank=0 Total Time=64.24109506607056\n",
      "Rank=0 Mem Usage=1188.4423828125 MB\n",
      "Rank=1 Epoch 15/15: loss=0.6182, test_acc=54.33%, LR=0.000000, \n",
      "Rank=1 Total Time=64.24111890792847\n",
      "Rank=1 Mem Usage=1189.30078125 MB\n",
      "Total time: 75.83182978630066\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py --batch_size 128 --backend nccl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "Rank=1 Epoch 1/15: loss=1.1161, test_acc=27.43%, LR=0.000153, \n",
      "Rank=0 Epoch 1/15: loss=1.1224, test_acc=28.07%, LR=0.000153, \n",
      "Rank=1 Epoch 2/15: loss=0.9773, test_acc=34.80%, LR=0.000439, \n",
      "Rank=0 Epoch 2/15: loss=0.9775, test_acc=37.00%, LR=0.000439, \n",
      "Rank=1 Epoch 3/15: loss=0.8613, test_acc=38.95%, LR=0.000762, \n",
      "Rank=0 Epoch 3/15: loss=0.8723, test_acc=38.30%, LR=0.000762, \n",
      "Rank=1 Epoch 4/15: loss=0.7984, test_acc=43.68%, LR=0.000972, \n",
      "Rank=0 Epoch 4/15: loss=0.8109, test_acc=43.53%, LR=0.000972, \n",
      "Rank=1 Epoch 5/15: loss=0.7543, test_acc=48.05%, LR=0.000994, \n",
      "Rank=0 Epoch 5/15: loss=0.7601, test_acc=47.93%, LR=0.000994, \n",
      "Rank=1 Epoch 6/15: loss=0.7026, test_acc=50.68%, LR=0.000950, \n",
      "Rank=0 Epoch 6/15: loss=0.7111, test_acc=50.10%, LR=0.000950, \n",
      "Rank=0 Epoch 7/15: loss=0.6824, test_acc=51.35%, LR=0.000865, \n",
      "Rank=1 Epoch 7/15: loss=0.6770, test_acc=50.95%, LR=0.000865, \n",
      "Rank=0 Epoch 8/15: loss=0.6586, test_acc=54.80%, LR=0.000748, \n",
      "Rank=1 Epoch 8/15: loss=0.6380, test_acc=54.02%, LR=0.000748, \n",
      "Rank=0 Epoch 9/15: loss=0.6246, test_acc=55.88%, LR=0.000609, \n",
      "Rank=1 Epoch 9/15: loss=0.6102, test_acc=54.10%, LR=0.000609, \n",
      "Rank=0 Epoch 10/15: loss=0.6009, test_acc=57.95%, LR=0.000461, \n",
      "Rank=1 Epoch 10/15: loss=0.5973, test_acc=56.88%, LR=0.000461, \n",
      "Rank=0 Epoch 11/15: loss=0.5897, test_acc=59.08%, LR=0.000316, \n",
      "Rank=1 Epoch 11/15: loss=0.5722, test_acc=58.90%, LR=0.000316, \n",
      "Rank=0 Epoch 12/15: loss=0.5789, test_acc=60.60%, LR=0.000187, \n",
      "Rank=1 Epoch 12/15: loss=0.5591, test_acc=61.02%, LR=0.000187, \n",
      "Rank=0 Epoch 13/15: loss=0.5555, test_acc=60.92%, LR=0.000086, \n",
      "Rank=1 Epoch 13/15: loss=0.5512, test_acc=60.95%, LR=0.000086, \n",
      "Rank=0 Epoch 14/15: loss=0.5483, test_acc=62.30%, LR=0.000022, \n",
      "Rank=1 Epoch 14/15: loss=0.5338, test_acc=61.82%, LR=0.000022, \n",
      "Rank=0 Epoch 15/15: loss=0.5448, test_acc=62.25%, LR=0.000000, \n",
      "Rank=0 Total Time=66.09021282196045\n",
      "Rank=0 Mem Usage=312.9384765625 MB\n",
      "Rank=1 Epoch 15/15: loss=0.5341, test_acc=61.50%, LR=0.000000, \n",
      "Rank=1 Total Time=66.1113486289978\n",
      "Rank=1 Mem Usage=312.9384765625 MB\n",
      "Total time: 77.11005902290344\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py --batch_size 32 --backend gloo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "Rank=0 Epoch 1/15: loss=1.1625, test_acc=17.10%, LR=0.000155, \n",
      "Rank=1 Epoch 1/15: loss=1.1590, test_acc=16.57%, LR=0.000155, \n",
      "Rank=0 Epoch 2/15: loss=1.0588, test_acc=29.07%, LR=0.000444, \n",
      "Rank=1 Epoch 2/15: loss=1.0619, test_acc=29.35%, LR=0.000444, \n",
      "Rank=0 Epoch 3/15: loss=0.9568, test_acc=34.58%, LR=0.000770, \n",
      "Rank=1 Epoch 3/15: loss=0.9538, test_acc=33.25%, LR=0.000770, \n",
      "Rank=0 Epoch 4/15: loss=0.8741, test_acc=41.23%, LR=0.000976, \n",
      "Rank=1 Epoch 4/15: loss=0.8639, test_acc=39.98%, LR=0.000976, \n",
      "Rank=0 Epoch 5/15: loss=0.8091, test_acc=42.98%, LR=0.000993, \n",
      "Rank=1 Epoch 5/15: loss=0.7986, test_acc=40.62%, LR=0.000993, \n",
      "Rank=0 Epoch 6/15: loss=0.7668, test_acc=44.75%, LR=0.000947, \n",
      "Rank=1 Epoch 6/15: loss=0.7616, test_acc=43.10%, LR=0.000947, \n",
      "Rank=0 Epoch 7/15: loss=0.7368, test_acc=45.82%, LR=0.000861, \n",
      "Rank=1 Epoch 7/15: loss=0.7319, test_acc=43.50%, LR=0.000861, \n",
      "Rank=0 Epoch 8/15: loss=0.7111, test_acc=48.68%, LR=0.000743, \n",
      "Rank=1 Epoch 8/15: loss=0.7073, test_acc=47.65%, LR=0.000743, \n",
      "Rank=0 Epoch 9/15: loss=0.6847, test_acc=49.75%, LR=0.000604, \n",
      "Rank=1 Epoch 9/15: loss=0.6819, test_acc=49.02%, LR=0.000604, \n",
      "Rank=0 Epoch 10/15: loss=0.6643, test_acc=53.67%, LR=0.000455, \n",
      "Rank=1 Epoch 10/15: loss=0.6544, test_acc=52.75%, LR=0.000455, \n",
      "Rank=0 Epoch 11/15: loss=0.6457, test_acc=54.40%, LR=0.000310, \n",
      "Rank=1 Epoch 11/15: loss=0.6332, test_acc=53.27%, LR=0.000310, \n",
      "Rank=0 Epoch 12/15: loss=0.6316, test_acc=55.55%, LR=0.000182, \n",
      "Rank=1 Epoch 12/15: loss=0.6286, test_acc=54.50%, LR=0.000182, \n",
      "Rank=0 Epoch 13/15: loss=0.6212, test_acc=56.73%, LR=0.000083, \n",
      "Rank=1 Epoch 13/15: loss=0.6139, test_acc=55.45%, LR=0.000083, \n",
      "Rank=0 Epoch 14/15: loss=0.6181, test_acc=56.30%, LR=0.000020, \n",
      "Rank=1 Epoch 14/15: loss=0.6169, test_acc=55.65%, LR=0.000020, \n",
      "Rank=0 Epoch 15/15: loss=0.6125, test_acc=56.90%, LR=0.000000, \n",
      "Rank=0 Total Time=64.21590328216553\n",
      "Rank=0 Mem Usage=1188.4423828125 MB\n",
      "Rank=1 Epoch 15/15: loss=0.6079, test_acc=55.90%, LR=0.000000, \n",
      "Rank=1 Total Time=64.23475050926208\n",
      "Rank=1 Mem Usage=1188.4423828125 MB\n",
      "Total time: 75.30585050582886\n"
     ]
    }
   ],
   "source": [
    "!python train_ddp.py --batch_size 128 --backend gloo"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "My PyTorch Copy",
   "language": "python",
   "name": "my_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
